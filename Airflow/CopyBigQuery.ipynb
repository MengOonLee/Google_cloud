{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e14668-2790-4555-bfbe-b08ea01b3992",
   "metadata": {},
   "source": [
    "# Cloud Composer: Copying BigQuery Tables Across Different Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65836842-b4ff-421a-9897-e7b7d8b59b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bq_copy_across_locations.py\n",
    "\n",
    "\"\"\"Example Airflow DAG that performs an export from BQ tables listed in\n",
    "config file to GCS, copies GCS objects across locations (e.g., from US to\n",
    "EU) then imports from GCS to BQ. The DAG imports the gcs_to_gcs operator\n",
    "from plugins and dynamically builds the tasks based on the list of tables.\n",
    "Lastly, the DAG defines a specific application logger to generate logs.\n",
    "\n",
    "This DAG relies on three Airflow variables\n",
    "(https://airflow.apache.org/concepts.html#variables):\n",
    "* table_list_file_path - CSV file listing source and target tables, including\n",
    "Datasets.\n",
    "* gcs_source_bucket - Google Cloud Storage bucket to use for exporting\n",
    "BigQuery tables in source.\n",
    "* gcs_dest_bucket - Google Cloud Storage bucket to use for importing\n",
    "BigQuery tables in destination.\n",
    "See https://cloud.google.com/storage/docs/creating-buckets for creating a\n",
    "bucket.\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Load The Dependencies\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "import io\n",
    "import logging\n",
    "\n",
    "from airflow import models\n",
    "from airflow.contrib.operators import bigquery_to_gcs\n",
    "from airflow.contrib.operators import gcs_to_bq\n",
    "from airflow.operators import dummy_operator\n",
    "# Import operator from plugins\n",
    "from gcs_plugin.operators import gcs_to_gcs\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Set default arguments\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime.datetime.today(),\n",
    "    'depends_on_past': False,\n",
    "    'email': [''],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': datetime.timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Set variables\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# 'table_list_file_path': This variable will contain the location of the master\n",
    "# file.\n",
    "table_list_file_path = models.Variable.get('table_list_file_path')\n",
    "\n",
    "# Source Bucket\n",
    "source_bucket = models.Variable.get('gcs_source_bucket')\n",
    "\n",
    "# Destination Bucket\n",
    "dest_bucket = models.Variable.get('gcs_dest_bucket')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Set GCP logging\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "logger = logging.getLogger('bq_copy_us_to_eu_01')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Functions\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def read_table_list(table_list_file):\n",
    "    \"\"\"\n",
    "    Reads the table list file that will help in creating Airflow tasks in\n",
    "    the DAG dynamically.\n",
    "    :param table_list_file: (String) The file location of the table list file,\n",
    "    e.g. '/home/airflow/framework/table_list.csv'\n",
    "    :return table_list: (List) List of tuples containing the source and\n",
    "    target tables.\n",
    "    \"\"\"\n",
    "    table_list = []\n",
    "    logger.info('Reading table_list_file from : %s' % str(table_list_file))\n",
    "    try:\n",
    "        with io.open(table_list_file, 'rt', encoding='utf-8') as csv_file:\n",
    "            csv_reader = csv.reader(csv_file)\n",
    "            next(csv_reader)  # skip the headers\n",
    "            for row in csv_reader:\n",
    "                logger.info(row)\n",
    "                table_tuple = {\n",
    "                    'table_source': row[0],\n",
    "                    'table_dest': row[1]\n",
    "                }\n",
    "                table_list.append(table_tuple)\n",
    "            return table_list\n",
    "    except IOError as e:\n",
    "        logger.error('Error opening table_list_file %s: ' % str(\n",
    "            table_list_file), e)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Main DAG\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Define a DAG (directed acyclic graph) of tasks.\n",
    "# Any task you create within the context manager is automatically added to the\n",
    "# DAG object.\n",
    "with models.DAG('bq_copy_us_to_eu_01',\n",
    "                default_args=default_args,\n",
    "                schedule_interval=None) as dag:\n",
    "    start = dummy_operator.DummyOperator(\n",
    "        task_id='start',\n",
    "        trigger_rule='all_success'\n",
    "    )\n",
    "\n",
    "    end = dummy_operator.DummyOperator(\n",
    "        task_id='end',\n",
    "\n",
    "        trigger_rule='all_success'\n",
    "    )\n",
    "\n",
    "    # Get the table list from master file\n",
    "    all_records = read_table_list(table_list_file_path)\n",
    "\n",
    "    # Loop over each record in the 'all_records' python list to build up\n",
    "    # Airflow tasks\n",
    "    for record in all_records:\n",
    "        logger.info('Generating tasks to transfer table: {}'.format(record))\n",
    "\n",
    "        table_source = record['table_source']\n",
    "        table_dest = record['table_dest']\n",
    "\n",
    "        BQ_to_GCS = bigquery_to_gcs.BigQueryToCloudStorageOperator(\n",
    "            # Replace \":\" with valid character for Airflow task\n",
    "            task_id='{}_BQ_to_GCS'.format(table_source.replace(\":\", \"_\")),\n",
    "            source_project_dataset_table=table_source,\n",
    "            destination_cloud_storage_uris=['{}-*.avro'.format(\n",
    "                'gs://' + source_bucket + '/' + table_source)],\n",
    "            export_format='AVRO'\n",
    "        )\n",
    "\n",
    "        GCS_to_GCS = gcs_to_gcs.GoogleCloudStorageToGoogleCloudStorageOperator(\n",
    "            # Replace \":\" with valid character for Airflow task\n",
    "            task_id='{}_GCS_to_GCS'.format(table_source.replace(\":\", \"_\")),\n",
    "            source_bucket=source_bucket,\n",
    "            source_object='{}-*.avro'.format(table_source),\n",
    "            destination_bucket=dest_bucket,\n",
    "            # destination_object='{}-*.avro'.format(table_dest)\n",
    "        )\n",
    "\n",
    "        GCS_to_BQ = gcs_to_bq.GoogleCloudStorageToBigQueryOperator(\n",
    "            # Replace \":\" with valid character for Airflow task\n",
    "            task_id='{}_GCS_to_BQ'.format(table_dest.replace(\":\", \"_\")),\n",
    "            bucket=dest_bucket,\n",
    "            source_objects=['{}-*.avro'.format(table_source)],\n",
    "            destination_project_dataset_table=table_dest,\n",
    "            source_format='AVRO',\n",
    "            write_disposition='WRITE_TRUNCATE'\n",
    "        )\n",
    "\n",
    "        start >> BQ_to_GCS >> GCS_to_GCS >> GCS_to_BQ >> end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5d2089-682c-4f25-84a9-77e0addf68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a virtual environment\n",
    "sudo apt-get install -y virtualenv\n",
    "\n",
    "python3 -m venv venv\n",
    "\n",
    "source venv/bin/activate\n",
    "\n",
    "DAGS_BUCKET=us-east1-composer-advanced--455f59d9-bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e80734-b0e2-40c1-bffe-c01ccd7d89f6",
   "metadata": {},
   "source": [
    "## Setting Airflow variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cdb0ee-9f83-4ba7-9d46-69c20e695690",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud composer environments run composer-advanced-lab \\\n",
    "--location us-east1 variables -- \\\n",
    "set table_list_file_path /home/airflow/gcs/dags/bq_copy_eu_to_us_sample.csv\n",
    "\n",
    "gcloud composer environments run composer-advanced-lab \\\n",
    "--location us-east1 variables -- \\\n",
    "set gcs_source_bucket qwiklabs-gcp-04-afba1ced54ae-us\n",
    "\n",
    "gcloud composer environments run composer-advanced-lab \\\n",
    "--location us-east1 variables -- \\\n",
    "set gcs_dest_bucket qwiklabs-gcp-04-afba1ced54ae-eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863168c-2ccc-48c1-81c2-46f32c6f912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud composer environments run composer-advanced-lab \\\n",
    "--location us-east1 variables -- \\\n",
    "get gcs_source_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2aaa41-24e9-40b9-ab46-c4feac24f1da",
   "metadata": {},
   "source": [
    "## Upload DAG & dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c214235e-faec-4a52-a28b-78894c0143e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~\n",
    "gsutil -m cp -r gs://spls/gsp283/python-docs-samples .\n",
    "\n",
    "gsutil cp -r python-docs-samples/third_party/apache-airflow/plugins/* gs://$DAGS_BUCKET/plugins\n",
    "\n",
    "gsutil cp python-docs-samples/composer/workflows/bq_copy_across_locations.py gs://$DAGS_BUCKET/dags\n",
    "gsutil cp python-docs-samples/composer/workflows/bq_copy_eu_to_us_sample.csv gs://$DAGS_BUCKET/dags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
