{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e233cb4-b8be-4023-8de3-129363645202",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -qU pip wheel\n",
    "pip install -qU numpy pandas matplotlib seaborn\n",
    "pip check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc802485-b4be-4c2a-886a-6124b95f38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "sns.set(font='DejaVu Sans')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199da007-ba55-4d5e-8370-d71a9916176f",
   "metadata": {},
   "source": [
    "# SQL Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601f0e4-8443-418b-a709-e5beabb51060",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cloud Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9357c-d7c6-4d98-b49b-1c7cef520863",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud auth list\n",
    "\n",
    "gcloud config list project\n",
    "\n",
    "gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeae231-88ff-4aaa-a7fd-cdbd416f0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install -y virtualenv\n",
    "\n",
    "virtualenv -p python3 venv\n",
    "\n",
    "source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce164eb-00fb-47a5-977d-01eb49f56ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "export PROJECT_ID=$(gcloud info --format='value(config.project)')\n",
    "\n",
    "gsutil mb gs://${PROJECT_ID}-ml\n",
    "\n",
    "bq mk --project_id $PROJECT_ID flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a069816-28e7-486c-8007-1e2c11d504b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "export BUCKET=${PROJECT_ID}-ml\n",
    "\n",
    "gsutil cp airports.csv.gz \\\n",
    "gs://$BUCKET/flights/airports/airports.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6743d4-fe72-47a2-88dd-6157562d25da",
   "metadata": {},
   "outputs": [],
   "source": [
    "export ADDRESS=$(wget -qO - http://ipecho.net/plain)/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddfc2a8-828b-4c6e-83b5-cc176d1ba719",
   "metadata": {},
   "outputs": [],
   "source": [
    "MYSQLIP=$(gcloud sql instances describe \\\n",
    "flights --format=\"value(ipAddresses.ipAddress)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6bef8-5635-453f-88f2-7b1bf499b55d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SELECT, FROM, WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267cb176-6f65-4c83-8503-1ec0eafa8d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM \n",
    "    `bigquery-public-data.london_bicycles.cycle_hire` \n",
    "WHERE \n",
    "    duration>=1200;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5af339-c448-4cba-859b-44e481dac473",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GROUP BY, COUNT, AS, ORDER BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d851618-940c-47fb-9d7d-8107748a3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \n",
    "    start_station_name \n",
    "FROM \n",
    "    `bigquery-public-data.london_bicycles.cycle_hire` \n",
    "GROUP BY \n",
    "    start_station_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e5385-254f-40a2-b35c-1f9ad3895b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \n",
    "    start_station_name, \n",
    "    COUNT(*) AS num_starts \n",
    "FROM \n",
    "    `bigquery-public-data.london_bicycles.cycle_hire` \n",
    "GROUP BY \n",
    "    start_station_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32268528-3ef4-4d2d-8a64-366cda91e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \n",
    "    start_station_name, \n",
    "    COUNT(*) AS num \n",
    "FROM \n",
    "    `bigquery-public-data.london_bicycles.cycle_hire` \n",
    "GROUP BY \n",
    "    start_station_name \n",
    "ORDER BY \n",
    "    num DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95341d-94a4-4ac4-a979-c63b03b8a07a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Process data using Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54471be5-4e70-47c0-a022-4c31edb26004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import csv\n",
    "\n",
    "DATETIME_FORMAT='%Y-%m-%dT%H:%M:%S'\n",
    "\n",
    "def addtimezone(lat, lon):\n",
    "   try:\n",
    "      import timezonefinder\n",
    "      tf = timezonefinder.TimezoneFinder()\n",
    "      return (lat, lon, tf.timezone_at(lng=float(lon), lat=float(lat)))\n",
    "      #return (lat, lon, 'America/Los_Angeles') # FIXME\n",
    "   except ValueError:\n",
    "      return (lat, lon, 'TIMEZONE') # header\n",
    "\n",
    "def as_utc(date, hhmm, tzone):\n",
    "   try:\n",
    "      if len(hhmm) > 0 and tzone is not None:\n",
    "         import datetime, pytz\n",
    "         loc_tz = pytz.timezone(tzone)\n",
    "         loc_dt = loc_tz.localize(datetime.datetime.strptime(date,'%Y-%m-%d'), is_dst=False)\n",
    "         # can't just parse hhmm because the data contains 2400 and the like ...\n",
    "         loc_dt += datetime.timedelta(hours=int(hhmm[:2]), minutes=int(hhmm[2:]))\n",
    "         utc_dt = loc_dt.astimezone(pytz.utc)\n",
    "         return utc_dt.strftime(DATETIME_FORMAT), loc_dt.utcoffset().total_seconds()\n",
    "      else:\n",
    "         return '',0 # empty string corresponds to canceled flights\n",
    "   except ValueError as e:\n",
    "      print ('{} {} {}'.format(date, hhmm, tzone))\n",
    "      raise e\n",
    "\n",
    "def add_24h_if_before(arrtime, deptime):\n",
    "   import datetime\n",
    "   if len(arrtime) > 0 and len(deptime) > 0 and (arrtime < deptime):\n",
    "      adt = datetime.datetime.strptime(arrtime, DATETIME_FORMAT)\n",
    "      adt += datetime.timedelta(hours=24)\n",
    "      return adt.strftime(DATETIME_FORMAT)\n",
    "   else:\n",
    "      return arrtime\n",
    "\n",
    "def tz_correct(line, airport_timezones_dict):\n",
    "   def airport_timezone(airport_id):\n",
    "       if airport_id in airport_timezones_dict:\n",
    "          return airport_timezones_dict[airport_id]\n",
    "       else:\n",
    "          return ('37.52', '-92.17', u'America/Chicago') # population center of US\n",
    "   fields = line.split(',')\n",
    "   if fields[0] != 'FL_DATE' and len(fields) == 27:\n",
    "      # convert all times to UTC\n",
    "      dep_airport_id = fields[6]\n",
    "      arr_airport_id = fields[10]\n",
    "      dep_timezone = airport_timezone(dep_airport_id)[2] \n",
    "      arr_timezone = airport_timezone(arr_airport_id)[2]\n",
    "      \n",
    "      for f in [13, 14, 17]: #crsdeptime, deptime, wheelsoff\n",
    "         fields[f], deptz = as_utc(fields[0], fields[f], dep_timezone)\n",
    "      for f in [18, 20, 21]: #wheelson, crsarrtime, arrtime\n",
    "         fields[f], arrtz = as_utc(fields[0], fields[f], arr_timezone)\n",
    "      \n",
    "      for f in [17, 18, 20, 21]:\n",
    "         fields[f] = add_24h_if_before(fields[f], fields[14])\n",
    "\n",
    "      fields.extend(airport_timezone(dep_airport_id))\n",
    "      fields[-1] = str(deptz)\n",
    "      fields.extend(airport_timezone(arr_airport_id))\n",
    "      fields[-1] = str(arrtz)\n",
    "\n",
    "      yield fields\n",
    "\n",
    "def get_next_event(fields):\n",
    "    if len(fields[14]) > 0:\n",
    "       event = list(fields) # copy\n",
    "       event.extend(['departed', fields[14]])\n",
    "       for f in [16,17,18,19,21,22,25]:\n",
    "          event[f] = ''  # not knowable at departure time\n",
    "       yield event\n",
    "    if len(fields[17]) > 0:\n",
    "       event = list(fields) # copy\n",
    "       event.extend(['wheelsoff', fields[17]])\n",
    "       for f in [18,19,21,22,25]:\n",
    "          event[f] = ''  # not knowable at wheelsoff time\n",
    "       yield event\n",
    "    if len(fields[21]) > 0:\n",
    "       event = list(fields)\n",
    "       event.extend(['arrived', fields[21]])\n",
    "       yield event\n",
    "\n",
    "def create_row(fields):\n",
    "    header = 'FL_DATE,UNIQUE_CARRIER,AIRLINE_ID,CARRIER,FL_NUM,ORIGIN_AIRPORT_ID,ORIGIN_AIRPORT_SEQ_ID,ORIGIN_CITY_MARKET_ID,ORIGIN,DEST_AIRPORT_ID,DEST_AIRPORT_SEQ_ID,DEST_CITY_MARKET_ID,DEST,CRS_DEP_TIME,DEP_TIME,DEP_DELAY,TAXI_OUT,WHEELS_OFF,WHEELS_ON,TAXI_IN,CRS_ARR_TIME,ARR_TIME,ARR_DELAY,CANCELLED,CANCELLATION_CODE,DIVERTED,DISTANCE,DEP_AIRPORT_LAT,DEP_AIRPORT_LON,DEP_AIRPORT_TZOFFSET,ARR_AIRPORT_LAT,ARR_AIRPORT_LON,ARR_AIRPORT_TZOFFSET,EVENT,NOTIFY_TIME'.split(',')\n",
    "\n",
    "    featdict = {}\n",
    "    for name, value in zip(header, fields):\n",
    "        featdict[name] = value\n",
    "    featdict['EVENT_DATA'] = ','.join(fields)\n",
    "    return featdict\n",
    " \n",
    "def run(project, bucket, dataset, region):\n",
    "   argv = [\n",
    "      '--project={0}'.format(project),\n",
    "      '--job_name=ch04timecorr',\n",
    "      '--save_main_session',\n",
    "      '--staging_location=gs://{0}/flights/staging/'.format(bucket),\n",
    "      '--temp_location=gs://{0}/flights/temp/'.format(bucket),\n",
    "      '--setup_file=./setup.py',\n",
    "      '--max_num_workers=8',\n",
    "      '--region={}'.format(region),\n",
    "      '--autoscaling_algorithm=THROUGHPUT_BASED',\n",
    "      '--runner=DataflowRunner'\n",
    "   ]\n",
    "   airports_filename = 'gs://{}/flights/airports/airports.csv.gz'.format(bucket)\n",
    "   flights_raw_files = 'gs://{}/flights/raw/*.csv'.format(bucket)\n",
    "   flights_output = 'gs://{}/flights/tzcorr/all_flights'.format(bucket)\n",
    "   events_output = '{}:{}.simevents'.format(project, dataset)\n",
    "\n",
    "   pipeline = beam.Pipeline(argv=argv)\n",
    "   \n",
    "   airports = (pipeline \n",
    "      | 'airports:read' >> beam.io.ReadFromText(airports_filename)\n",
    "      | 'airports:fields' >> beam.Map(lambda line: next(csv.reader([line])))\n",
    "      | 'airports:tz' >> beam.Map(lambda fields: (fields[0], addtimezone(fields[21], fields[26])))\n",
    "   )\n",
    "\n",
    "   flights = (pipeline \n",
    "      | 'flights:read' >> beam.io.ReadFromText (flights_raw_files)\n",
    "      | 'flights:tzcorr' >> beam.FlatMap(tz_correct, beam.pvalue.AsDict(airports))\n",
    "   )\n",
    "\n",
    "   (flights \n",
    "      | 'flights:tostring' >> beam.Map(lambda fields: ','.join(fields)) \n",
    "      | 'flights:out' >> beam.io.textio.WriteToText(flights_output)\n",
    "   )\n",
    "\n",
    "   events = flights | beam.FlatMap(get_next_event)\n",
    "\n",
    "   schema = 'FL_DATE:date,UNIQUE_CARRIER:string,AIRLINE_ID:string,CARRIER:string,FL_NUM:string,ORIGIN_AIRPORT_ID:string,ORIGIN_AIRPORT_SEQ_ID:integer,ORIGIN_CITY_MARKET_ID:string,ORIGIN:string,DEST_AIRPORT_ID:string,DEST_AIRPORT_SEQ_ID:integer,DEST_CITY_MARKET_ID:string,DEST:string,CRS_DEP_TIME:timestamp,DEP_TIME:timestamp,DEP_DELAY:float,TAXI_OUT:float,WHEELS_OFF:timestamp,WHEELS_ON:timestamp,TAXI_IN:float,CRS_ARR_TIME:timestamp,ARR_TIME:timestamp,ARR_DELAY:float,CANCELLED:string,CANCELLATION_CODE:string,DIVERTED:string,DISTANCE:float,DEP_AIRPORT_LAT:float,DEP_AIRPORT_LON:float,DEP_AIRPORT_TZOFFSET:float,ARR_AIRPORT_LAT:float,ARR_AIRPORT_LON:float,ARR_AIRPORT_TZOFFSET:float,EVENT:string,NOTIFY_TIME:timestamp,EVENT_DATA:string'\n",
    "\n",
    "   (events \n",
    "      | 'events:totablerow' >> beam.Map(lambda fields: create_row(fields)) \n",
    "      | 'events:out' >> beam.io.WriteToBigQuery(\n",
    "                              events_output, schema=schema,\n",
    "                              write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "                              create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n",
    "   )\n",
    "\n",
    "   pipeline.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   import argparse\n",
    "   parser = argparse.ArgumentParser(description='Run pipeline on the cloud')\n",
    "   parser.add_argument('-p','--project', help='Unique project ID', required=True)\n",
    "   parser.add_argument('-b','--bucket', help='Bucket where your data were ingested in Chapter 2', required=True)\n",
    "   parser.add_argument('-r','--region', help='Region in which to run the Dataflow job. Choose the same region as your bucket.', required=True)\n",
    "   parser.add_argument('-d','--dataset', help='BigQuery dataset', default='flights')\n",
    "   args = vars(parser.parse_args())\n",
    "\n",
    "   print (\"Correcting timestamps and writing to BigQuery dataset {}\".format(args['dataset']))\n",
    "  \n",
    "   run(project=args['project'], bucket=args['bucket'], dataset=args['dataset'], region=args['region'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1e974-40c9-420f-ba08-55cf5263b507",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inspect the processed data in Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e0530-de4f-4acd-858f-3a4442750043",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT\n",
    "  FL_NUM,\n",
    "  ORIGIN,\n",
    "  DEP_TIME,\n",
    "  DEP_DELAY,\n",
    "  DEST,\n",
    "  ARR_TIME,\n",
    "  ARR_DELAY,\n",
    "  EVENT,\n",
    "  NOTIFY_TIME\n",
    "FROM\n",
    "  `flightssample.simevents`\n",
    "WHERE\n",
    "  (DEP_DELAY > 15 and ORIGIN = 'SEA') or\n",
    "  (ARR_DELAY > 15 and DEST = 'SEA')\n",
    "ORDER BY\n",
    "  NOTIFY_TIME ASC\n",
    "LIMIT\n",
    "  10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152473e-bd75-4935-bb9b-508139623afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT\n",
    "  EVENT,\n",
    "  NOTIFY_TIME,\n",
    "  EVENT_DATA\n",
    "FROM\n",
    "  `flightssample.simevents`\n",
    "WHERE\n",
    "  NOTIFY_TIME >= TIMESTAMP('2015-01-01 00:00:00 UTC')\n",
    "  AND NOTIFY_TIME < TIMESTAMP('2015-01-03 00:00:00 UTC')\n",
    "ORDER BY\n",
    "  NOTIFY_TIME ASC\n",
    "LIMIT\n",
    "  10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4358a7d-3444-497c-8f8f-0a96a6075383",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the real-time Google Dataflow stream processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34e090-82ec-4402-abaf-8cca8019799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pytz\n",
    "import logging\n",
    "import argparse\n",
    "import datetime\n",
    "from google.cloud import pubsub_v1 # Upgrading The Library\n",
    "import google.cloud.bigquery as bq\n",
    "\n",
    "TIME_FORMAT = '%Y-%m-%d %H:%M:%S %Z'\n",
    "RFC3339_TIME_FORMAT = '%Y-%m-%dT%H:%M:%S-00:00'\n",
    "\n",
    "def publish(publisher, topics, allevents, notify_time):\n",
    "   timestamp = notify_time.strftime(RFC3339_TIME_FORMAT)\n",
    "   for key in topics:  # 'departed', 'arrived', etc.\n",
    "      topic = topics[key]\n",
    "      events = allevents[key]\n",
    "      # the client automatically batches\n",
    "      logging.info('Publishing {} {} till {}'.format(len(events), key, timestamp))\n",
    "      for event_data in events:\n",
    "          publisher.publish(topic, event_data.encode(), EventTimeStamp=timestamp)\n",
    "\n",
    "def notify(publisher, topics, rows, simStartTime, programStart, speedFactor):\n",
    "   # sleep computation\n",
    "   def compute_sleep_secs(notify_time):\n",
    "        time_elapsed = (datetime.datetime.utcnow() - programStart).total_seconds()\n",
    "        sim_time_elapsed = (notify_time - simStartTime).total_seconds() / speedFactor\n",
    "        to_sleep_secs = sim_time_elapsed - time_elapsed\n",
    "        return to_sleep_secs\n",
    "\n",
    "   tonotify = {}\n",
    "   for key in topics:\n",
    "     tonotify[key] = list()\n",
    "\n",
    "   for row in rows:\n",
    "       event, notify_time, event_data = row\n",
    "\n",
    "       # how much time should we sleep?\n",
    "       if compute_sleep_secs(notify_time) > 1:\n",
    "          # notify the accumulated tonotify\n",
    "          publish(publisher, topics, tonotify, notify_time)\n",
    "          for key in topics:\n",
    "             tonotify[key] = list()\n",
    "\n",
    "          # recompute sleep, since notification takes a while\n",
    "          to_sleep_secs = compute_sleep_secs(notify_time)\n",
    "          if to_sleep_secs > 0:\n",
    "             logging.info('Sleeping {} seconds'.format(to_sleep_secs))\n",
    "             time.sleep(to_sleep_secs)\n",
    "       tonotify[event].append(event_data)\n",
    "\n",
    "   # left-over records; notify again\n",
    "   publish(publisher, topics, tonotify, notify_time)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   parser = argparse.ArgumentParser(description='Send simulated flight events to Cloud Pub/Sub')\n",
    "   parser.add_argument('--startTime', help='Example: 2015-05-01 00:00:00 UTC', required=True)\n",
    "   parser.add_argument('--endTime', help='Example: 2015-05-03 00:00:00 UTC', required=True)\n",
    "   parser.add_argument('--project', help='your project id, to create pubsub topic', required=True)\n",
    "   parser.add_argument('--speedFactor', help='Example: 60 implies 1 hour of data sent to Cloud Pub/Sub in 1 minute', required=True, type=float)\n",
    "   parser.add_argument('--jitter', help='type of jitter to add: None, uniform, exp  are the three options', default='None')\n",
    "\n",
    "   # set up BigQuery bqclient\n",
    "   logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)\n",
    "   args = parser.parse_args()\n",
    "   bqclient = bq.Client(args.project)\n",
    "   dataset =  bqclient.get_dataset( bqclient.dataset('flights') )  # throws exception on failure\n",
    "\n",
    "   # jitter?\n",
    "   if args.jitter == 'exp':\n",
    "      jitter = 'CAST (-LN(RAND()*0.99 + 0.01)*30 + 90.5 AS INT64)'\n",
    "   elif args.jitter == 'uniform':\n",
    "      jitter = 'CAST(90.5 + RAND()*30 AS INT64)'\n",
    "   else:\n",
    "      jitter = '0'\n",
    "\n",
    "\n",
    "   # run the query to pull simulated events\n",
    "   querystr = \"\"\"\n",
    "SELECT\n",
    "  EVENT,\n",
    "  TIMESTAMP_ADD(NOTIFY_TIME, INTERVAL {} SECOND) AS NOTIFY_TIME,\n",
    "  EVENT_DATA\n",
    "FROM\n",
    "  flights.simevents\n",
    "WHERE\n",
    "  NOTIFY_TIME >= TIMESTAMP('{}')\n",
    "  AND NOTIFY_TIME < TIMESTAMP('{}')\n",
    "ORDER BY\n",
    "  NOTIFY_TIME ASC\n",
    "\"\"\"\n",
    "   rows = bqclient.query(querystr.format(jitter,\n",
    "                                         args.startTime,\n",
    "                                         args.endTime))\n",
    "\n",
    "   # create one Pub/Sub notification topic for each type of event\n",
    "   publisher = pubsub_v1.PublisherClient()\n",
    "   topics = {}\n",
    "   for event_type in ['wheelsoff', 'arrived', 'departed']:\n",
    "       topics[event_type] = publisher.topic_path(args.project, event_type)\n",
    "       try:\n",
    "         # Getting the new topics from PubSub\n",
    "          for topic in publisher.list_topics(request={\"project\": project_path}):\n",
    "                print(topic)\n",
    "       except:\n",
    "         #Creating New topics\n",
    "           publisher.create_topic(request={\"name\": topics[event_type]})\n",
    "\n",
    "   # notify about each row in the dataset\n",
    "   programStartTime = datetime.datetime.utcnow()\n",
    "   simStartTime = datetime.datetime.strptime(args.startTime, TIME_FORMAT).replace(tzinfo=pytz.UTC)\n",
    "   print('Simulation start time is {}'.format(simStartTime))\n",
    "   notify(publisher, topics, rows, simStartTime, programStartTime, args.speedFactor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c220a3bb-537d-43a5-8200-de50f317b1a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The delay data for all events that have happened within the last 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b782286-85c5-4a04-b87b-0325f5108dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardsql\n",
    "SELECT\n",
    "  airport,\n",
    "  arr_delay,\n",
    "  dep_delay,\n",
    "  timestamp,\n",
    "  latitude,\n",
    "  longitude,\n",
    "  num_flights\n",
    "FROM\n",
    "  flights.streaming_delays\n",
    "WHERE\n",
    "  ABS(TIMESTAMP_DIFF(timestamp,\n",
    "      (\n",
    "      SELECT\n",
    "        MAX(timestamp) latest\n",
    "      FROM\n",
    "        flights.streaming_delays ),\n",
    "      MINUTE)) < 29\n",
    "  AND num_flights > 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9585ff-b902-4271-bb4e-084d054dae2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The aggregate arrival and delay times for all airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb0c9a-875a-4565-9f6e-3d042cd90ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardSQL\n",
    "SELECT\n",
    "  airport,\n",
    "  last[safe_OFFSET(0)].*,\n",
    "  CONCAT(CAST(last[safe_OFFSET(0)].latitude AS STRING), \",\",\n",
    "        CAST(last[safe_OFFSET(0)].longitude AS STRING)) AS location\n",
    "FROM (\n",
    "  SELECT\n",
    "    airport,\n",
    "    ARRAY_AGG(STRUCT(arr_delay,\n",
    "        dep_delay,\n",
    "        timestamp,\n",
    "        latitude,\n",
    "        longitude,\n",
    "        num_flights)\n",
    "    ORDER BY\n",
    "      timestamp DESC\n",
    "    LIMIT\n",
    "      1) last\n",
    "  FROM\n",
    "    `[PROJECT_ID].flights.streaming_delays`\n",
    "  GROUP BY\n",
    "    airport )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47888b52-1468-45ff-addf-59c095e2d60b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191483d0-28fe-4fcc-a4cd-88c46ed14756",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a federated table in a BigQuery dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b8ab7-ca5a-413d-bcc1-55e97e033562",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq mk --external_table_definition=./tzcorr.json@CSV=gs://$BUCKET/flights/tzcorr/all_flights-00001-of-00025 flights.fedtzcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13f8c0-b6a1-4d9b-a598-b82bd0de1e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT\n",
    "  ORIGIN,\n",
    "  AVG(DEP_DELAY) AS dep_delay,\n",
    "  AVG(ARR_DELAY) AS arr_delay,\n",
    "  COUNT(ARR_DELAY) AS num_flights\n",
    "FROM\n",
    "  `flights.tzcorr`\n",
    "GROUP BY\n",
    "  ORIGIN\n",
    "HAVING\n",
    "  num_flights > 3650\n",
    "ORDER BY\n",
    "  dep_delay DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b5ec3-cc1e-46cc-a6f1-0bf6d380d936",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create the Model using the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee79d5f7-a482-4a7e-ac4e-e23a02783398",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze\n",
    "\n",
    "!pip install --upgrade google-cloud-bigquery\n",
    "!pip install google-cloud-bigquery-storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af2929c-e75f-4eb3-aba9-64c85e6c8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1659b6-1fa2-4b2d-8d86-d47805facf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery df\n",
    "SELECT\n",
    "  COUNTIF(arr_delay >= 15)/COUNT(arr_delay) AS frac_delayed\n",
    "FROM flights.tzcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f4dde9-316c-4f56-8a3a-f566a4621c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(font=\"DejaVu Sans\")\n",
    "ax = sns.violinplot(data=df, x='ARR_DELAY', inner='box', orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d6f3a-8f37-4da1-9511-ae0d5b2525e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT ARR_DELAY, DEP_DELAY\n",
    "FROM `flights.tzcorr`\n",
    "WHERE DEP_DELAY >= 10 AND RAND() < 0.01\n",
    "\"\"\"\n",
    "df = client.query(sql).to_dataframe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b18410-7311-4fbd-be07-7b67d3657726",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT ARR_DELAY, DEP_DELAY\n",
    "FROM `flights.tzcorr`\n",
    "WHERE RAND() < 0.001\n",
    "\"\"\"\n",
    "df = client.query(sql).to_dataframe()\n",
    "df['ontime'] = df['DEP_DELAY'] < 10\n",
    "ax = sns.violinplot(data=df, x='ARR_DELAY', y='ontime',\n",
    "    inner='box', orient='h', gridsize=1000)\n",
    "ax.set_xlim(-50, 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed322f-9683-422e-9090-e43c6354f6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a filter based on values within three times the standard deviation\n",
    "SELECT\n",
    "  AVG(DEP_DELAY) - 3*STDDEV(DEP_DELAY) AS filtermin,\n",
    "  AVG(DEP_DELAY) + 3*STDDEV(DEP_DELAY) AS filtermax\n",
    "FROM\n",
    "  `flights.tzcorr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173644c-8df9-4d08-96a6-c260cb6b6a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a filter based on quantiles\n",
    "SELECT\n",
    "  APPROX_QUANTILES(DEP_DELAY, 20)\n",
    "FROM\n",
    "  `flights.tzcorr`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f711d9-bca2-43c4-b2f0-797098116111",
   "metadata": {},
   "source": [
    "370 examples is the sample size that is required to cover three standard deviations in a dataset with a normal distribution. A filter that selects only data points where you have at least 370 examples ensures that you have enough samples at each datapoint to satisfy the three sigma rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b86459-60b3-4da8-b639-e45ba2cff8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT\n",
    "  DEP_DELAY,\n",
    "  AVG(ARR_DELAY) AS arrival_delay,\n",
    "  STDDEV(ARR_DELAY) AS stddev_arrival_delay,\n",
    "  COUNT(ARR_DELAY) AS numflights\n",
    "FROM\n",
    "  `flights.tzcorr`\n",
    "GROUP BY\n",
    "  DEP_DELAY\n",
    "HAVING\n",
    "  numflights > 370\n",
    "ORDER BY\n",
    "  DEP_DELAY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527113c8-28fb-47f7-ba58-ed5f72da3ad4",
   "metadata": {},
   "source": [
    "Analyze the sensitivity of the data set with a number of different threshold values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a53f2f-92d9-4439-adeb-d16c32bdc2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT\n",
    "  (5714008 - SUM(numflights)) AS num_removed,\n",
    "  AVG(arrival_delay * numflights)/AVG(DEP_DELAY * numflights) AS lm\n",
    "FROM (\n",
    "  #standardsql\n",
    "SELECT\n",
    "  DEP_DELAY,\n",
    "  AVG(ARR_DELAY) AS arrival_delay,\n",
    "  COUNT(ARR_DELAY) AS numflights\n",
    "FROM\n",
    "  `flights.tzcorr`\n",
    "GROUP BY\n",
    "  DEP_DELAY\n",
    "ORDER BY\n",
    "  DEP_DELAY\n",
    ")\n",
    "WHERE\n",
    "  numflights > 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d0f896-ccdb-4138-ac15-b6e2b1a0ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "depdelayquery = \"\"\"\n",
    "SELECT\n",
    "  DEP_DELAY,\n",
    "  arrival_delay,\n",
    "  stddev_arrival_delay,\n",
    "  numflights\n",
    "FROM (\n",
    "  SELECT\n",
    "    DEP_DELAY,\n",
    "    AVG(ARR_DELAY) AS arrival_delay,\n",
    "    STDDEV(ARR_DELAY) AS stddev_arrival_delay,\n",
    "    COUNT(ARR_DELAY) AS numflights\n",
    "  FROM\n",
    "    `flights.tzcorr`\n",
    "  GROUP BY\n",
    "    DEP_DELAY )\n",
    "WHERE\n",
    "  numflights > 370\n",
    "ORDER BY\n",
    "  DEP_DELAY\n",
    "\"\"\"\n",
    "depdelay = client.query(depdelayquery).to_dataframe()\n",
    "depdelay.head()\n",
    "\n",
    "ax = depdelay.plot(kind='line',\n",
    "    x='DEP_DELAY', y='arrival_delay',\n",
    "    yerr='stddev_arrival_delay');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b186aaef-252c-4138-b40f-2d6f8763bfeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Complementary cumulative distribution\n",
    "\n",
    "A probability that a statistic is greater than Z.\n",
    "\\begin{equation}\n",
    "f(Z) = 1 - \\Phi(Z)\n",
    "\\end{equation}\n",
    "\n",
    "The Z value to be 0.52 at which the probability is 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2204743-3caf-47fd-94e8-78b7332e3a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_30 = 0.52\n",
    "depdelay['arr_delay_30'] = (\n",
    "  (Z_30 * depdelay['stddev_arrival_delay'])\n",
    "  + depdelay['arrival_delay']\n",
    ")\n",
    "\n",
    "ax = plt.axes()\n",
    "depdelay.plot(kind='line',\n",
    "    x='DEP_DELAY', y='arr_delay_30', ax=ax,\n",
    "    ylim=(0, 30), xlim=(0, 30), legend=False)\n",
    "ax.set_xlabel('Departure Delay (minutes)')\n",
    "ax.set_ylabel('> 30% likelihood of this Arrival Delay (minutes)')\n",
    "plt.axhline(y=15, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405c8729-eba9-4427-97dd-2274cce41d85",
   "metadata": {},
   "source": [
    "Breaks up the arrival delays for each departure delay into 100 bins and then selects the arrival delay for the 70th bin as the appropriate value. The 70th bin contains the value that will occur 30% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ea4a9-195b-4654-b1d2-8a70dbaeea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "depdelayquery2 = \"\"\"\n",
    "SELECT\n",
    "  DEP_DELAY,\n",
    "  APPROX_QUANTILES(ARR_DELAY, 101)[OFFSET(70)] AS arrival_delay,\n",
    "  COUNT(ARR_DELAY) AS numflights\n",
    "FROM\n",
    "  `flights.tzcorr`\n",
    "GROUP BY\n",
    "  DEP_DELAY\n",
    "HAVING\n",
    "  numflights > 370\n",
    "ORDER BY\n",
    "  DEP_DELAY\n",
    "\"\"\"\n",
    "depdelay = client.query(depdelayquery2).to_dataframe()\n",
    "\n",
    "ax = plt.axes()\n",
    "depdelay.plot(kind='line',\n",
    "    x='DEP_DELAY', y='arrival_delay', ax=ax,\n",
    "    ylim=(0, 30), xlim=(0, 30), legend=False)\n",
    "ax.set_xlabel('Departure Delay (minutes)')\n",
    "ax.set_ylabel('> 30% likelihood of this Arrival Delay (minutes)')\n",
    "plt.axhline(y=15, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf0ad5e-ae7a-4045-9a4e-ebf67d3bdfe4",
   "metadata": {},
   "source": [
    "## Evaluating data model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cce02a0-0585-4cd4-be5d-895ca2211760",
   "metadata": {},
   "source": [
    "### Selecting a training data set\n",
    "\n",
    "When selecting your training data set, you need to decide on a repeatable mechanism to select a reasonable subset of the data as the training data set, to be used to create predictive models. The remaining data in the dataset will then form the test set that will be used to evaluate the effectiveness of your models.\n",
    "\n",
    "The best approach to take is to identify a specific set of dates, each of which is initially chosen at random, as the training data set. Those dates are saved in a database table of their own, allowing multiple replays of training and test queries to be carried out in a consistent manner.\n",
    "\n",
    "The next step is to randomly select 70% of these dates to be the training days. The technique used here is use a cryptographic hashing function to generate a fingerprint of the flight date and then to select the date as a training day if the last two digits of the fingerprint are < 70. Since cryptographic functions are designed to maximize entropy, this approach provides a pseudo-random selection process that is repeatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e74cc6-b3be-4d5b-bdc4-992d3baea576",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT\n",
    "  FL_DATE,\n",
    "  IF(MOD(ABS(FARM_FINGERPRINT(CAST(FL_DATE AS STRING))), \n",
    "    100) < 70, 'True', 'False') AS is_train_day\n",
    "FROM (\n",
    "  SELECT\n",
    "    DISTINCT(FL_DATE) AS FL_DATE\n",
    "  FROM\n",
    "    `flights.tzcorr`)\n",
    "ORDER BY\n",
    "  FL_DATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e35896b-86ba-476a-90fa-dc78e7de3942",
   "metadata": {},
   "source": [
    "Create a variable with the new BigQuery query using just the training day partition of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e42654-d6f3-4531-88b6-45145a7e45d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "depdelayquery3 = \"\"\"\n",
    "SELECT\n",
    "  *\n",
    "FROM (\n",
    "  SELECT\n",
    "    DEP_DELAY,\n",
    "    APPROX_QUANTILES(ARR_DELAY,\n",
    "      101)[OFFSET(70)] AS arrival_delay,\n",
    "    COUNT(ARR_DELAY) AS numflights\n",
    "  FROM\n",
    "    `flights.tzcorr` f\n",
    "  JOIN\n",
    "    `flights.trainday` t\n",
    "  ON\n",
    "    f.FL_DATE = t.FL_DATE\n",
    "  WHERE\n",
    "    t.is_train_day = 'True'\n",
    "  GROUP BY\n",
    "    DEP_DELAY )\n",
    "WHERE\n",
    "  numflights > 370\n",
    "ORDER BY\n",
    "  DEP_DELAY\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f522412f-7b77-4510-b1fe-ea0c6371ab34",
   "metadata": {},
   "source": [
    "Plot the intersection of the 15 minute delay line with the 30% arrival delay probability line for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8b9282-665b-4512-9820-995bf3a28721",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery depdelay\n",
    "SELECT\n",
    "  DEP_DELAY,\n",
    "  arrival_delay,\n",
    "  numflights\n",
    "FROM (\n",
    "  SELECT\n",
    "    DEP_DELAY,\n",
    "    APPROX_QUANTILES(ARR_DELAY,\n",
    "      101)[OFFSET(70)] AS arrival_delay,\n",
    "    COUNT(ARR_DELAY) AS numflights\n",
    "  FROM\n",
    "    `flights.tzcorr`\n",
    "  GROUP BY\n",
    "    DEP_DELAY )\n",
    "WHERE\n",
    "  numflights > 370\n",
    "ORDER BY\n",
    "  DEP_DELAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d41be5-3828-4825-8f87-11cf39a7a728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.axes()\n",
    "depdelay.plot(kind='line', x='DEP_DELAY', y='arrival_delay',\n",
    "    ax=ax, ylim=(0,30), xlim=(0,30), legend=False)\n",
    "ax.set_xlabel('Departure Delay (minutes)')\n",
    "ax.set_ylabel('> 30% likelihood of this Arrival Delay (minutes)')\n",
    "plt.axhline(y=15, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a42457-d806-4eee-b7b9-892b4e1652af",
   "metadata": {},
   "source": [
    "Next, evaluate how well the recommendation of 15 minutes does in predicting an arrival delay of 15 minutes or more. You'll create a query that analyses the test data set using the model parameters and displays the results of the scoring function, reporting out true positive, false positive, true negative and false negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a511fe1-b8d8-45b2-b463-1bf22556b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery eval\n",
    "SELECT\n",
    "  SUM(IF(DEP_DELAY < 16\n",
    "      AND arr_delay < 15, 1, 0)) AS correct_nocancel,\n",
    "  SUM(IF(DEP_DELAY < 16\n",
    "      AND arr_delay >= 15, 1, 0)) AS wrong_nocancel,\n",
    "  SUM(IF(DEP_DELAY >= 16\n",
    "      AND arr_delay < 15, 1, 0)) AS wrong_cancel,\n",
    "  SUM(IF(DEP_DELAY >= 16\n",
    "      AND arr_delay >= 15, 1, 0)) AS correct_cancel\n",
    "FROM (\n",
    "  SELECT\n",
    "    DEP_DELAY,\n",
    "    ARR_DELAY\n",
    "  FROM\n",
    "    `flights.tzcorr` f\n",
    "  JOIN\n",
    "    `flights.trainday` t\n",
    "  ON\n",
    "    f.FL_DATE = t.FL_DATE\n",
    "  WHERE\n",
    "    t.is_train_day = 'False' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e320341-8443-4b5f-b25f-c118fcfe30ef",
   "metadata": {},
   "source": [
    "Display the ratio of correct to incorrect calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a447c9-344b-46c2-8d54-be043a9f2708",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (eval['correct_nocancel'] / (eval['correct_nocancel'] + \\\n",
    "eval['wrong_nocancel']))\n",
    "print (eval['correct_cancel'] / (eval['correct_cancel'] + \\\n",
    "eval['wrong_cancel']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
