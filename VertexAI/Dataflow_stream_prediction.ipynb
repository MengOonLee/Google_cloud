{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "654619e4-82fb-405a-966e-71c80888f0dc",
   "metadata": {},
   "source": [
    "# Real time Machine Learning with Cloud Dataflow and Vertex AI\n",
    "\n",
    "`Dataflow` is a managed service for executing a wide variety of data processing patterns.\n",
    "\n",
    "`Vertex AI` is a unified ML platform used to build, deploy, and scale ML models with pre-trained and custom tooling within a unified artificial intelligence platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25247b-2dc3-41d2-b9e2-fe1cf362c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/GoogleCloudPlatform/data-science-on-gcp/\n",
    "cd ~/data-science-on-gcp/11_realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b82e40-0401-4193-aad0-9069b3ff2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install google-cloud-aiplatform cloudml-hypertune pyfarmhash tensorflow\n",
    "pip3 install kfp 'apache-beam[gcp]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d7366-7f6d-44bc-b0da-1ddac9eb42d8",
   "metadata": {},
   "source": [
    "## Machine learning training dataset\n",
    "\n",
    "Run a Dataflow pipeline to create the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a30c7de-e135-4fb8-806f-66b81571caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"./flightstxf\", exist_ok=True)\n",
    "open(\"./flightstxf/__init__.py\", \"w\").close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbc785a-6e0f-4242-9ec3-ec215d5a8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./flightstxf/flights_transforms.py\n",
    "#!/usr/bin/env python3\n",
    "import apache_beam as beam\n",
    "import datetime as dt\n",
    "import logging\n",
    "import numpy as np\n",
    "import farmhash  # pip install pyfarmhash\n",
    "\n",
    "DATETIME_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "WINDOW_DURATION = 60 * 60\n",
    "WINDOW_EVERY = 5 * 60\n",
    "\n",
    "\n",
    "def get_data_split(fl_date):\n",
    "    fl_date_str = str(fl_date)\n",
    "    # Use farm fingerprint just like in BigQuery\n",
    "    x = np.abs(np.uint64(farmhash.fingerprint64(fl_date_str)).astype('int64') % 100)\n",
    "    if x < 60:\n",
    "        data_split = 'TRAIN'\n",
    "    elif x < 80:\n",
    "        data_split = 'VALIDATE'\n",
    "    else:\n",
    "        data_split = 'TEST'\n",
    "    return data_split\n",
    "\n",
    "\n",
    "def get_data_split_2019(fl_date):\n",
    "    fl_date_str = str(fl_date)\n",
    "    if fl_date_str > '2019':\n",
    "        data_split = 'TEST'\n",
    "    else:\n",
    "        # Use farm fingerprint just like in BigQuery\n",
    "        x = np.abs(np.uint64(farmhash.fingerprint64(fl_date_str)).astype('int64') % 100)\n",
    "        if x < 95:\n",
    "            data_split = 'TRAIN'\n",
    "        else:\n",
    "            data_split = 'VALIDATE'\n",
    "    return data_split\n",
    "\n",
    "\n",
    "def to_datetime(event_time):\n",
    "    if isinstance(event_time, str):\n",
    "        # In BigQuery, this is a datetime.datetime.  In JSON, it's a string\n",
    "        # sometimes it has a T separating the date, sometimes it doesn't\n",
    "        # Handle all the possibilities\n",
    "        event_time = dt.datetime.strptime(event_time.replace('T', ' '), DATETIME_FORMAT)\n",
    "    return event_time\n",
    "\n",
    "\n",
    "def approx_miles_between(lat1, lon1, lat2, lon2):\n",
    "    # convert to radians\n",
    "    lat1 = float(lat1) * np.pi / 180.0\n",
    "    lat2 = float(lat2) * np.pi / 180.0\n",
    "    lon1 = float(lon1) * np.pi / 180.0\n",
    "    lon2 = float(lon2) * np.pi / 180.0\n",
    "\n",
    "    # apply Haversine formula\n",
    "    d_lat = lat2 - lat1\n",
    "    d_lon = lon2 - lon1\n",
    "    a = (pow(np.sin(d_lat / 2), 2) +\n",
    "         pow(np.sin(d_lon / 2), 2) *\n",
    "         np.cos(lat1) * np.cos(lat2));\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return float(6371 * c * 0.621371)  # miles\n",
    "\n",
    "\n",
    "def create_features_and_label(event, for_training):\n",
    "    try:\n",
    "        model_input = {}\n",
    "\n",
    "        if for_training:\n",
    "            model_input.update({\n",
    "                'ontime': 1.0 if float(event['ARR_DELAY'] or 0) < 15 else 0,\n",
    "            })\n",
    "\n",
    "        # features for both training and prediction\n",
    "        model_input.update({\n",
    "            # same as in ch9\n",
    "            'dep_delay': event['DEP_DELAY'],\n",
    "            'taxi_out': event['TAXI_OUT'],\n",
    "            # distance is not in wheelsoff\n",
    "            'distance': approx_miles_between(\n",
    "                event['DEP_AIRPORT_LAT'], event['DEP_AIRPORT_LON'],\n",
    "                event['ARR_AIRPORT_LAT'], event['ARR_AIRPORT_LON']),\n",
    "            'origin': event['ORIGIN'],\n",
    "            'dest': event['DEST'],\n",
    "            'dep_hour': to_datetime(event['DEP_TIME']).hour,\n",
    "            'is_weekday': 1.0 if to_datetime(event['DEP_TIME']).isoweekday() < 6 else 0.0,\n",
    "            'carrier': event['UNIQUE_CARRIER'],\n",
    "            'dep_airport_lat': event['DEP_AIRPORT_LAT'],\n",
    "            'dep_airport_lon': event['DEP_AIRPORT_LON'],\n",
    "            'arr_airport_lat': event['ARR_AIRPORT_LAT'],\n",
    "            'arr_airport_lon': event['ARR_AIRPORT_LON'],\n",
    "            # newly computed averages\n",
    "            'avg_dep_delay': event['AVG_DEP_DELAY'],\n",
    "            'avg_taxi_out': event['AVG_TAXI_OUT'],\n",
    "\n",
    "        })\n",
    "\n",
    "        if for_training:\n",
    "            model_input.update({\n",
    "                # training data split\n",
    "                'data_split': get_data_split(event['FL_DATE'])\n",
    "            })\n",
    "        else:\n",
    "            model_input.update({\n",
    "                # prediction output should include timestamp\n",
    "                'event_time': event['WHEELS_OFF']\n",
    "            })\n",
    "\n",
    "        yield model_input\n",
    "    except Exception as e:\n",
    "        # if any key is not present, don't use for training\n",
    "        logging.warning('Ignoring {} because: {}'.format(event, e), exc_info=True)\n",
    "        pass\n",
    "\n",
    "\n",
    "def compute_mean(events, col_name):\n",
    "    values = [float(event[col_name]) for event in events \n",
    "              if col_name in event and event[col_name]]\n",
    "    return float(np.mean(values)) if len(values) > 0 else None\n",
    "\n",
    "\n",
    "def add_stats(element, window=beam.DoFn.WindowParam):\n",
    "    # result of a group-by, so this will be called once for each airport and window\n",
    "    # all averages here are by airport\n",
    "    airport = element[0]\n",
    "    events = element[1]\n",
    "\n",
    "    # how late are flights leaving?\n",
    "    avg_dep_delay = compute_mean(events, 'DEP_DELAY')\n",
    "    avg_taxiout = compute_mean(events, 'TAXI_OUT')\n",
    "\n",
    "    # remember that an event will be present for 60 minutes, but we want to emit\n",
    "    # it only if it has just arrived (if it is within 5 minutes of the start of the window)\n",
    "    emit_end_time = window.start + WINDOW_EVERY\n",
    "    for event in events:\n",
    "        event_time = to_datetime(event['WHEELS_OFF']).timestamp()\n",
    "        if event_time < emit_end_time:\n",
    "            event_plus_stat = event.copy()\n",
    "            event_plus_stat['AVG_DEP_DELAY'] = avg_dep_delay\n",
    "            event_plus_stat['AVG_TAXI_OUT'] = avg_taxiout\n",
    "            yield event_plus_stat\n",
    "\n",
    "\n",
    "def assign_timestamp(event):\n",
    "    try:\n",
    "        event_time = to_datetime(event['WHEELS_OFF'])\n",
    "        yield beam.window.TimestampedValue(event, event_time.timestamp())\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def is_normal_operation(event):\n",
    "    for flag in ['CANCELLED', 'DIVERTED']:\n",
    "        if flag in event:\n",
    "            s = str(event[flag]).lower()\n",
    "            if s == 'true':\n",
    "                return False;  # cancelled or diverted\n",
    "    return True  # normal operation\n",
    "\n",
    "\n",
    "def transform_events_to_features(events, for_training=True):\n",
    "    # events are assigned the time at which predictions will have to be made \n",
    "    # -- the wheels off time\n",
    "    events = events | 'assign_time' >> beam.FlatMap(assign_timestamp)\n",
    "    events = events | 'remove_cancelled' >> beam.Filter(is_normal_operation)\n",
    "\n",
    "    # compute stats by airport, and add to events\n",
    "    features = (\n",
    "            events\n",
    "            | 'window' >> beam.WindowInto(beam.window.SlidingWindows(\n",
    "                WINDOW_DURATION, WINDOW_EVERY))\n",
    "            | 'by_airport' >> beam.Map(lambda x: (x['ORIGIN'], x))\n",
    "            | 'group_by_airport' >> beam.GroupByKey()\n",
    "            | 'events_and_stats' >> beam.FlatMap(add_stats)\n",
    "            | 'events_to_features' >> beam.FlatMap(\n",
    "                lambda x: create_features_and_label(x, for_training))\n",
    "    )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e1230-4388-4ef1-bd47-d9fd94a559c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_traindata.py\n",
    "#!/usr/bin/env python3\n",
    "import apache_beam as beam\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "\n",
    "from flightstxf import flights_transforms as ftxf\n",
    "\n",
    "CSV_HEADER = 'ontime,dep_delay,taxi_out,distance,origin,dest,dep_hour,is_weekday,carrier,\\\n",
    "dep_airport_lat,dep_airport_lon,arr_airport_lat,arr_airport_lon,avg_dep_delay,\\\n",
    "avg_taxi_out,data_split'\n",
    "\n",
    "\n",
    "def dict_to_csv(f):\n",
    "    try:\n",
    "        yield ','.join([str(x) for x in f.values()])\n",
    "    except Exception as e:\n",
    "        logging.warning('Ignoring {} because: {}'.format(f, e), exc_info=True)\n",
    "        pass\n",
    "\n",
    "\n",
    "def run(project, bucket, region, input):\n",
    "    if input == 'local':\n",
    "        logging.info('Running locally on small extract')\n",
    "        argv = [\n",
    "            '--runner=DirectRunner'\n",
    "        ]\n",
    "        flights_output = '/tmp/'\n",
    "    else:\n",
    "        logging.info('Running in the cloud on full dataset input={}'.format(input))\n",
    "        argv = [\n",
    "            '--project={0}'.format(project),\n",
    "            '--job_name=ch11traindata',\n",
    "            # '--save_main_session', # not needed as we are running as a package now\n",
    "            '--staging_location=gs://{0}/flights/staging/'.format(bucket),\n",
    "            '--temp_location=gs://{0}/flights/temp/'.format(bucket),\n",
    "            '--setup_file=./setup.py',\n",
    "            '--autoscaling_algorithm=THROUGHPUT_BASED',\n",
    "            '--max_num_workers=20',\n",
    "            # '--max_num_workers=4', '--worker_machine_type=m1-ultramem-40', \n",
    "            # '--disk_size_gb=500',  # for full 2015-2019 dataset\n",
    "            '--region={}'.format(region),\n",
    "            '--runner=DataflowRunner'\n",
    "        ]\n",
    "        flights_output = 'gs://{}/ch11/data/'.format(bucket)\n",
    "\n",
    "    with beam.Pipeline(argv=argv) as pipeline:\n",
    "\n",
    "        # read the event stream\n",
    "        if input == 'local':\n",
    "            input_file = './alldata_sample.json'\n",
    "            logging.info(\"Reading from {} ... Writing to {}\".format(\n",
    "                input_file, flights_output))\n",
    "            events = (\n",
    "                    pipeline\n",
    "                    | 'read_input' >> beam.io.ReadFromText(input_file)\n",
    "                    | 'parse_input' >> beam.Map(lambda line: json.loads(line))\n",
    "            )\n",
    "        elif input == 'bigquery':\n",
    "            input_table = 'dsongcp.flights_tzcorr'\n",
    "            logging.info(\"Reading from {} ... Writing to {}\".format(\n",
    "                input_table, flights_output))\n",
    "            events = (\n",
    "                    pipeline\n",
    "                    | 'read_input' >> beam.io.ReadFromBigQuery(table=input_table)\n",
    "            )\n",
    "        else:\n",
    "            logging.error(\"Unknown input type {}\".format(input))\n",
    "            return\n",
    "\n",
    "        # events -> features.  See ./flights_transforms.py for the code shared \n",
    "        # between training & prediction\n",
    "        features = ftxf.transform_events_to_features(events)\n",
    "\n",
    "        # shuffle globally so that we are not at mercy of TensorFlow's shuffle buffer\n",
    "        features = (\n",
    "            features\n",
    "            | 'into_global' >> beam.WindowInto(beam.window.GlobalWindows())\n",
    "            | 'shuffle' >> beam.util.Reshuffle()\n",
    "        )\n",
    "\n",
    "        # write out\n",
    "        for split in ['ALL', 'TRAIN', 'VALIDATE', 'TEST']:\n",
    "            feats = features\n",
    "            if split != 'ALL':\n",
    "                feats = feats | 'only_{}'.format(split) >> beam.Filter(\n",
    "                    lambda f: f['data_split'] == split)\n",
    "            (\n",
    "                feats\n",
    "                | '{}_to_string'.format(split) >> beam.FlatMap(dict_to_csv)\n",
    "                | '{}_to_gcs'.format(split) >> beam.io.textio.WriteToText(\n",
    "                    os.path.join(flights_output, split.lower()),\n",
    "                    file_name_suffix='.csv', header=CSV_HEADER,\n",
    "                    # workaround b/207384805\n",
    "                    num_shards=1)\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Create training CSV file that includes time-aggregate features')\n",
    "    parser.add_argument('-p', '--project', \n",
    "        help='Project to be billed for Dataflow job. Omit if running locally.')\n",
    "    parser.add_argument('-b', '--bucket', \n",
    "        help='Training data will be written to gs://BUCKET/flights/ch11/')\n",
    "    parser.add_argument('-r', '--region', \n",
    "        help='Region to run Dataflow job. Choose the same region as your bucket.')\n",
    "    parser.add_argument('-i', '--input', \n",
    "        help='local OR bigquery', required=True)\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    args = vars(parser.parse_args())\n",
    "\n",
    "    if args['input'] != 'local':\n",
    "        if not args['bucket'] or not args['project'] or not args['region']:\n",
    "            print(\"Project, Bucket, Region are needed in order to run \\\n",
    "                on the cloud on full dataset.\")\n",
    "            parser.print_help()\n",
    "            parser.exit()\n",
    "\n",
    "    run(project=args['project'], bucket=args['bucket'], \n",
    "        region=args['region'], input=args['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dfe19e-038f-47e3-8db7-e5903d76faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PROJECT_ID=$(gcloud info --format='value(config.project)')\n",
    "export BUCKET=$PROJECT_ID-ml\n",
    "python3 create_traindata.py --input bigquery --project $PROJECT_ID \\\n",
    "--bucket $BUCKET --region us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38ed426-2697-447e-b7da-40c6323bf396",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Run a script to train the model. Train custom ML model on the enriched dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e429dbd-9a41-4c35-b486-87caa8d554a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile change_ch10_files.py\n",
    "import os\n",
    "\n",
    "CHANGES = [\n",
    "    # both\n",
    "    (\"ch9\", \"ch11\"),\n",
    "\n",
    "    # train_on_vertexai.py\n",
    "    (\"ENDPOINT_NAME = 'flights'\", \"ENDPOINT_NAME = 'flights-ch11'\"),\n",
    "\n",
    "    # model.py\n",
    "    (\"arr_airport_lat,arr_airport_lon\", \"arr_airport_lat,arr_airport_lon,\\\n",
    "        avg_dep_delay,avg_taxi_out\"),\n",
    "    (\"43.41694444, -124.24694444, 39.86166667, -104.67305556, 'TRAIN'\",\n",
    "     \"43.41694444, -124.24694444, 39.86166667, -104.67305556, -3.0, 5.0, 'TRAIN'\"),\n",
    "\n",
    "    # call_predict.py\n",
    "    ('\"carrier\": \"AS\"', '\"carrier\": \"AS\", \"avg_dep_delay\": -3.0, \"avg_taxi_out\": 5.0'),\n",
    "    ('\"carrier\": \"HA\"', '\"carrier\": \"HA\", \"avg_dep_delay\": 3.0, \"avg_taxi_out\": 8.0'),\n",
    "]\n",
    "\n",
    "for filename in ['train_on_vertexai.py', 'model.py', 'call_predict.py']:\n",
    "    in_filename = os.path.join('../10_mlops', filename)\n",
    "    with open(in_filename, \"r\") as ifp:\n",
    "        with open(filename, \"w\") as ofp:\n",
    "            ofp.write(\"#### DO NOT EDIT! Autogenerated from {}\".format(in_filename))\n",
    "            for line in ifp.readlines():\n",
    "                for change in CHANGES:\n",
    "                    new_line = line.replace(change[0], change[1])\n",
    "                    if new_line != line:\n",
    "                        print('<<' + line + '>>' + new_line)\n",
    "                        line = new_line\n",
    "                ofp.write(line)\n",
    "\n",
    "    print(\"*** Wrote out {}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67c440-7884-48d6-8692-53ca057feec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import argparse\n",
    "import logging\n",
    "import os, time\n",
    "import hypertune\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "BUCKET = None\n",
    "TF_VERSION = '2-' + tf.__version__[2:3]  # needed to choose container\n",
    "\n",
    "DEVELOP_MODE = True\n",
    "NUM_EXAMPLES = 5000 * 1000  # doesn't need to be precise but get order of magnitude right.\n",
    "\n",
    "NUM_BUCKETS = 5\n",
    "NUM_EMBEDS = 3\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "DNN_HIDDEN_UNITS = '64,32'\n",
    "\n",
    "CSV_COLUMNS = (\n",
    "    'ontime,dep_delay,taxi_out,distance,origin,dest,dep_hour,is_weekday,carrier,' +\n",
    "    'dep_airport_lat,dep_airport_lon,arr_airport_lat,arr_airport_lon,data_split'\n",
    ").split(',')\n",
    "\n",
    "CSV_COLUMN_TYPES = [\n",
    "    1.0, -3.0, 5.0, 1037.493622678299, 'OTH', 'DEN', 21, 1.0, 'OO',\n",
    "    43.41694444, -124.24694444, 39.86166667, -104.67305556, 'TRAIN'\n",
    "]\n",
    "\n",
    "\n",
    "def features_and_labels(features):\n",
    "    label = features.pop('ontime')  # this is what we will train for\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def read_dataset(pattern, batch_size, mode=tf.estimator.ModeKeys.TRAIN, truncate=None):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=CSV_COLUMN_TYPES,\n",
    "        sloppy=True,\n",
    "        num_parallel_reads=2,\n",
    "        ignore_errors=True,\n",
    "        num_epochs=1)\n",
    "    dataset = dataset.map(features_and_labels)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(batch_size * 10)\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(1)\n",
    "    if truncate is not None:\n",
    "        dataset = dataset.take(truncate)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    real = {\n",
    "        colname: tf.feature_column.numeric_column(colname)\n",
    "        for colname in\n",
    "        (\n",
    "                'dep_delay,taxi_out,distance,dep_hour,is_weekday,' +\n",
    "                'dep_airport_lat,dep_airport_lon,' +\n",
    "                'arr_airport_lat,arr_airport_lon'\n",
    "        ).split(',')\n",
    "    }\n",
    "    sparse = {\n",
    "        'carrier': tf.feature_column.categorical_column_with_vocabulary_list('carrier',\n",
    "            vocabulary_list='AS,VX,F9,UA,US,WN,HA,EV,MQ,DL,OO,B6,NK,AA'.split(',')),\n",
    "        'origin': tf.feature_column.categorical_column_with_hash_bucket('origin', \n",
    "            hash_bucket_size=1000),\n",
    "        'dest': tf.feature_column.categorical_column_with_hash_bucket('dest', \n",
    "            hash_bucket_size=1000),\n",
    "    }\n",
    "\n",
    "    inputs = {\n",
    "        colname: tf.keras.layers.Input(name=colname, shape=(), dtype='float32')\n",
    "        for colname in real.keys()\n",
    "    }\n",
    "    inputs.update({\n",
    "        colname: tf.keras.layers.Input(name=colname, shape=(), dtype='string')\n",
    "        for colname in sparse.keys()\n",
    "    })\n",
    "\n",
    "    latbuckets = np.linspace(20.0, 50.0, NUM_BUCKETS).tolist()  # USA\n",
    "    lonbuckets = np.linspace(-120.0, -70.0, NUM_BUCKETS).tolist()  # USA\n",
    "    disc = {}\n",
    "    disc.update({\n",
    "        'd_{}'.format(key): tf.feature_column.bucketized_column(real[key], latbuckets)\n",
    "        for key in ['dep_airport_lat', 'arr_airport_lat']\n",
    "    })\n",
    "    disc.update({\n",
    "        'd_{}'.format(key): tf.feature_column.bucketized_column(real[key], lonbuckets)\n",
    "        for key in ['dep_airport_lon', 'arr_airport_lon']\n",
    "    })\n",
    "\n",
    "    # cross columns that make sense in combination\n",
    "    sparse['dep_loc'] = tf.feature_column.crossed_column(\n",
    "        [disc['d_dep_airport_lat'], disc['d_dep_airport_lon']], NUM_BUCKETS * NUM_BUCKETS)\n",
    "    sparse['arr_loc'] = tf.feature_column.crossed_column(\n",
    "        [disc['d_arr_airport_lat'], disc['d_arr_airport_lon']], NUM_BUCKETS * NUM_BUCKETS)\n",
    "    sparse['dep_arr'] = tf.feature_column.crossed_column(\n",
    "        [sparse['dep_loc'], sparse['arr_loc']], NUM_BUCKETS ** 4)\n",
    "\n",
    "    # embed all the sparse columns\n",
    "    embed = {\n",
    "        'embed_{}'.format(colname): tf.feature_column.embedding_column(col, NUM_EMBEDS)\n",
    "        for colname, col in sparse.items()\n",
    "    }\n",
    "    real.update(embed)\n",
    "\n",
    "    # one-hot encode the sparse columns\n",
    "    sparse = {\n",
    "        colname: tf.feature_column.indicator_column(col)\n",
    "        for colname, col in sparse.items()\n",
    "    }\n",
    "\n",
    "    model = wide_and_deep_classifier(\n",
    "        inputs,\n",
    "        linear_feature_columns=sparse.values(),\n",
    "        dnn_feature_columns=real.values(),\n",
    "        dnn_hidden_units=DNN_HIDDEN_UNITS)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def wide_and_deep_classifier(inputs, linear_feature_columns, \n",
    "    dnn_feature_columns, dnn_hidden_units):\n",
    "    deep = tf.keras.layers.DenseFeatures(dnn_feature_columns, \n",
    "        name='deep_inputs')(inputs)\n",
    "    layers = [int(x) for x in dnn_hidden_units.split(',')]\n",
    "    for layerno, numnodes in enumerate(layers):\n",
    "        deep = tf.keras.layers.Dense(numnodes, activation='relu', \n",
    "            name='dnn_{}'.format(layerno + 1))(deep)\n",
    "    wide = tf.keras.layers.DenseFeatures(linear_feature_columns, \n",
    "            name='wide_inputs')(inputs)\n",
    "    both = tf.keras.layers.concatenate([deep, wide], name='both')\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid', name='pred')(both)\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "    model.compile(optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', rmse, tf.keras.metrics.AUC()])\n",
    "    return model\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def train_and_evaluate(train_data_pattern, eval_data_pattern, \n",
    "    test_data_pattern, export_dir, output_dir):\n",
    "    train_batch_size = TRAIN_BATCH_SIZE\n",
    "    if DEVELOP_MODE:\n",
    "        eval_batch_size = 100\n",
    "        steps_per_epoch = 3\n",
    "        epochs = 2\n",
    "        num_eval_examples = eval_batch_size * 10\n",
    "    else:\n",
    "        eval_batch_size = 100\n",
    "        steps_per_epoch = NUM_EXAMPLES // train_batch_size\n",
    "        epochs = NUM_EPOCHS\n",
    "        num_eval_examples = eval_batch_size * 100\n",
    "\n",
    "    train_dataset = read_dataset(train_data_pattern, train_batch_size)\n",
    "    eval_dataset = read_dataset(eval_data_pattern, eval_batch_size, \n",
    "        tf.estimator.ModeKeys.EVAL, num_eval_examples)\n",
    "\n",
    "    # checkpoint\n",
    "    checkpoint_path = '{}/checkpoints/flights.cpt'.format(output_dir)\n",
    "    logging.info(\"Checkpointing to {}\".format(checkpoint_path))\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "        save_weights_only=True, verbose=1)\n",
    "\n",
    "    # call back to write out hyperparameter tuning metric\n",
    "    METRIC = 'val_rmse'\n",
    "    hpt = hypertune.HyperTune()\n",
    "\n",
    "    class HpCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if logs and METRIC in logs:\n",
    "                logging.info(\"Epoch {}: {} = {}\".format(epoch, METRIC, logs[METRIC]))\n",
    "                hpt.report_hyperparameter_tuning_metric(hyperparameter_metric_tag=METRIC,\n",
    "                    metric_value=logs[METRIC], global_step=epoch)\n",
    "\n",
    "    # train the model\n",
    "    model = create_model()\n",
    "    logging.info(f\"Training on {train_data_pattern}; eval on {eval_data_pattern}; \n",
    "        {epochs} epochs; {steps_per_epoch}\")\n",
    "    history = model.fit(train_dataset,\n",
    "        validation_data=eval_dataset,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=[cp_callback, HpCallback()])\n",
    "\n",
    "    # export\n",
    "    logging.info('Exporting to {}'.format(export_dir))\n",
    "    tf.saved_model.save(model, export_dir)\n",
    "\n",
    "    # write out final metric\n",
    "    final_rmse = history.history[METRIC][-1]\n",
    "    logging.info(\"Validation metric {} on {} samples = {}\".format(\n",
    "        METRIC, num_eval_examples, final_rmse))\n",
    "\n",
    "    if (not DEVELOP_MODE) and (test_data_pattern is not None) and (not SKIP_FULL_EVAL):\n",
    "        logging.info(\"Evaluating over full test dataset\")\n",
    "        test_dataset = read_dataset(test_data_pattern, eval_batch_size, \n",
    "            tf.estimator.ModeKeys.EVAL, None)\n",
    "        final_metrics = model.evaluate(test_dataset)\n",
    "        logging.info(\"Final metrics on full test dataset = {}\".format(final_metrics))\n",
    "    else:\n",
    "        logging.info(\"Skipping evaluation on full test dataset\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.info(\"Tensorflow version \" + tf.__version__)\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--bucket',\n",
    "        help='Data will be read from gs://BUCKET/ch9/data and output will be in \\\n",
    "            gs://BUCKET/ch9/trained_model',\n",
    "        required=True\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--num_examples',\n",
    "        help='Number of examples per epoch. Get order of magnitude correct.',\n",
    "        type=int,\n",
    "        default=5000000\n",
    "    )\n",
    "\n",
    "    # for hyper-parameter tuning\n",
    "    parser.add_argument(\n",
    "        '--train_batch_size',\n",
    "        help='Number of examples to compute gradient on',\n",
    "        type=int,\n",
    "        default=256  # originally 64\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--nbuckets',\n",
    "        help='Number of bins into which to discretize lats and lons',\n",
    "        type=int,\n",
    "        default=10  # originally 5\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--nembeds',\n",
    "        help='Embedding dimension for categorical variables',\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_epochs',\n",
    "        help='Number of epochs (used only if --develop is not set)',\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--dnn_hidden_units',\n",
    "        help='Architecture of DNN part of wide-and-deep network',\n",
    "        default='64,64,64,8'  # originally '64,32'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--develop',\n",
    "        help='Train on a small subset in development',\n",
    "        dest='develop',\n",
    "        action='store_true')\n",
    "    parser.set_defaults(develop=False)\n",
    "    parser.add_argument(\n",
    "        '--skip_full_eval',\n",
    "        help='Just train. Do not evaluate on test dataset.',\n",
    "        dest='skip_full_eval',\n",
    "        action='store_true')\n",
    "    parser.set_defaults(skip_full_eval=False)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args().__dict__\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    # The Vertex AI contract. If not running in Vertex AI Training, these will be None\n",
    "    OUTPUT_MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")  # or None\n",
    "    TRAIN_DATA_PATTERN = os.getenv(\"AIP_TRAINING_DATA_URI\")\n",
    "    EVAL_DATA_PATTERN = os.getenv(\"AIP_VALIDATION_DATA_URI\")\n",
    "    TEST_DATA_PATTERN = os.getenv(\"AIP_TEST_DATA_URI\")\n",
    "\n",
    "    # set top-level output directory for checkpoints, etc.\n",
    "    BUCKET = args['bucket']\n",
    "    OUTPUT_DIR = 'gs://{}/ch9/train_output'.format(BUCKET)\n",
    "    # During hyperparameter tuning, we need to make sure different trials don't clobber each other\n",
    "    # https://cloud.google.com/ai-platform/training/docs/distributed-training-details#tf-config-format\n",
    "    # This doesn't exist in Vertex AI\n",
    "    # OUTPUT_DIR = os.path.join(\n",
    "    #     OUTPUT_DIR,\n",
    "    #     json.loads(\n",
    "    #         os.environ.get('TF_CONFIG', '{}')\n",
    "    #     ).get('task', {}).get('trial', '')\n",
    "    # )\n",
    "    if OUTPUT_MODEL_DIR:\n",
    "        # convert gs://ai-analytics-solutions-dsongcp2/aiplatform-custom-job-2021-11-13-22:22:46.175/1/model/\n",
    "        # to gs://ai-analytics-solutions-dsongcp2/aiplatform-custom-job-2021-11-13-22:22:46.175/1\n",
    "        OUTPUT_DIR = os.path.join(\n",
    "            os.path.dirname(OUTPUT_MODEL_DIR if OUTPUT_MODEL_DIR[-1] != '/' \n",
    "                else OUTPUT_MODEL_DIR[:-1]), 'train_output')\n",
    "    logging.info('Writing checkpoints and other outputs to {}'.format(OUTPUT_DIR))\n",
    "\n",
    "    # Set default values for the contract variables in case we are not running in Vertex AI Training\n",
    "    if not OUTPUT_MODEL_DIR:\n",
    "        OUTPUT_MODEL_DIR = os.path.join(OUTPUT_DIR,\n",
    "            'export/flights_{}'.format(time.strftime(\"%Y%m%d-%H%M%S\")))\n",
    "    if not TRAIN_DATA_PATTERN:\n",
    "        TRAIN_DATA_PATTERN = 'gs://{}/ch9/data/train*'.format(BUCKET)\n",
    "        CSV_COLUMNS.pop()  # the data_split column won't exist\n",
    "        CSV_COLUMN_TYPES.pop()  # the data_split column won't exist\n",
    "    if not EVAL_DATA_PATTERN:\n",
    "        EVAL_DATA_PATTERN = 'gs://{}/ch9/data/eval*'.format(BUCKET)\n",
    "    logging.info('Exporting trained model to {}'.format(OUTPUT_MODEL_DIR))\n",
    "    logging.info(\"Reading training data from {}\".format(TRAIN_DATA_PATTERN))\n",
    "    logging.info('Writing trained model to {}'.format(OUTPUT_MODEL_DIR))\n",
    "\n",
    "    # other global parameters\n",
    "    NUM_BUCKETS = args['nbuckets']\n",
    "    NUM_EMBEDS = args['nembeds']\n",
    "    NUM_EXAMPLES = args['num_examples']\n",
    "    NUM_EPOCHS = args['num_epochs']\n",
    "    TRAIN_BATCH_SIZE = args['train_batch_size']\n",
    "    DNN_HIDDEN_UNITS = args['dnn_hidden_units']\n",
    "    DEVELOP_MODE = args['develop']\n",
    "    SKIP_FULL_EVAL = args['skip_full_eval']\n",
    "\n",
    "    # run\n",
    "    train_and_evaluate(TRAIN_DATA_PATTERN, EVAL_DATA_PATTERN, \n",
    "        TEST_DATA_PATTERN, OUTPUT_MODEL_DIR, OUTPUT_DIR)\n",
    "\n",
    "    logging.info(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9678b0a-cdb1-4a99-8727-e7638ead2557",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_on_vertexai.py\n",
    "import argparse\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from kfp.v2 import compiler, dsl\n",
    "\n",
    "ENDPOINT_NAME = 'flights'\n",
    "\n",
    "\n",
    "def train_custom_model(data_set, timestamp, develop_mode, cpu_only_mode, \n",
    "    tf_version, extra_args=None):\n",
    "    # Set up training and deployment infra\n",
    "    \n",
    "    if cpu_only_mode:\n",
    "        train_image='us-docker.pkg.dev/vertex-ai/training/tf-cpu.{}:latest'.format(\n",
    "            tf_version)\n",
    "        deploy_image='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.{}:latest'.format(\n",
    "            tf_version)\n",
    "    else:\n",
    "        train_image = \"us-docker.pkg.dev/vertex-ai/training/tf-gpu.{}:latest\".format(\n",
    "            tf_version)\n",
    "        deploy_image = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.{}:latest\".format(\n",
    "            tf_version)\n",
    "\n",
    "    # train\n",
    "    model_display_name = '{}-{}'.format(ENDPOINT_NAME, timestamp)\n",
    "    job = aiplatform.CustomTrainingJob(\n",
    "        display_name='train-{}'.format(model_display_name),\n",
    "        script_path=\"model.py\",\n",
    "        container_uri=train_image,\n",
    "        requirements=['cloudml-hypertune'],  # any extra Python packages\n",
    "        model_serving_container_image_uri=deploy_image\n",
    "    )\n",
    "    model_args = [\n",
    "        '--bucket', BUCKET,\n",
    "    ]\n",
    "    if develop_mode:\n",
    "        model_args += ['--develop']\n",
    "    if extra_args:\n",
    "        model_args += extra_args\n",
    "    \n",
    "    if cpu_only_mode:\n",
    "        model = job.run(\n",
    "            dataset=data_set,\n",
    "            # See https://googleapis.dev/python/aiplatform/latest/aiplatform.html#\n",
    "            predefined_split_column_name='data_split',\n",
    "            model_display_name=model_display_name,\n",
    "            args=model_args,\n",
    "            replica_count=1,\n",
    "            machine_type='n1-standard-4',\n",
    "            sync=develop_mode\n",
    "        )\n",
    "    else:\n",
    "        model = job.run(\n",
    "            dataset=data_set,\n",
    "            # See https://googleapis.dev/python/aiplatform/latest/aiplatform.html#\n",
    "            predefined_split_column_name='data_split',\n",
    "            model_display_name=model_display_name,\n",
    "            args=model_args,\n",
    "            replica_count=1,\n",
    "            machine_type='n1-standard-4',\n",
    "            # See https://cloud.google.com/vertex-ai/docs/general/locations#accelerators\n",
    "            accelerator_type=aip.AcceleratorType.NVIDIA_TESLA_T4.name,\n",
    "            accelerator_count=1,\n",
    "            sync=develop_mode\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_automl_model(data_set, timestamp, develop_mode):\n",
    "    # train\n",
    "    model_display_name = '{}-{}'.format(ENDPOINT_NAME, timestamp)\n",
    "    job = aiplatform.AutoMLTabularTrainingJob(\n",
    "        display_name='train-{}'.format(model_display_name),\n",
    "        optimization_prediction_type='classification'\n",
    "    )\n",
    "    model = job.run(\n",
    "        dataset=data_set,\n",
    "        # See https://googleapis.dev/python/aiplatform/latest/aiplatform.html#\n",
    "        predefined_split_column_name='data_split',\n",
    "        target_column='ontime',\n",
    "        model_display_name=model_display_name,\n",
    "        budget_milli_node_hours=(300 if develop_mode else 2000),\n",
    "        disable_early_stopping=False,\n",
    "        export_evaluated_data_items=True,\n",
    "        export_evaluated_data_items_bigquery_destination_uri=\\\n",
    "            '{}:dsongcp.ch9_automl_evaluated'.format(PROJECT),\n",
    "        export_evaluated_data_items_override_destination=True,\n",
    "        sync=develop_mode\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def do_hyperparameter_tuning(data_set, timestamp, develop_mode, cpu_only_mode, tf_version):\n",
    "    # Vertex AI services require regional API endpoints.\n",
    "    if cpu_only_mode:\n",
    "        train_image='us-docker.pkg.dev/vertex-ai/training/tf-cpu.{}:latest'.format(\n",
    "            tf_version)\n",
    "    else: \n",
    "        train_image = \"us-docker.pkg.dev/vertex-ai/training/tf-gpu.{}:latest\".format(\n",
    "            tf_version)\n",
    "\n",
    "    # a single trial job\n",
    "    model_display_name = '{}-{}'.format(ENDPOINT_NAME, timestamp)\n",
    "    if cpu_only_mode:\n",
    "        trial_job = aiplatform.CustomJob.from_local_script(\n",
    "            display_name='train-{}'.format(model_display_name),\n",
    "            script_path=\"model.py\",\n",
    "            container_uri=train_image,\n",
    "            args=[\n",
    "                '--bucket', BUCKET,\n",
    "                '--skip_full_eval',  # no need to evaluate on test data set\n",
    "                '--num_epochs', '10',\n",
    "                '--num_examples', '500000'  # 1/10 actual size to finish faster\n",
    "            ],\n",
    "            requirements=['cloudml-hypertune'],  # any extra Python packages\n",
    "            replica_count=1,\n",
    "            machine_type='n1-standard-4'\n",
    "        )\n",
    "    else:\n",
    "        trial_job = aiplatform.CustomJob.from_local_script(\n",
    "            display_name='train-{}'.format(model_display_name),\n",
    "            script_path=\"model.py\",\n",
    "            container_uri=train_image,\n",
    "            args=[\n",
    "                '--bucket', BUCKET,\n",
    "                '--skip_full_eval',  # no need to evaluate on test data set\n",
    "                '--num_epochs', '10',\n",
    "                '--num_examples', '500000'  # 1/10 actual size to finish faster\n",
    "            ],\n",
    "            requirements=['cloudml-hypertune'],  # any extra Python packages\n",
    "            replica_count=1,\n",
    "            machine_type='n1-standard-4',\n",
    "            # See https://cloud.google.com/vertex-ai/docs/general/locations#accelerators\n",
    "            accelerator_type=aip.AcceleratorType.NVIDIA_TESLA_T4.name,\n",
    "            accelerator_count=1,\n",
    "        )\n",
    "\n",
    "    # the tuning job\n",
    "    hparam_job = aiplatform.HyperparameterTuningJob(\n",
    "        # See https://googleapis.dev/python/aiplatform/latest/aiplatform.html#\n",
    "        display_name='hparam-{}'.format(model_display_name),\n",
    "        custom_job=trial_job,\n",
    "        metric_spec={'val_rmse': 'minimize'},\n",
    "        parameter_spec={\n",
    "            \"train_batch_size\": hpt.IntegerParameterSpec(min=16, max=256, scale='log'),\n",
    "            \"nbuckets\": hpt.IntegerParameterSpec(min=5, max=10, scale='linear'),\n",
    "            \"dnn_hidden_units\": hpt.CategoricalParameterSpec(\n",
    "                values=[\"64,16\", \"64,16,4\", \"64,64,64,8\", \"256,64,16\"])\n",
    "        },\n",
    "        max_trial_count=2 if develop_mode else NUM_HPARAM_TRIALS,\n",
    "        parallel_trial_count=2,\n",
    "        search_algorithm=None,  # Bayesian\n",
    "    )\n",
    "\n",
    "    hparam_job.run(sync=True)  # has to finish before we can get trials.\n",
    "\n",
    "    # get the parameters corresponding to the best trial\n",
    "    best = sorted(hparam_job.trials, key=lambda x: x.final_measurement.metrics[0].value)[0]\n",
    "    logging.info('Best trial: {}'.format(best))\n",
    "    best_params = []\n",
    "    for param in best.parameters:\n",
    "        best_params.append('--{}'.format(param.parameter_id))\n",
    "\n",
    "        if param.parameter_id in [\"train_batch_size\", \"nbuckets\"]:\n",
    "            # hparam returns 10.0 even though it's an integer param. so round it.\n",
    "            # but CustomTrainingJob makes integer args into floats. so make it a string\n",
    "            best_params.append(str(int(round(param.value))))\n",
    "        else:\n",
    "            # string or float parameters\n",
    "            best_params.append(param.value)\n",
    "\n",
    "    # run the best trial to completion\n",
    "    logging.info('Launching full training job with {}'.format(best_params))\n",
    "    return train_custom_model(data_set, timestamp, develop_mode, \n",
    "        cpu_only_mode, tf_version, extra_args=best_params)\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=\"flights-ch9-pipeline\",\n",
    "              description=\"ds-on-gcp ch9 flights pipeline\"\n",
    ")\n",
    "def main():\n",
    "    aiplatform.init(project=PROJECT, location=REGION, \n",
    "        staging_bucket='gs://{}'.format(BUCKET))\n",
    "\n",
    "    # create data set\n",
    "    all_files = tf.io.gfile.glob('gs://{}/ch9/data/all*.csv'.format(BUCKET))\n",
    "    logging.info(\"Training on {}\".format(all_files))\n",
    "    data_set = aiplatform.TabularDataset.create(\n",
    "        display_name='data-{}'.format(ENDPOINT_NAME),\n",
    "        gcs_source=all_files\n",
    "    )\n",
    "    if TF_VERSION is not None:\n",
    "        tf_version = TF_VERSION.replace(\".\", \"-\")\n",
    "    else:\n",
    "        tf_version = '2-' + tf.__version__[2:3]\n",
    "\n",
    "    # train\n",
    "    if AUTOML:\n",
    "        model = train_automl_model(data_set, TIMESTAMP, DEVELOP_MODE)\n",
    "    elif NUM_HPARAM_TRIALS > 1:\n",
    "        model = do_hyperparameter_tuning(data_set, TIMESTAMP, DEVELOP_MODE, \n",
    "            CPU_ONLY_MODE, tf_version)\n",
    "    else:\n",
    "        model = train_custom_model(data_set, TIMESTAMP, DEVELOP_MODE, \n",
    "            CPU_ONLY_MODE, tf_version)\n",
    "\n",
    "    # create endpoint if it doesn't already exist\n",
    "    endpoints = aiplatform.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(ENDPOINT_NAME),\n",
    "        order_by='create_time desc',\n",
    "        project=PROJECT, location=REGION,\n",
    "    )\n",
    "    if len(endpoints) > 0:\n",
    "        endpoint = endpoints[0]  # most recently created\n",
    "    else:\n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=ENDPOINT_NAME, project=PROJECT, location=REGION,\n",
    "            sync=DEVELOP_MODE\n",
    "        )\n",
    "\n",
    "    # deploy\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        machine_type='n1-standard-2',\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1,\n",
    "        sync=DEVELOP_MODE\n",
    "    )\n",
    "\n",
    "    if DEVELOP_MODE:\n",
    "        model.wait()\n",
    "\n",
    "\n",
    "def run_pipeline():\n",
    "    compiler.Compiler().compile(pipeline_func=main, package_path='flights_pipeline.json')\n",
    "\n",
    "    job = aip.PipelineJob(\n",
    "        display_name=\"{}-pipeline\".format(ENDPOINT_NAME),\n",
    "        template_path=\"{}_pipeline.json\".format(ENDPOINT_NAME),\n",
    "        pipeline_root=\"{}/pipeline_root/intro\".format(BUCKET),\n",
    "        enable_caching=False\n",
    "    )\n",
    "\n",
    "    job.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--bucket',\n",
    "        help='Data will be read from gs://BUCKET/ch9/data and \\\n",
    "            checkpoints will be in gs://BUCKET/ch9/trained_model',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--region',\n",
    "        help='Where to run the trainer',\n",
    "        default='us-central1'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--project',\n",
    "        help='Project to be billed',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--develop',\n",
    "        help='Train on a small subset in development',\n",
    "        dest='develop',\n",
    "        action='store_true')\n",
    "    parser.set_defaults(develop=False)\n",
    "    parser.add_argument(\n",
    "        '--automl',\n",
    "        help='Train an AutoML Table, instead of using model.py',\n",
    "        dest='automl',\n",
    "        action='store_true')\n",
    "    parser.set_defaults(automl=False)\n",
    "    parser.add_argument(\n",
    "        '--num_hparam_trials',\n",
    "        help='Number of hyperparameter trials. 0/1 means no hyperparam. \\\n",
    "            Ignored if --automl is set.',\n",
    "        type=int,\n",
    "        default=0)\n",
    "    parser.add_argument(\n",
    "        '--pipeline',\n",
    "        help='Run as pipeline',\n",
    "        dest='pipeline',\n",
    "        action='store_true')\n",
    "    parser.add_argument(\n",
    "        '--cpuonly',\n",
    "        help='Run without GPU',\n",
    "        dest='cpuonly',\n",
    "        action='store_true')\n",
    "    parser.set_defaults(cpuonly=False)\n",
    "    parser.add_argument(\n",
    "        '--tfversion',\n",
    "        help='TensorFlow version to use'\n",
    "    )\n",
    "\n",
    "    # parse args\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    args = parser.parse_args().__dict__\n",
    "    BUCKET = args['bucket']\n",
    "    PROJECT = args['project']\n",
    "    REGION = args['region']\n",
    "    DEVELOP_MODE = args['develop']\n",
    "    CPU_ONLY_MODE = args['cpuonly']\n",
    "    TF_VERSION = args['tfversion']    \n",
    "    AUTOML = args['automl']\n",
    "    NUM_HPARAM_TRIALS = args['num_hparam_trials']\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    if args['pipeline']:\n",
    "        run_pipeline()\n",
    "    else:\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f63d1-0b98-4d76-abae-2c2016fdb76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile call_predict.py\n",
    "import sys, json\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "\n",
    "ENDPOINT_NAME = 'flights'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    endpoints = aiplatform.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(ENDPOINT_NAME),\n",
    "        order_by='create_time desc'\n",
    "    )\n",
    "    if len(endpoints) == 0:\n",
    "        print(\"No endpoint named {}\".format(ENDPOINT_NAME))\n",
    "        sys.exit(-1)\n",
    "    \n",
    "    endpoint = endpoints[0]\n",
    "    \n",
    "    input_data = {\"instances\": [\n",
    "        {\"dep_hour\": 2, \"is_weekday\": 1, \"dep_delay\": 40, \"taxi_out\": 17, \"distance\": 41, \n",
    "        \"carrier\": \"AS\", \"dep_airport_lat\": 58.42527778, \"dep_airport_lon\": -135.7075, \n",
    "        \"arr_airport_lat\": 58.35472222, \"arr_airport_lon\": -134.57472222, \"origin\": \"GST\", \n",
    "        \"dest\": \"JNU\"},\n",
    "        {\"dep_hour\": 22, \"is_weekday\": 0, \"dep_delay\": -7, \"taxi_out\": 7, \"distance\": 201, \n",
    "        \"carrier\": \"HA\", \"dep_airport_lat\": 21.97611111, \"dep_airport_lon\": -159.33888889, \n",
    "        \"arr_airport_lat\": 20.89861111, \"arr_airport_lon\": -156.43055556, \"origin\": \"LIH\", \n",
    "        \"dest\": \"OGG\"}\n",
    "    ]}\n",
    "\n",
    "    preds = endpoint.predict(input_data['instances'])\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194061b-1519-4b98-9cde-3a226574af89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 change_ch10_files.py\n",
    "\n",
    "python3 train_on_vertexai.py --project $PROJECT_ID  --bucket $BUCKET --region us-central1 \\\n",
    "--develop --cpuonly --tfversion=2.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c4b92b-b8e9-4ba5-a39a-ddac877430c1",
   "metadata": {},
   "source": [
    "Create a small, local sample of BigQuery datasets for local experimentation. Run a local pipeline to invoke predictions and verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c631674-5659-4337-acec-37526c334ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile make_predictions.py\n",
    "#!/usr/bin/env python3\n",
    "import apache_beam as beam\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "\n",
    "from flightstxf import flights_transforms as ftxf\n",
    "\n",
    "\n",
    "CSV_HEADER = 'event_time,dep_delay,taxi_out,distance,origin,dest,dep_hour,is_weekday,\\\n",
    "carrier,dep_airport_lat,dep_airport_lon,arr_airport_lat,arr_airport_lon,avg_dep_delay,\\\n",
    "avg_taxi_out,prob_ontime'\n",
    "\n",
    "\n",
    "# class FlightsModelSharedInvoker(beam.DoFn):\n",
    "#     # https://beam.apache.org/releases/pydoc/2.24.0/apache_beam.utils.shared.html\n",
    "#     def __init__(self, shared_handle):\n",
    "#         self._shared_handle = shared_handle\n",
    "#\n",
    "#     def process(self, input_data):\n",
    "#         def create_endpoint():\n",
    "#             from google.cloud import aiplatform\n",
    "#             endpoint_name = 'flights-ch10'\n",
    "#             endpoints = aiplatform.Endpoint.list(\n",
    "#                 filter='display_name=\"{}\"'.format(endpoint_name),\n",
    "#                 order_by='create_time desc'\n",
    "#             )\n",
    "#             if len(endpoints) == 0:\n",
    "#                 raise EnvironmentError(\"No endpoint named {}\".format(endpoint_name))\n",
    "#             logging.info(\"Found endpoint {}\".format(endpoints[0]))\n",
    "#             return endpoints[0]\n",
    "#\n",
    "#         # get already created endpoint if possible\n",
    "#         endpoint = self._shared_handle.acquire(create_endpoint)\n",
    "#\n",
    "#         # call predictions and pull out probability\n",
    "#         logging.info(\"Invoking ML model on {} flights\".format(len(input_data)))\n",
    "#         predictions = endpoint.predict(input_data).predictions\n",
    "#         for idx, input_instance in enumerate(input_data):\n",
    "#             result = input_instance.copy()\n",
    "#             result['prob_ontime'] = predictions[idx][0]\n",
    "#             yield result\n",
    "\n",
    "\n",
    "class FlightsModelInvoker(beam.DoFn):\n",
    "    def __init__(self):\n",
    "        self.endpoint = None\n",
    "\n",
    "    def setup(self):\n",
    "        from google.cloud import aiplatform\n",
    "        endpoint_name = 'flights-ch11'\n",
    "        endpoints = aiplatform.Endpoint.list(\n",
    "            filter='display_name=\"{}\"'.format(endpoint_name),\n",
    "            order_by='create_time desc'\n",
    "        )\n",
    "        if len(endpoints) == 0:\n",
    "            raise EnvironmentError(\"No endpoint named {}\".format(endpoint_name))\n",
    "        logging.info(\"Found endpoint {}\".format(endpoints[0]))\n",
    "        self.endpoint = endpoints[0]\n",
    "\n",
    "    def process(self, input_data):\n",
    "        # call predictions and pull out probability\n",
    "        logging.info(\"Invoking ML model on {} flights\".format(len(input_data)))\n",
    "        # drop inputs not needed by model\n",
    "        features = [x.copy() for x in input_data]\n",
    "        for f in features:\n",
    "            f.pop('event_time')\n",
    "        # call model\n",
    "        predictions = self.endpoint.predict(features).predictions\n",
    "        for idx, input_instance in enumerate(input_data):\n",
    "            result = input_instance.copy()\n",
    "            result['prob_ontime'] = predictions[idx][0]\n",
    "            yield result\n",
    "\n",
    "\n",
    "def run(project, bucket, region, source, sink):\n",
    "    if source == 'local':\n",
    "        logging.info('Running locally on small extract')\n",
    "        argv = [\n",
    "            '--project={0}'.format(project),\n",
    "            '--runner=DirectRunner'\n",
    "        ]\n",
    "        flights_output = '/tmp/predictions'\n",
    "    else:\n",
    "        logging.info('Running in the cloud on full dataset input={}'.format(source))\n",
    "        argv = [\n",
    "            '--project={0}'.format(project),\n",
    "            '--job_name=ch10predictions',\n",
    "            '--save_main_session',\n",
    "            '--staging_location=gs://{0}/flights/staging/'.format(bucket),\n",
    "            '--temp_location=gs://{0}/flights/temp/'.format(bucket),\n",
    "            '--setup_file=./setup.py',\n",
    "            '--autoscaling_algorithm=THROUGHPUT_BASED',\n",
    "            '--max_num_workers=8',\n",
    "            '--region={}'.format(region),\n",
    "            '--runner=DataflowRunner'\n",
    "        ]\n",
    "        if source == 'pubsub':\n",
    "            logging.info(\"Turning on streaming. Cancel the pipeline from GCP console\")\n",
    "            argv += ['--streaming']\n",
    "        flights_output = 'gs://{}/flights/ch11/predictions'.format(bucket)\n",
    "\n",
    "    with beam.Pipeline(argv=argv) as pipeline:\n",
    "\n",
    "        # read the event stream\n",
    "        if source == 'local':\n",
    "            input_file = './simevents_sample.json'\n",
    "            logging.info(\"Reading from {} ... Writing to {}\".format(\n",
    "                input_file, flights_output))\n",
    "            events = (\n",
    "                    pipeline\n",
    "                    | 'read_input' >> beam.io.ReadFromText(input_file)\n",
    "                    | 'parse_input' >> beam.Map(lambda line: json.loads(line))\n",
    "            )\n",
    "        elif source == 'bigquery':\n",
    "            input_query = (\"SELECT EVENT_DATA FROM dsongcp.flights_simevents \" +\n",
    "                \"WHERE EVENT_TIME BETWEEN '2015-03-01' AND '2015-03-02'\")\n",
    "            logging.info(\"Reading from {} ... Writing to {}\".format(\n",
    "                input_query, flights_output))\n",
    "            events = (\n",
    "                    pipeline\n",
    "                    | 'read_input' >> beam.io.ReadFromBigQuery(\n",
    "                        query=input_query, use_standard_sql=True)\n",
    "                    | 'parse_input' >> beam.Map(\n",
    "                        lambda row: json.loads(row['EVENT_DATA']))\n",
    "            )\n",
    "        elif source == 'pubsub':\n",
    "            input_topic = \"projects/{}/topics/wheelsoff\".format(project)\n",
    "            logging.info(\"Reading from {} ... Writing to {}\".format(\n",
    "                input_topic, flights_output))\n",
    "            events = (\n",
    "                    pipeline\n",
    "                    | 'read_input' >> beam.io.ReadFromPubSub(topic=input_topic,\n",
    "                        timestamp_attribute='EventTimeStamp')\n",
    "                    | 'parse_input' >> beam.Map(lambda s: json.loads(s))\n",
    "            )\n",
    "        else:\n",
    "            logging.error(\"Unknown input type {}\".format(source))\n",
    "            return\n",
    "\n",
    "        # events -> features.  See ./flights_transforms.py for the code shared between training & prediction\n",
    "        features = ftxf.transform_events_to_features(events, for_training=False)\n",
    "\n",
    "        # call model endpoint\n",
    "        # shared_handle = beam.utils.shared.Shared()\n",
    "        preds = (\n",
    "                features\n",
    "                | 'into_global' >> beam.WindowInto(beam.window.GlobalWindows())\n",
    "                | 'batch_instances' >> beam.BatchElements(\n",
    "                    min_batch_size=1, max_batch_size=64)\n",
    "                | 'model_predict' >> beam.ParDo(FlightsModelInvoker())\n",
    "        )\n",
    "\n",
    "        # write it out\n",
    "        if sink == 'file':\n",
    "            (preds\n",
    "                | 'to_string' >> beam.Map(lambda f: ','.join([str(x) for x in f.values()]))\n",
    "                | 'to_gcs' >> beam.io.textio.WriteToText(flights_output,\n",
    "                    file_name_suffix='.csv', header=CSV_HEADER,\n",
    "                    # workaround b/207384805\n",
    "                    num_shards=1)\n",
    "             )\n",
    "        elif sink == 'bigquery':\n",
    "            preds_schema = ','.join([\n",
    "                'event_time:timestamp',\n",
    "                'prob_ontime:float',\n",
    "                'dep_delay:float',\n",
    "                'taxi_out:float',\n",
    "                'distance:float',\n",
    "                'origin:string',\n",
    "                'dest:string',\n",
    "                'dep_hour:integer',\n",
    "                'is_weekday:integer',\n",
    "                'carrier:string',\n",
    "                'dep_airport_lat:float,dep_airport_lon:float',\n",
    "                'arr_airport_lat:float,arr_airport_lon:float',\n",
    "                'avg_dep_delay:float',\n",
    "                'avg_taxi_out:float',\n",
    "            ])\n",
    "            (preds\n",
    "                | 'to_bigquery' >> beam.io.WriteToBigQuery(\n",
    "                    'dsongcp.streaming_preds', schema=preds_schema,\n",
    "                    # write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "                    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                    method='STREAMING_INSERTS'\n",
    "                )\n",
    "             )\n",
    "        else:\n",
    "            logging.error(\"Unknown output type {}\".format(sink))\n",
    "            return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Create training CSV file that includes time-aggregate features')\n",
    "    parser.add_argument('-p', '--project', \n",
    "        help='Project to be billed for Dataflow/BigQuery', required=True)\n",
    "    parser.add_argument('-b', '--bucket', \n",
    "        help='data will be read from written to gs://BUCKET/flights/ch11/')\n",
    "    parser.add_argument('-r', '--region', \n",
    "        help='Region to run Dataflow job. Choose the same region as your bucket.')\n",
    "    parser.add_argument('-i', '--input', \n",
    "        help='local, bigquery OR pubsub', required=True)\n",
    "    parser.add_argument('-o', '--output', \n",
    "        help='file, bigquery OR bigtable', default='file')\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    args = vars(parser.parse_args())\n",
    "\n",
    "    if args['input'] != 'local':\n",
    "        if not args['bucket'] or not args['project'] or not args['region']:\n",
    "            print(\"Project, Bucket, Region are needed in order to \\\n",
    "                run on the cloud on full dataset.\")\n",
    "            parser.print_help()\n",
    "            parser.exit()\n",
    "\n",
    "    run(project=args['project'], bucket=args['bucket'], region=args['region'],\n",
    "        source=args['input'], sink=args['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c978fa-34f8-47b1-b84c-da1fbb645746",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bq query --nouse_legacy_sql --format=sparse \\\n",
    "    \"SELECT EVENT_DATA FROM dsongcp.flights_simevents \\\n",
    "    WHERE EVENT_TYPE = 'wheelsoff' \\\n",
    "    AND EVENT_TIME BETWEEN '2015-03-01' AND '2015-03-02'\" \\\n",
    "    | grep FL_DATE \\\n",
    "    > simevents_sample.json\n",
    "\n",
    "python3 make_predictions.py --input local -p $PROJECT_ID \n",
    "\n",
    "cat /tmp/predictions*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaccefd4-0845-485e-99ce-3939d6082711",
   "metadata": {},
   "source": [
    "## Stream data through pipeline\n",
    "\n",
    "Generates flight data events using the real flight data from 2015. Run the real-time prediction Dataflow job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72201c-4a20-4a8a-9d4f-b9b3655aa81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile simulate.py\n",
    "#!/usr/bin/env python3\n",
    "import time\n",
    "import pytz\n",
    "import logging\n",
    "import argparse\n",
    "import datetime\n",
    "import google.cloud.pubsub_v1 as pubsub # Use v1 of the API\n",
    "import google.cloud.bigquery as bq\n",
    "\n",
    "TIME_FORMAT = '%Y-%m-%d %H:%M:%S %Z'\n",
    "RFC3339_TIME_FORMAT = '%Y-%m-%dT%H:%M:%S-00:00'\n",
    "\n",
    "def publish(publisher, topics, allevents, notify_time):\n",
    "   timestamp = notify_time.strftime(RFC3339_TIME_FORMAT)\n",
    "   for key in topics:  # 'departed', 'arrived', etc.\n",
    "      topic = topics[key]\n",
    "      events = allevents[key]\n",
    "      # the client automatically batches\n",
    "      logging.info('Publishing {} {} till {}'.format(len(events), key, timestamp))\n",
    "      for event_data in events:\n",
    "          publisher.publish(topic, event_data.encode(), EventTimeStamp=timestamp)\n",
    "\n",
    "def notify(publisher, topics, rows, simStartTime, programStart, speedFactor):\n",
    "   # sleep computation\n",
    "   def compute_sleep_secs(notify_time):\n",
    "        time_elapsed = (datetime.datetime.utcnow() - programStart).total_seconds()\n",
    "        sim_time_elapsed = (notify_time - simStartTime).total_seconds() / speedFactor\n",
    "        to_sleep_secs = sim_time_elapsed - time_elapsed\n",
    "        return to_sleep_secs\n",
    "\n",
    "   tonotify = {}\n",
    "   for key in topics:\n",
    "     tonotify[key] = list()\n",
    "\n",
    "   for row in rows:\n",
    "       event_type, notify_time, event_data = row\n",
    "\n",
    "       # how much time should we sleep?\n",
    "       if compute_sleep_secs(notify_time) > 1:\n",
    "          # notify the accumulated tonotify\n",
    "          publish(publisher, topics, tonotify, notify_time)\n",
    "          for key in topics:\n",
    "             tonotify[key] = list()\n",
    "\n",
    "          # recompute sleep, since notification takes a while\n",
    "          to_sleep_secs = compute_sleep_secs(notify_time)\n",
    "          if to_sleep_secs > 0:\n",
    "             logging.info('Sleeping {} seconds'.format(to_sleep_secs))\n",
    "             time.sleep(to_sleep_secs)\n",
    "       tonotify[event_type].append(event_data)\n",
    "\n",
    "   # left-over records; notify again\n",
    "   publish(publisher, topics, tonotify, notify_time)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   parser = argparse.ArgumentParser(\n",
    "       description='Send simulated flight events to Cloud Pub/Sub')\n",
    "   parser.add_argument('--startTime', \n",
    "       help='Example: 2015-05-01 00:00:00 UTC', required=True)\n",
    "   parser.add_argument('--endTime', \n",
    "       help='Example: 2015-05-03 00:00:00 UTC', required=True)\n",
    "   parser.add_argument('--project', \n",
    "       help='your project id, to create pubsub topic', required=True)\n",
    "   parser.add_argument('--speedFactor', \n",
    "       help='Example: 60 implies 1 hour of data sent to Cloud Pub/Sub in 1 minute', \n",
    "       required=True, type=float)\n",
    "   parser.add_argument('--jitter', \n",
    "       help='type of jitter to add: None, uniform, exp  are the three options', \n",
    "       default='None')\n",
    "\n",
    "   # set up BigQuery bqclient\n",
    "   logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)\n",
    "   args = parser.parse_args()\n",
    "   bqclient = bq.Client(args.project)\n",
    "   bqclient.get_table('dsongcp.flights_simevents')  # throws exception on failure\n",
    "\n",
    "   # jitter?\n",
    "   if args.jitter == 'exp':\n",
    "      jitter = 'CAST (-LN(RAND()*0.99 + 0.01)*30 + 90.5 AS INT64)'\n",
    "   elif args.jitter == 'uniform':\n",
    "      jitter = 'CAST(90.5 + RAND()*30 AS INT64)'\n",
    "   else:\n",
    "      jitter = '0'\n",
    "\n",
    "\n",
    "   # run the query to pull simulated events\n",
    "   querystr = \"\"\"\n",
    "SELECT\n",
    "  EVENT_TYPE,\n",
    "  TIMESTAMP_ADD(EVENT_TIME, INTERVAL @jitter SECOND) AS NOTIFY_TIME,\n",
    "  EVENT_DATA\n",
    "FROM\n",
    "  dsongcp.flights_simevents\n",
    "WHERE\n",
    "  EVENT_TIME >= @startTime\n",
    "  AND EVENT_TIME < @endTime\n",
    "ORDER BY\n",
    "  EVENT_TIME ASC\n",
    "\"\"\"\n",
    "   job_config = bq.QueryJobConfig(\n",
    "       query_parameters=[\n",
    "           bq.ScalarQueryParameter(\"jitter\", \"INT64\", jitter),\n",
    "           bq.ScalarQueryParameter(\"startTime\", \"TIMESTAMP\", args.startTime),\n",
    "           bq.ScalarQueryParameter(\"endTime\", \"TIMESTAMP\", args.endTime),\n",
    "       ]\n",
    "   )\n",
    "   rows = bqclient.query(querystr, job_config=job_config)\n",
    "\n",
    "   # create one Pub/Sub notification topic for each type of event\n",
    "   publisher = pubsub.PublisherClient()\n",
    "   topics = {}\n",
    "   for event_type in ['wheelsoff', 'arrived', 'departed']:\n",
    "       topics[event_type] = publisher.topic_path(args.project, event_type)\n",
    "       try:\n",
    "           publisher.get_topic(topic=topics[event_type])\n",
    "           logging.info(\"Already exists: {}\".format(topics[event_type]))\n",
    "       except:\n",
    "           logging.info(\"Creating {}\".format(topics[event_type]))\n",
    "           publisher.create_topic(name=topics[event_type])\n",
    "\n",
    "\n",
    "   # notify about each row in the dataset\n",
    "   programStartTime = datetime.datetime.utcnow()\n",
    "   simStartTime = datetime.datetime.strptime(args.startTime, TIME_FORMAT)\\\n",
    "      .replace(tzinfo=pytz.UTC)\n",
    "   logging.info('Simulation start time is {}'.format(simStartTime))\n",
    "   notify(publisher, topics, rows, simStartTime, programStartTime, args.speedFactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79125b-e153-4b58-ac2a-0e0a6b5de44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd  ~/data-science-on-gcp/04_streaming/simulate\n",
    "export PROJECT_ID=$(gcloud info --format='value(config.project)')\n",
    "\n",
    "python3 ./simulate.py --startTime '2015-02-01 00:00:00 UTC' \\\n",
    "--endTime '2015-03-03 00:00:00 UTC' --speedFactor=30 --project $PROJECT_ID \n",
    "\n",
    "python3 make_predictions.py --input pubsub --output bigquery \\\n",
    "--project $PROJECT_ID --bucket $BUCKET --region us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8cc163-4806-4316-bf9b-0e7ac5b3e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bq\n",
    "SELECT * FROM dsongcp.streaming_preds ORDER BY event_time DESC LIMIT 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
