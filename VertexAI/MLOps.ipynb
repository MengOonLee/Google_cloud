{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6cc00-92d9-4a03-ba13-c4460e4963aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install google-cloud-aiplatform cloudml-hypertune kfp numpy tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acbc8b1-e58d-4c57-869a-569f6c699b82",
   "metadata": {},
   "source": [
    "## Run a standalone model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08936a68-3dbd-4dda-8521-2f60ffc98714",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model.py\n",
    "import argparse\n",
    "import logging\n",
    "import os, time\n",
    "import hypertune\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "BUCKET = None\n",
    "TF_VERSION = '2-' + tf.__version__[2:3]  # needed to choose container\n",
    "\n",
    "DEVELOP_MODE = True\n",
    "NUM_EXAMPLES = 5000 * 1000  # doesn't need to be precise but get order of magnitude right.\n",
    "\n",
    "NUM_BUCKETS = 5\n",
    "NUM_EMBEDS = 3\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "DNN_HIDDEN_UNITS = '64,32'\n",
    "\n",
    "CSV_COLUMNS = (\n",
    "    'ontime,dep_delay,taxi_out,distance,origin,dest,dep_hour,is_weekday,carrier,' +\n",
    "    'dep_airport_lat,dep_airport_lon,arr_airport_lat,arr_airport_lon,data_split'\n",
    ").split(',')\n",
    "\n",
    "CSV_COLUMN_TYPES = [\n",
    "    1.0, -3.0, 5.0, 1037.493622678299, 'OTH', 'DEN', 21, 1.0, 'OO',\n",
    "    43.41694444, -124.24694444, 39.86166667, -104.67305556, 'TRAIN'\n",
    "]\n",
    "\n",
    "\n",
    "def features_and_labels(features):\n",
    "    label = features.pop('ontime')  # this is what we will train for\n",
    "    return features, label\n",
    "\n",
    "\n",
    "def read_dataset(pattern, batch_size, mode=tf.estimator.ModeKeys.TRAIN, truncate=None):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        pattern, batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=CSV_COLUMN_TYPES,\n",
    "        sloppy=True,\n",
    "        num_parallel_reads=2,\n",
    "        ignore_errors=True,\n",
    "        num_epochs=1)\n",
    "    dataset = dataset.map(features_and_labels)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(batch_size * 10)\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(1)\n",
    "    if truncate is not None:\n",
    "        dataset = dataset.take(truncate)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    real = {\n",
    "        colname: tf.feature_column.numeric_column(colname)\n",
    "        for colname in\n",
    "        (\n",
    "                'dep_delay,taxi_out,distance,dep_hour,is_weekday,' +\n",
    "                'dep_airport_lat,dep_airport_lon,' +\n",
    "                'arr_airport_lat,arr_airport_lon'\n",
    "        ).split(',')\n",
    "    }\n",
    "    sparse = {\n",
    "        'carrier': tf.feature_column.categorical_column_with_vocabulary_list('carrier',\n",
    "                                                                             vocabulary_list='AS,VX,F9,UA,US,WN,HA,EV,MQ,DL,OO,B6,NK,AA'.split(\n",
    "                                                                                 ',')),\n",
    "        'origin': tf.feature_column.categorical_column_with_hash_bucket('origin', hash_bucket_size=1000),\n",
    "        'dest': tf.feature_column.categorical_column_with_hash_bucket('dest', hash_bucket_size=1000),\n",
    "    }\n",
    "\n",
    "    inputs = {\n",
    "        colname: tf.keras.layers.Input(name=colname, shape=(), dtype='float32')\n",
    "        for colname in real.keys()\n",
    "    }\n",
    "    inputs.update({\n",
    "        colname: tf.keras.layers.Input(name=colname, shape=(), dtype='string')\n",
    "        for colname in sparse.keys()\n",
    "    })\n",
    "\n",
    "    latbuckets = np.linspace(20.0, 50.0, NUM_BUCKETS).tolist()  # USA\n",
    "    lonbuckets = np.linspace(-120.0, -70.0, NUM_BUCKETS).tolist()  # USA\n",
    "    disc = {}\n",
    "    disc.update({\n",
    "        'd_{}'.format(key): tf.feature_column.bucketized_column(real[key], latbuckets)\n",
    "        for key in ['dep_airport_lat', 'arr_airport_lat']\n",
    "    })\n",
    "    disc.update({\n",
    "        'd_{}'.format(key): tf.feature_column.bucketized_column(real[key], lonbuckets)\n",
    "        for key in ['dep_airport_lon', 'arr_airport_lon']\n",
    "    })\n",
    "\n",
    "    # cross columns that make sense in combination\n",
    "    sparse['dep_loc'] = tf.feature_column.crossed_column(\n",
    "        [disc['d_dep_airport_lat'], disc['d_dep_airport_lon']], NUM_BUCKETS * NUM_BUCKETS)\n",
    "    sparse['arr_loc'] = tf.feature_column.crossed_column(\n",
    "        [disc['d_arr_airport_lat'], disc['d_arr_airport_lon']], NUM_BUCKETS * NUM_BUCKETS)\n",
    "    sparse['dep_arr'] = tf.feature_column.crossed_column([sparse['dep_loc'], sparse['arr_loc']], NUM_BUCKETS ** 4)\n",
    "\n",
    "    # embed all the sparse columns\n",
    "    embed = {\n",
    "        'embed_{}'.format(colname): tf.feature_column.embedding_column(col, NUM_EMBEDS)\n",
    "        for colname, col in sparse.items()\n",
    "    }\n",
    "    real.update(embed)\n",
    "\n",
    "    # one-hot encode the sparse columns\n",
    "    sparse = {\n",
    "        colname: tf.feature_column.indicator_column(col)\n",
    "        for colname, col in sparse.items()\n",
    "    }\n",
    "\n",
    "    model = wide_and_deep_classifier(\n",
    "        inputs,\n",
    "        linear_feature_columns=sparse.values(),\n",
    "        dnn_feature_columns=real.values(),\n",
    "        dnn_hidden_units=DNN_HIDDEN_UNITS)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def wide_and_deep_classifier(inputs, linear_feature_columns, dnn_feature_columns, dnn_hidden_units):\n",
    "    deep = tf.keras.layers.DenseFeatures(dnn_feature_columns, name='deep_inputs')(inputs)\n",
    "    layers = [int(x) for x in dnn_hidden_units.split(',')]\n",
    "    for layerno, numnodes in enumerate(layers):\n",
    "        deep = tf.keras.layers.Dense(numnodes, activation='relu', name='dnn_{}'.format(layerno + 1))(deep)\n",
    "    wide = tf.keras.layers.DenseFeatures(linear_feature_columns, name='wide_inputs')(inputs)\n",
    "    both = tf.keras.layers.concatenate([deep, wide], name='both')\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid', name='pred')(both)\n",
    "    model = tf.keras.Model(inputs, output)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', rmse, tf.keras.metrics.AUC()])\n",
    "    return model\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def train_and_evaluate(train_data_pattern, eval_data_pattern, test_data_pattern, export_dir, output_dir):\n",
    "    train_batch_size = TRAIN_BATCH_SIZE\n",
    "    if DEVELOP_MODE:\n",
    "        eval_batch_size = 100\n",
    "        steps_per_epoch = 3\n",
    "        epochs = 2\n",
    "        num_eval_examples = eval_batch_size * 10\n",
    "    else:\n",
    "        eval_batch_size = 100\n",
    "        steps_per_epoch = NUM_EXAMPLES // train_batch_size\n",
    "        epochs = NUM_EPOCHS\n",
    "        num_eval_examples = eval_batch_size * 100\n",
    "\n",
    "    train_dataset = read_dataset(train_data_pattern, train_batch_size)\n",
    "    eval_dataset = read_dataset(eval_data_pattern, eval_batch_size, tf.estimator.ModeKeys.EVAL, num_eval_examples)\n",
    "\n",
    "    # checkpoint\n",
    "    checkpoint_path = '{}/checkpoints/flights.cpt'.format(output_dir)\n",
    "    logging.info(\"Checkpointing to {}\".format(checkpoint_path))\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1)\n",
    "\n",
    "    # call back to write out hyperparameter tuning metric\n",
    "    METRIC = 'val_rmse'\n",
    "    hpt = hypertune.HyperTune()\n",
    "\n",
    "    class HpCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if logs and METRIC in logs:\n",
    "                logging.info(\"Epoch {}: {} = {}\".format(epoch, METRIC, logs[METRIC]))\n",
    "                hpt.report_hyperparameter_tuning_metric(hyperparameter_metric_tag=METRIC,\n",
    "                                                        metric_value=logs[METRIC],\n",
    "                                                        global_step=epoch)\n",
    "\n",
    "    # train the model\n",
    "    model = create_model()\n",
    "    logging.info(f\"Training on {train_data_pattern}; eval on {eval_data_pattern}; {epochs} epochs; {steps_per_epoch}\")\n",
    "    history = model.fit(train_dataset,\n",
    "                        validation_data=eval_dataset,\n",
    "                        epochs=epochs,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        callbacks=[cp_callback, HpCallback()])\n",
    "\n",
    "    # export\n",
    "    logging.info('Exporting to {}'.format(export_dir))\n",
    "    tf.saved_model.save(model, export_dir)\n",
    "\n",
    "    # write out final metric\n",
    "    final_rmse = history.history[METRIC][-1]\n",
    "    logging.info(\"Validation metric {} on {} samples = {}\".format(METRIC, num_eval_examples, final_rmse))\n",
    "\n",
    "    if (not DEVELOP_MODE) and (test_data_pattern is not None) and (not SKIP_FULL_EVAL):\n",
    "        logging.info(\"Evaluating over full test dataset\")\n",
    "        test_dataset = read_dataset(test_data_pattern, eval_batch_size, tf.estimator.ModeKeys.EVAL, None)\n",
    "        final_metrics = model.evaluate(test_dataset)\n",
    "        logging.info(\"Final metrics on full test dataset = {}\".format(final_metrics))\n",
    "    else:\n",
    "        logging.info(\"Skipping evaluation on full test dataset\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.info(\"Tensorflow version \" + tf.__version__)\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--bucket',\n",
    "        help='Data will be read from gs://BUCKET/ch9/data and output will be in gs://BUCKET/ch9/trained_model',\n",
    "        required=True\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--num_examples',\n",
    "        help='Number of examples per epoch. Get order of magnitude correct.',\n",
    "        type=int,\n",
    "        default=5000000\n",
    "    )\n",
    "\n",
    "    # for hyper-parameter tuning\n",
    "    parser.add_argument(\n",
    "        '--train_batch_size',\n",
    "        help='Number of examples to compute gradient on',\n",
    "        type=int,\n",
    "        default=256  # originally 64\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--nbuckets',\n",
    "        help='Number of bins into which to discretize lats and lons',\n",
    "        type=int,\n",
    "        default=10  # originally 5\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--nembeds',\n",
    "        help='Embedding dimension for categorical variables',\n",
    "        type=int,\n",
    "        default=3\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_epochs',\n",
    "        help='Number of epochs (used only if --develop is not set)',\n",
    "        type=int,\n",
    "        default=10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--dnn_hidden_units',\n",
    "        help='Architecture of DNN part of wide-and-deep network',\n",
    "        default='64,64,64,8'  # originally '64,32'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--develop',\n",
    "        help='Train on a small subset in development',\n",
    "        dest='develop',\n",
    "        action='store_true')\n",
    "    parser.set_defaults(develop=False)\n",
    "    parser.add_argument(\n",
    "        '--skip_full_eval',\n",
    "        help='Just train. Do not evaluate on test dataset.',\n",
    "        dest='skip_full_eval',\n",
    "        action='store_true')\n",
    "    parser.set_defaults(skip_full_eval=False)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args().__dict__\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    # The Vertex AI contract. If not running in Vertex AI Training, these will be None\n",
    "    OUTPUT_MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")  # or None\n",
    "    TRAIN_DATA_PATTERN = os.getenv(\"AIP_TRAINING_DATA_URI\")\n",
    "    EVAL_DATA_PATTERN = os.getenv(\"AIP_VALIDATION_DATA_URI\")\n",
    "    TEST_DATA_PATTERN = os.getenv(\"AIP_TEST_DATA_URI\")\n",
    "\n",
    "    # set top-level output directory for checkpoints, etc.\n",
    "    BUCKET = args['bucket']\n",
    "    OUTPUT_DIR = 'gs://{}/ch9/train_output'.format(BUCKET)\n",
    "    # During hyperparameter tuning, we need to make sure different trials don't clobber each other\n",
    "    # https://cloud.google.com/ai-platform/training/docs/distributed-training-details#tf-config-format\n",
    "    # This doesn't exist in Vertex AI\n",
    "    # OUTPUT_DIR = os.path.join(\n",
    "    #     OUTPUT_DIR,\n",
    "    #     json.loads(\n",
    "    #         os.environ.get('TF_CONFIG', '{}')\n",
    "    #     ).get('task', {}).get('trial', '')\n",
    "    # )\n",
    "    if OUTPUT_MODEL_DIR:\n",
    "        # convert gs://ai-analytics-solutions-dsongcp2/aiplatform-custom-job-2021-11-13-22:22:46.175/1/model/\n",
    "        # to gs://ai-analytics-solutions-dsongcp2/aiplatform-custom-job-2021-11-13-22:22:46.175/1\n",
    "        OUTPUT_DIR = os.path.join(\n",
    "            os.path.dirname(OUTPUT_MODEL_DIR if OUTPUT_MODEL_DIR[-1] != '/' else OUTPUT_MODEL_DIR[:-1]),\n",
    "            'train_output')\n",
    "    logging.info('Writing checkpoints and other outputs to {}'.format(OUTPUT_DIR))\n",
    "\n",
    "    # Set default values for the contract variables in case we are not running in Vertex AI Training\n",
    "    if not OUTPUT_MODEL_DIR:\n",
    "        OUTPUT_MODEL_DIR = os.path.join(OUTPUT_DIR,\n",
    "                                        'export/flights_{}'.format(time.strftime(\"%Y%m%d-%H%M%S\")))\n",
    "    if not TRAIN_DATA_PATTERN:\n",
    "        TRAIN_DATA_PATTERN = 'gs://{}/ch9/data/train*'.format(BUCKET)\n",
    "        CSV_COLUMNS.pop()  # the data_split column won't exist\n",
    "        CSV_COLUMN_TYPES.pop()  # the data_split column won't exist\n",
    "    if not EVAL_DATA_PATTERN:\n",
    "        EVAL_DATA_PATTERN = 'gs://{}/ch9/data/eval*'.format(BUCKET)\n",
    "    logging.info('Exporting trained model to {}'.format(OUTPUT_MODEL_DIR))\n",
    "    logging.info(\"Reading training data from {}\".format(TRAIN_DATA_PATTERN))\n",
    "    logging.info('Writing trained model to {}'.format(OUTPUT_MODEL_DIR))\n",
    "\n",
    "    # other global parameters\n",
    "    NUM_BUCKETS = args['nbuckets']\n",
    "    NUM_EMBEDS = args['nembeds']\n",
    "    NUM_EXAMPLES = args['num_examples']\n",
    "    NUM_EPOCHS = args['num_epochs']\n",
    "    TRAIN_BATCH_SIZE = args['train_batch_size']\n",
    "    DNN_HIDDEN_UNITS = args['dnn_hidden_units']\n",
    "    DEVELOP_MODE = args['develop']\n",
    "    SKIP_FULL_EVAL = args['skip_full_eval']\n",
    "\n",
    "    # run\n",
    "    train_and_evaluate(TRAIN_DATA_PATTERN, EVAL_DATA_PATTERN, TEST_DATA_PATTERN, OUTPUT_MODEL_DIR, OUTPUT_DIR)\n",
    "\n",
    "    logging.info(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48920345-508c-4664-a42f-214a382d6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PROJECT_ID=$(gcloud info --format='value(config.project)')\n",
    "export BUCKET_NAME=$PROJECT_ID-dsongcp\n",
    "python3 model.py --bucket $BUCKET_NAME --develop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caad181-a11a-42f3-99b7-47f5a4157227",
   "metadata": {},
   "source": [
    "## Develop and delopy model using Vertex AI\n",
    "\n",
    "- Load up a managed dataset in Vertex AI\n",
    "- Set up training infrastructure to run model.py\n",
    "- Train the model by invoking functions in model.py on the managed dataset\n",
    "- Find the endpoint to which to deploy the model\n",
    "- Deploy the model to the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e64362-7381-46d9-9659-8e73888f93dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_on_vertexai.py\n",
    "import argparse\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "from kfp.v2 import compiler, dsl\n",
    "\n",
    "ENDPOINT_NAME = 'flights'\n",
    "\n",
    "\n",
    "def train_custom_model(data_set, timestamp, develop_mode, cpu_only_mode, tf_version, extra_args=None):\n",
    "    # Set up training and deployment infra\n",
    "    \n",
    "    if cpu_only_mode:\n",
    "        train_image='us-docker.pkg.dev/vertex-ai/training/tf-cpu.{}:latest'.format(tf_version)\n",
    "        deploy_image='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.{}:latest'.format(tf_version)\n",
    "    else:\n",
    "        train_image = \"us-docker.pkg.dev/vertex-ai/training/tf-gpu.{}:latest\".format(tf_version)\n",
    "        deploy_image = \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.{}:latest\".format(tf_version)\n",
    "\n",
    "    # train\n",
    "    model_display_name = '{}-{}'.format(ENDPOINT_NAME, timestamp)\n",
    "    job = aiplatform.CustomTrainingJob(\n",
    "        display_name='train-{}'.format(model_display_name),\n",
    "        script_path=\"model.py\",\n",
    "        container_uri=train_image,\n",
    "        requirements=['cloudml-hypertune'],  # any extra Python packages\n",
    "        model_serving_container_image_uri=deploy_image\n",
    "    )\n",
    "    model_args = [\n",
    "        '--bucket', BUCKET,\n",
    "    ]\n",
    "    if develop_mode:\n",
    "        model_args += ['--develop']\n",
    "    if extra_args:\n",
    "        model_args += extra_args\n",
    "    \n",
    "    if cpu_only_mode:\n",
    "        model = job.run(\n",
    "            dataset=data_set,\n",
    "            # See https://googleapis.dev/python/aiplatform/latest/aiplatform.html#\n",
    "            predefined_split_column_name='data_split',\n",
    "            model_display_name=model_display_name,\n",
    "            args=model_args,\n",
    "            replica_count=1,\n",
    "            machine_type='n1-standard-4',\n",
    "            sync=develop_mode\n",
    "        )\n",
    "    else:\n",
    "        model = job.run(\n",
    "            dataset=data_set,\n",
    "            # See https://googleapis.dev/python/aiplatform/latest/aiplatform.html#\n",
    "            predefined_split_column_name='data_split',\n",
    "            model_display_name=model_display_name,\n",
    "            args=model_args,\n",
    "            replica_count=1,\n",
    "            machine_type='n1-standard-4',\n",
    "            # See https://cloud.google.com/vertex-ai/docs/general/locations#accelerators\n",
    "            accelerator_type=aip.AcceleratorType.NVIDIA_TESLA_T4.name,\n",
    "            accelerator_count=1,\n",
    "            sync=develop_mode\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_automl_model(data_set, timestamp, develop_mode):\n",
    "    # train\n",
    "    model_display_name = '{}-{}'.format(ENDPOINT_NAME, timestamp)\n",
    "    job = aiplatform.AutoMLTabularTrainingJob(\n",
    "        display_name='train-{}'.format(model_display_name),\n",
    "        optimization_prediction_type='classification'\n",
    "    )\n",
    "    model = job.run(\n",
    "        dataset=data_set,\n",
    "        # See https://googleapis.dev/python/aiplatform/latest/aiplatform.html#\n",
    "        predefined_split_column_name='data_split',\n",
    "        target_column='ontime',\n",
    "        model_display_name=model_display_name,\n",
    "        budget_milli_node_hours=(300 if develop_mode else 2000),\n",
    "        disable_early_stopping=False,\n",
    "        export_evaluated_data_items=True,\n",
    "        export_evaluated_data_items_bigquery_destination_uri='{}:dsongcp.ch9_automl_evaluated'.format(PROJECT),\n",
    "        export_evaluated_data_items_override_destination=True,\n",
    "        sync=develop_mode\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def do_hyperparameter_tuning(data_set, timestamp, develop_mode, cpu_only_mode, tf_version):\n",
    "    # Vertex AI services require regional API endpoints.\n",
    "    if cpu_only_mode:\n",
    "        train_image='us-docker.pkg.dev/vertex-ai/training/tf-cpu.{}:latest'.format(tf_version)\n",
    "    else: \n",
    "        train_image = \"us-docker.pkg.dev/vertex-ai/training/tf-gpu.{}:latest\".format(tf_version)\n",
    "\n",
    "    # a single trial job\n",
    "    model_display_name = '{}-{}'.format(ENDPOINT_NAME, timestamp)\n",
    "    if cpu_only_mode:\n",
    "        trial_job = aiplatform.CustomJob.from_local_script(\n",
    "            display_name='train-{}'.format(model_display_name),\n",
    "            script_path=\"model.py\",\n",
    "            container_uri=train_image,\n",
    "            args=[\n",
    "                '--bucket', BUCKET,\n",
    "                '--skip_full_eval',  # no need to evaluate on test data set\n",
    "                '--num_epochs', '10',\n",
    "                '--num_examples', '500000'  # 1/10 actual size to finish faster\n",
    "            ],\n",
    "            requirements=['cloudml-hypertune'],  # any extra Python packages\n",
    "            replica_count=1,\n",
    "            machine_type='n1-standard-4'\n",
    "        )\n",
    "    else:\n",
    "        trial_job = aiplatform.CustomJob.from_local_script(\n",
    "            display_name='train-{}'.format(model_display_name),\n",
    "            script_path=\"model.py\",\n",
    "            container_uri=train_image,\n",
    "            args=[\n",
    "                '--bucket', BUCKET,\n",
    "                '--skip_full_eval',  # no need to evaluate on test data set\n",
    "                '--num_epochs', '10',\n",
    "                '--num_examples', '500000'  # 1/10 actual size to finish faster\n",
    "            ],\n",
    "            requirements=['cloudml-hypertune'],  # any extra Python packages\n",
    "            replica_count=1,\n",
    "            machine_type='n1-standard-4',\n",
    "            # See https://cloud.google.com/vertex-ai/docs/general/locations#accelerators\n",
    "            accelerator_type=aip.AcceleratorType.NVIDIA_TESLA_T4.name,\n",
    "            accelerator_count=1,\n",
    "        )\n",
    "\n",
    "    # the tuning job\n",
    "    hparam_job = aiplatform.HyperparameterTuningJob(\n",
    "        # See https://googleapis.dev/python/aiplatform/latest/aiplatform.html#\n",
    "        display_name='hparam-{}'.format(model_display_name),\n",
    "        custom_job=trial_job,\n",
    "        metric_spec={'val_rmse': 'minimize'},\n",
    "        parameter_spec={\n",
    "            \"train_batch_size\": hpt.IntegerParameterSpec(min=16, max=256, scale='log'),\n",
    "            \"nbuckets\": hpt.IntegerParameterSpec(min=5, max=10, scale='linear'),\n",
    "            \"dnn_hidden_units\": hpt.CategoricalParameterSpec(values=[\"64,16\", \"64,16,4\", \"64,64,64,8\", \"256,64,16\"])\n",
    "        },\n",
    "        max_trial_count=2 if develop_mode else NUM_HPARAM_TRIALS,\n",
    "        parallel_trial_count=2,\n",
    "        search_algorithm=None,  # Bayesian\n",
    "    )\n",
    "\n",
    "    hparam_job.run(sync=True)  # has to finish before we can get trials.\n",
    "\n",
    "    # get the parameters corresponding to the best trial\n",
    "    best = sorted(hparam_job.trials, key=lambda x: x.final_measurement.metrics[0].value)[0]\n",
    "    logging.info('Best trial: {}'.format(best))\n",
    "    best_params = []\n",
    "    for param in best.parameters:\n",
    "        best_params.append('--{}'.format(param.parameter_id))\n",
    "\n",
    "        if param.parameter_id in [\"train_batch_size\", \"nbuckets\"]:\n",
    "            # hparam returns 10.0 even though it's an integer param. so round it.\n",
    "            # but CustomTrainingJob makes integer args into floats. so make it a string\n",
    "            best_params.append(str(int(round(param.value))))\n",
    "        else:\n",
    "            # string or float parameters\n",
    "            best_params.append(param.value)\n",
    "\n",
    "    # run the best trial to completion\n",
    "    logging.info('Launching full training job with {}'.format(best_params))\n",
    "    return train_custom_model(data_set, timestamp, develop_mode, cpu_only_mode, tf_version, extra_args=best_params)\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=\"flights-ch9-pipeline\",\n",
    "              description=\"ds-on-gcp ch9 flights pipeline\"\n",
    ")\n",
    "def main():\n",
    "    aiplatform.init(project=PROJECT, location=REGION, staging_bucket='gs://{}'.format(BUCKET))\n",
    "\n",
    "    # create data set\n",
    "    all_files = tf.io.gfile.glob('gs://{}/ch9/data/all*.csv'.format(BUCKET))\n",
    "    logging.info(\"Training on {}\".format(all_files))\n",
    "    data_set = aiplatform.TabularDataset.create(\n",
    "        display_name='data-{}'.format(ENDPOINT_NAME),\n",
    "        gcs_source=all_files\n",
    "    )\n",
    "    if TF_VERSION is not None:\n",
    "        tf_version = TF_VERSION.replace(\".\", \"-\")\n",
    "    else:\n",
    "        tf_version = '2-' + tf.__version__[2:3]\n",
    "\n",
    "    # train\n",
    "    if AUTOML:\n",
    "        model = train_automl_model(data_set, TIMESTAMP, DEVELOP_MODE)\n",
    "    elif NUM_HPARAM_TRIALS > 1:\n",
    "        model = do_hyperparameter_tuning(data_set, TIMESTAMP, DEVELOP_MODE, CPU_ONLY_MODE, tf_version)\n",
    "    else:\n",
    "        model = train_custom_model(data_set, TIMESTAMP, DEVELOP_MODE, CPU_ONLY_MODE, tf_version)\n",
    "\n",
    "    # create endpoint if it doesn't already exist\n",
    "    endpoints = aiplatform.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(ENDPOINT_NAME),\n",
    "        order_by='create_time desc',\n",
    "        project=PROJECT, location=REGION,\n",
    "    )\n",
    "    if len(endpoints) > 0:\n",
    "        endpoint = endpoints[0]  # most recently created\n",
    "    else:\n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=ENDPOINT_NAME, project=PROJECT, location=REGION,\n",
    "            sync=DEVELOP_MODE\n",
    "        )\n",
    "\n",
    "    # deploy\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        traffic_split={\"0\": 100},\n",
    "        machine_type='n1-standard-2',\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1,\n",
    "        sync=DEVELOP_MODE\n",
    "    )\n",
    "\n",
    "    if DEVELOP_MODE:\n",
    "        model.wait()\n",
    "\n",
    "\n",
    "def run_pipeline():\n",
    "    compiler.Compiler().compile(pipeline_func=main, package_path='flights_pipeline.json')\n",
    "\n",
    "    job = aip.PipelineJob(\n",
    "        display_name=\"{}-pipeline\".format(ENDPOINT_NAME),\n",
    "        template_path=\"{}_pipeline.json\".format(ENDPOINT_NAME),\n",
    "        pipeline_root=\"{}/pipeline_root/intro\".format(BUCKET),\n",
    "        enable_caching=False\n",
    "    )\n",
    "\n",
    "    job.run()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--bucket',\n",
    "        help='Data will be read from gs://BUCKET/ch9/data and checkpoints will be in gs://BUCKET/ch9/trained_model',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--region',\n",
    "        help='Where to run the trainer',\n",
    "        default='us-central1'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--project',\n",
    "        help='Project to be billed',\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--develop',\n",
    "        help='Train on a small subset in development',\n",
    "        dest='develop',\n",
    "        action='store_true')\n",
    "    parser.set_defaults(develop=False)\n",
    "    parser.add_argument(\n",
    "        '--automl',\n",
    "        help='Train an AutoML Table, instead of using model.py',\n",
    "        dest='automl',\n",
    "        action='store_true')\n",
    "    parser.set_defaults(automl=False)\n",
    "    parser.add_argument(\n",
    "        '--num_hparam_trials',\n",
    "        help='Number of hyperparameter trials. 0/1 means no hyperparam. Ignored if --automl is set.',\n",
    "        type=int,\n",
    "        default=0)\n",
    "    parser.add_argument(\n",
    "        '--pipeline',\n",
    "        help='Run as pipeline',\n",
    "        dest='pipeline',\n",
    "        action='store_true')\n",
    "    parser.add_argument(\n",
    "        '--cpuonly',\n",
    "        help='Run without GPU',\n",
    "        dest='cpuonly',\n",
    "        action='store_true')\n",
    "    parser.set_defaults(cpuonly=False)\n",
    "    parser.add_argument(\n",
    "        '--tfversion',\n",
    "        help='TensorFlow version to use'\n",
    "    )\n",
    "\n",
    "    # parse args\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    args = parser.parse_args().__dict__\n",
    "    BUCKET = args['bucket']\n",
    "    PROJECT = args['project']\n",
    "    REGION = args['region']\n",
    "    DEVELOP_MODE = args['develop']\n",
    "    CPU_ONLY_MODE = args['cpuonly']\n",
    "    TF_VERSION = args['tfversion']    \n",
    "    AUTOML = args['automl']\n",
    "    NUM_HPARAM_TRIALS = args['num_hparam_trials']\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    if args['pipeline']:\n",
    "        run_pipeline()\n",
    "    else:\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a7159-1c36-4ee4-9b85-4c28cc3b233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 train_on_vertexai.py \\\n",
    "--project $PROJECT_ID \\\n",
    "--bucket $BUCKET_NAME \\\n",
    "--develop --cpuonly --tfversion 2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a824b290-8ab0-4e02-96cc-a01552a8ce98",
   "metadata": {},
   "source": [
    "## Make predictions from the deployed model\n",
    "\n",
    "Sending the normal prediction request to the model endpoint will return a response that contains feature attributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870bc6f9-0f8d-42fb-bc0b-e8793d01dc9d",
   "metadata": {},
   "source": [
    "### Call the model using bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8614af-64f0-414d-8924-c9f8640a8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile call_predict.sh\n",
    "#!/bin/bash\n",
    "\n",
    "REGION=us-central1\n",
    "ENDPOINT_NAME=flights\n",
    "\n",
    "ENDPOINT_ID=$(gcloud ai endpoints list --region=$REGION \\\n",
    "              --format='value(ENDPOINT_ID)' --filter=display_name=${ENDPOINT_NAME} \\\n",
    "              --sort-by=creationTimeStamp | tail -1)\n",
    "echo $ENDPOINT_ID\n",
    "gcloud ai endpoints predict $ENDPOINT_ID --region=$REGION --json-request=example_input.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e50cd-e3c6-471c-8cb7-105adc785bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash ./call_predict.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b541f-47ed-404e-b5f2-06e439c8ee8e",
   "metadata": {},
   "source": [
    "### Call the model using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d29ea62-ee03-4ac6-a6ef-e31b55b2d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile call_predict.py\n",
    "import sys, json\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "\n",
    "ENDPOINT_NAME = 'flights'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    endpoints = aiplatform.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(ENDPOINT_NAME),\n",
    "        order_by='create_time desc'\n",
    "    )\n",
    "    if len(endpoints) == 0:\n",
    "        print(\"No endpoint named {}\".format(ENDPOINT_NAME))\n",
    "        sys.exit(-1)\n",
    "    \n",
    "    endpoint = endpoints[0]\n",
    "    \n",
    "    input_data = {\"instances\": [\n",
    "        {\"dep_hour\": 2, \"is_weekday\": 1, \"dep_delay\": 40, \"taxi_out\": 17, \"distance\": 41, \"carrier\": \"AS\",\n",
    "         \"dep_airport_lat\": 58.42527778, \"dep_airport_lon\": -135.7075, \"arr_airport_lat\": 58.35472222,\n",
    "         \"arr_airport_lon\": -134.57472222, \"origin\": \"GST\", \"dest\": \"JNU\"},\n",
    "        {\"dep_hour\": 22, \"is_weekday\": 0, \"dep_delay\": -7, \"taxi_out\": 7, \"distance\": 201, \"carrier\": \"HA\",\n",
    "         \"dep_airport_lat\": 21.97611111, \"dep_airport_lon\": -159.33888889, \"arr_airport_lat\": 20.89861111,\n",
    "         \"arr_airport_lon\": -156.43055556, \"origin\": \"LIH\", \"dest\": \"OGG\"}\n",
    "    ]}\n",
    "\n",
    "    preds = endpoint.predict(input_data['instances'])\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46918aa3-ea6f-4431-aac9-f5f0a68d087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 call_predict.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a16b5-b4bf-47cb-8552-8a8a31f412e1",
   "metadata": {},
   "source": [
    "### Run a Vertex AI pipeline on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ede6b-1d8a-47f6-a4c7-7b48e889a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 train_on_vertexai.py \\\n",
    "--project $PROJECT_ID \\\n",
    "--bucket $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aee3e1-9869-4af3-80f6-068bea68b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 train_on_vertexai.py \\\n",
    "--project $PROJECT_ID \\\n",
    "--bucket $BUCKET_NAME \\\n",
    "--tfversion 2.6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
