{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab: Continuous Training with TensorFlow, PyTorch, XGBoost, and Scikit-learn Models with KubeFlow and AI Platform Pipelines\n",
    "\n",
    "In this lab we will create containerized training applications for ML models in TensorFlow, PyTorch, XGBoost, and Scikit-learn. Will will then use these images as ops in a KubeFlow pipeline and train multiple models in parallel. We will then set up recurring runs of our KubeFlow pipeline in the UI. \n",
    "\n",
    "First, we will containerize models in TF, PyTorch, XGBoost and Scikit-learn following a step-wise process for each:\n",
    "* Create the training script\n",
    "* Package training script into a Docker Image \n",
    "* Build and push training image to Google Cloud Container Registry\n",
    "\n",
    "Once we have all four training images built and pushed to the Container Registry, we will build a KubeFlow pipeline that does two things:\n",
    "* Queries BigQuery to create training/validation splits and export results as sharded CSV files in GCS\n",
    "* Launches AI Platform training jobs with our four containerized training applications, using the exported CSV data as input \n",
    "\n",
    "Finally, we will compile and deploy our pipeline. In the UI we will set up Continuous Training with recurring pipeline runs.\n",
    "\n",
    "**PRIOR TO STARTING THE LAB:** Make sure you create a new instance with AI Platform Pipelines. Once the GKE cluster is spun up, copy the endpoint because you will need it in this lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install needed package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-bigquery-storage in /opt/conda/lib/python3.10/site-packages (2.25.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery-storage) (2.29.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery-storage) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery-storage) (3.20.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (1.63.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (1.63.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery-storage) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery-storage) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery-storage) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery-storage) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-bigquery-storage) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade google-cloud-bigquery-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by setting some global variables. Make sure you have created a bucket that is gs://PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "BUCKET = 'gs://' + PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a BigQuery dataset. We will then query a public BigQuery dataset to populate a table in this dataset. This is census data. We will use age, workclass, education, occupation, and hours per week to predict income bracket. Note: We also grab functional_weight in our query. We do not use this feature in our models, however we use it to hash on when creating training/validation splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'qwiklabs-gcp-04-71169a1ff84c:census' successfully created.\n"
     ]
    }
   ],
   "source": [
    "!bq --location=US mk census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c551ad5559764b58a3ff0f232d461339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "    \n",
    "CREATE OR REPLACE TABLE census.data AS\n",
    "    \n",
    "SELECT age, workclass, education_num, occupation, hours_per_week,income_bracket,functional_weight \n",
    "FROM `bigquery-public-data.ml_datasets.census_adult_income` \n",
    "WHERE AGE IS NOT NULL\n",
    "AND workclass IS NOT NULL\n",
    "AND education_num IS NOT NULL\n",
    "AND occupation IS NOT NULL\n",
    "AND hours_per_week IS NOT NULL\n",
    "AND income_bracket IS NOT NULL \n",
    "AND functional_weight IS NOT NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Scikit-learn Training Script\n",
    "\n",
    "We will develop our first training script with Scikit-learn. We will use Pandas to read the CSV data then train a simple [SGD Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir scikit_trainer_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./scikit_trainer_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scikit_trainer_image/train.py\n",
    "\n",
    "\"\"\"Census Scikit classifier trainer script.\"\"\"\n",
    "\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import fire\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def train_evaluate(training_dataset_path, validation_dataset_path,output_dir):\n",
    "    \"\"\"Trains the Census Classifier model.\"\"\"\n",
    "    \n",
    "    # Ingest data into Pandas Dataframes \n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "    df_train = pd.concat([df_train, df_validation])\n",
    "    \n",
    "    numeric_features = [\n",
    "        'age', 'education_num','hours_per_week'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = ['workclass', 'occupation']\n",
    "    \n",
    "    # Scale numeric features, one-hot encode categorical features\n",
    "    preprocessor = ColumnTransformer(transformers=[(\n",
    "        'num', StandardScaler(),\n",
    "        numeric_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)])\n",
    "    \n",
    "    pipeline = Pipeline([('preprocessor', preprocessor),\n",
    "                         ('classifier', SGDClassifier(loss='log'))])\n",
    "    \n",
    "    num_features_type_map = {feature: 'float64' for feature in numeric_features}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map)\n",
    "    \n",
    "    X_train = df_train.drop('income_bracket', axis=1)\n",
    "    y_train = df_train['income_bracket']\n",
    "    \n",
    "    # Set parameters of the model and fit\n",
    "    pipeline.set_params(classifier__alpha=0.0005, classifier__max_iter=250)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the model locally\n",
    "    model_filename = 'model.pkl'\n",
    "    with open(model_filename, 'wb') as model_file:\n",
    "        pickle.dump(pipeline, model_file)\n",
    "        \n",
    "    # Copy to model to GCS \n",
    "    EXPORT_PATH = os.path.join(\n",
    "        output_dir, datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    \n",
    "    gcs_model_path = '{}/{}'.format(EXPORT_PATH, model_filename)\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path])\n",
    "    print('Saved model in: {}'.format(gcs_model_path))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Scikit-learn training script into a Docker image\n",
    "The next step is to package this training script into a Docker image. We need to be sure to list the dependencies in this Dockerfile. For the Scikit-learn model we need to ensure scikit-learn version 0.23.2 and Pandas version 1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./scikit_trainer_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scikit_trainer_image/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu.py37\n",
    "RUN pip install -U fire scikit-learn==0.23.2 pandas==1.1.1\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Scikit-learn trainer image\n",
    "\n",
    "Now we will use Cloud Build to build the image and push it your project's Container Registry. Here we are using the remote cloud service to build the image, so we don't need a local installation of Docker. Note: Building and pushing the image will take a few minutes. Since we will be building and pushing 4 different images in this lab, I suggest taking a detailed look at the training scripts while you wait and make sure you understand the data ingestion/model building/training code for frameworks you develop with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCIKIT_IMAGE_NAME='scikit_trainer_image'\n",
    "SCIKIT_IMAGE_TAG='latest'\n",
    "SCIKIT_IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, SCIKIT_IMAGE_NAME, SCIKIT_IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 2 file(s) totalling 2.4 KiB before compression.\n",
      "Uploading tarball of [scikit_trainer_image] to [gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715919779.903176-10523e8ee17f469b8ad27ee997489c3f.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-04-71169a1ff84c/locations/global/builds/0096e2b5-491e-4146-abee-5860f53b25e7].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/0096e2b5-491e-4146-abee-5860f53b25e7?project=871018649403 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"0096e2b5-491e-4146-abee-5860f53b25e7\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715919779.903176-10523e8ee17f469b8ad27ee997489c3f.tgz#1715919780815708\n",
      "Copying gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715919779.903176-10523e8ee17f469b8ad27ee997489c3f.tgz#1715919780815708...\n",
      "/ [1 files][  1.3 KiB/  1.3 KiB]                                                \n",
      "Operation completed over 1 objects/1.3 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   5.12kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu.py37\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu.py37\n",
      "edaedc954fb5: Already exists\n",
      "30ec18464f16: Pulling fs layer\n",
      "b1210a962261: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "b372f52c3359: Pulling fs layer\n",
      "46820f8f141d: Pulling fs layer\n",
      "7e106c42709e: Pulling fs layer\n",
      "06336d59a562: Pulling fs layer\n",
      "41427056d78b: Pulling fs layer\n",
      "e48eb86068f2: Pulling fs layer\n",
      "56e204eacca1: Pulling fs layer\n",
      "1c8205fce1bf: Pulling fs layer\n",
      "9f79d1d1728e: Pulling fs layer\n",
      "dca84d2f2059: Pulling fs layer\n",
      "6ce62f5256c2: Pulling fs layer\n",
      "3642ea3e80d4: Pulling fs layer\n",
      "e16a032d433f: Pulling fs layer\n",
      "2478c9cd2ebf: Pulling fs layer\n",
      "f629568b334f: Pulling fs layer\n",
      "44cb10575b68: Pulling fs layer\n",
      "ed4ddcc30bc5: Pulling fs layer\n",
      "eaa97cab871a: Pulling fs layer\n",
      "f5fcd2c9d555: Pulling fs layer\n",
      "b372f52c3359: Waiting\n",
      "46820f8f141d: Waiting\n",
      "7e106c42709e: Waiting\n",
      "06336d59a562: Waiting\n",
      "41427056d78b: Waiting\n",
      "e48eb86068f2: Waiting\n",
      "56e204eacca1: Waiting\n",
      "1c8205fce1bf: Waiting\n",
      "9f79d1d1728e: Waiting\n",
      "dca84d2f2059: Waiting\n",
      "6ce62f5256c2: Waiting\n",
      "3642ea3e80d4: Waiting\n",
      "e16a032d433f: Waiting\n",
      "2478c9cd2ebf: Waiting\n",
      "f629568b334f: Waiting\n",
      "44cb10575b68: Waiting\n",
      "ed4ddcc30bc5: Waiting\n",
      "eaa97cab871a: Waiting\n",
      "f5fcd2c9d555: Waiting\n",
      "b1210a962261: Verifying Checksum\n",
      "b1210a962261: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "30ec18464f16: Download complete\n",
      "7e106c42709e: Verifying Checksum\n",
      "7e106c42709e: Download complete\n",
      "06336d59a562: Verifying Checksum\n",
      "06336d59a562: Download complete\n",
      "41427056d78b: Download complete\n",
      "e48eb86068f2: Verifying Checksum\n",
      "e48eb86068f2: Download complete\n",
      "46820f8f141d: Verifying Checksum\n",
      "46820f8f141d: Download complete\n",
      "30ec18464f16: Pull complete\n",
      "1c8205fce1bf: Verifying Checksum\n",
      "1c8205fce1bf: Download complete\n",
      "b1210a962261: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "9f79d1d1728e: Verifying Checksum\n",
      "9f79d1d1728e: Download complete\n",
      "dca84d2f2059: Verifying Checksum\n",
      "dca84d2f2059: Download complete\n",
      "6ce62f5256c2: Verifying Checksum\n",
      "6ce62f5256c2: Download complete\n",
      "3642ea3e80d4: Verifying Checksum\n",
      "3642ea3e80d4: Download complete\n",
      "e16a032d433f: Verifying Checksum\n",
      "e16a032d433f: Download complete\n",
      "56e204eacca1: Verifying Checksum\n",
      "56e204eacca1: Download complete\n",
      "b372f52c3359: Download complete\n",
      "2478c9cd2ebf: Verifying Checksum\n",
      "2478c9cd2ebf: Download complete\n",
      "f629568b334f: Verifying Checksum\n",
      "f629568b334f: Download complete\n",
      "44cb10575b68: Verifying Checksum\n",
      "44cb10575b68: Download complete\n",
      "ed4ddcc30bc5: Verifying Checksum\n",
      "ed4ddcc30bc5: Download complete\n",
      "f5fcd2c9d555: Verifying Checksum\n",
      "f5fcd2c9d555: Download complete\n",
      "eaa97cab871a: Verifying Checksum\n",
      "eaa97cab871a: Download complete\n",
      "b372f52c3359: Pull complete\n",
      "46820f8f141d: Pull complete\n",
      "7e106c42709e: Pull complete\n",
      "06336d59a562: Pull complete\n",
      "41427056d78b: Pull complete\n",
      "e48eb86068f2: Pull complete\n",
      "56e204eacca1: Pull complete\n",
      "1c8205fce1bf: Pull complete\n",
      "9f79d1d1728e: Pull complete\n",
      "dca84d2f2059: Pull complete\n",
      "6ce62f5256c2: Pull complete\n",
      "3642ea3e80d4: Pull complete\n",
      "e16a032d433f: Pull complete\n",
      "2478c9cd2ebf: Pull complete\n",
      "f629568b334f: Pull complete\n",
      "44cb10575b68: Pull complete\n",
      "ed4ddcc30bc5: Pull complete\n",
      "eaa97cab871a: Pull complete\n",
      "f5fcd2c9d555: Pull complete\n",
      "Digest: sha256:171807669808bf8f8ba8d7c80a3a05aa7942ee117c5fe84b23b8288cdde6eede\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu.py37:latest\n",
      " ---> 11bda29051df\n",
      "Step 2/5 : RUN pip install -U fire scikit-learn==0.23.2 pandas==1.1.1\n",
      " ---> Running in a2b42f6f114b\n",
      "Collecting fire\n",
      "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.4/88.4 kB 5.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn==0.23.2\n",
      "  Obtaining dependency information for scikit-learn==0.23.2 from https://files.pythonhosted.org/packages/f4/cb/64623369f348e9bfb29ff898a57ac7c91ed4921f228e9726546614d63ccb/scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl.metadata\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting pandas==1.1.1\n",
      "  Obtaining dependency information for pandas==1.1.1 from https://files.pythonhosted.org/packages/61/ed/10112535645ad7afd26c3b9defd20a32d9e42340b19c4f73ff26ccad06ee/pandas-1.1.1-cp37-cp37m-manylinux1_x86_64.whl.metadata\n",
      "  Downloading pandas-1.1.1-cp37-cp37m-manylinux1_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.1.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.1.1) (2023.3.post1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor (from fire)\n",
      "  Obtaining dependency information for termcolor from https://files.pythonhosted.org/packages/67/e1/434566ffce04448192369c1a282931cf4ae593e91907558eaecd2e9f2801/termcolor-2.3.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 73.0 MB/s eta 0:00:00\n",
      "Downloading pandas-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.5/10.5 MB 80.8 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117033 sha256=978802f2414eb7602b60e316171b531c52b08f2f6307a10865a1561fe543e854\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/30/ed/af256a2ae473c32a0c76e72fa25d794b127087f6897df9362a\n",
      "Successfully built fire\n",
      "Installing collected packages: termcolor, scikit-learn, pandas, fire\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "Successfully installed fire-0.6.0 pandas-1.1.1 scikit-learn-0.23.2 termcolor-2.3.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container a2b42f6f114b\n",
      " ---> 264babc03131\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in b78cde1ef7f2\n",
      "Removing intermediate container b78cde1ef7f2\n",
      " ---> 12a3e6b1ba3d\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> da16372654fb\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 5b8f1637128c\n",
      "Removing intermediate container 5b8f1637128c\n",
      " ---> 6de085a9fa47\n",
      "Successfully built 6de085a9fa47\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-04-71169a1ff84c/scikit_trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-04-71169a1ff84c/scikit_trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-04-71169a1ff84c/scikit_trainer_image]\n",
      "22641500a2a8: Preparing\n",
      "d3d12280987c: Preparing\n",
      "996583042380: Preparing\n",
      "5a7ed725a946: Preparing\n",
      "073a5f3bc8ec: Preparing\n",
      "f6face19c9dd: Preparing\n",
      "92affaab67e8: Preparing\n",
      "8c672b5f8a22: Preparing\n",
      "73066cf8673e: Preparing\n",
      "2ca62cf21221: Preparing\n",
      "b9a424722d3f: Preparing\n",
      "8a11b9d4bbb9: Preparing\n",
      "f2e60533bd85: Preparing\n",
      "f8eee1652f88: Preparing\n",
      "ddac17ce4d28: Preparing\n",
      "731f70dfb97d: Preparing\n",
      "dc8463ac1279: Preparing\n",
      "5520158f7ef1: Preparing\n",
      "152607a648ba: Preparing\n",
      "62bb75d94c35: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "1a1ffc16734e: Preparing\n",
      "8449535b89d9: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "9a2ad63887b6: Preparing\n",
      "bca2d3c0b431: Preparing\n",
      "954c82bdeb5f: Preparing\n",
      "f6face19c9dd: Waiting\n",
      "92affaab67e8: Waiting\n",
      "8c672b5f8a22: Waiting\n",
      "73066cf8673e: Waiting\n",
      "2ca62cf21221: Waiting\n",
      "b9a424722d3f: Waiting\n",
      "8a11b9d4bbb9: Waiting\n",
      "f2e60533bd85: Waiting\n",
      "f8eee1652f88: Waiting\n",
      "ddac17ce4d28: Waiting\n",
      "731f70dfb97d: Waiting\n",
      "dc8463ac1279: Waiting\n",
      "5520158f7ef1: Waiting\n",
      "152607a648ba: Waiting\n",
      "62bb75d94c35: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "1a1ffc16734e: Waiting\n",
      "8449535b89d9: Waiting\n",
      "9a2ad63887b6: Waiting\n",
      "bca2d3c0b431: Waiting\n",
      "954c82bdeb5f: Waiting\n",
      "5a7ed725a946: Mounted from deeplearning-platform-release/base-cpu.py37\n",
      "073a5f3bc8ec: Mounted from deeplearning-platform-release/base-cpu.py37\n",
      "f6face19c9dd: Layer already exists\n",
      "92affaab67e8: Layer already exists\n",
      "8c672b5f8a22: Layer already exists\n",
      "22641500a2a8: Pushed\n",
      "73066cf8673e: Layer already exists\n",
      "d3d12280987c: Pushed\n",
      "8a11b9d4bbb9: Layer already exists\n",
      "2ca62cf21221: Layer already exists\n",
      "b9a424722d3f: Layer already exists\n",
      "f2e60533bd85: Layer already exists\n",
      "ddac17ce4d28: Layer already exists\n",
      "f8eee1652f88: Layer already exists\n",
      "dc8463ac1279: Layer already exists\n",
      "731f70dfb97d: Layer already exists\n",
      "5520158f7ef1: Layer already exists\n",
      "62bb75d94c35: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "152607a648ba: Layer already exists\n",
      "8449535b89d9: Layer already exists\n",
      "1a1ffc16734e: Layer already exists\n",
      "9a2ad63887b6: Layer already exists\n",
      "bca2d3c0b431: Layer already exists\n",
      "954c82bdeb5f: Layer already exists\n",
      "996583042380: Pushed\n",
      "latest: digest: sha256:8de3ddf811eb75c3e7f0017ce3af8c0e116a150d232c071d94958a92d999e768 size: 5965\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                              STATUS\n",
      "0096e2b5-491e-4146-abee-5860f53b25e7  2024-05-17T04:23:01+00:00  3M10S     gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715919779.903176-10523e8ee17f469b8ad27ee997489c3f.tgz  gcr.io/qwiklabs-gcp-04-71169a1ff84c/scikit_trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $SCIKIT_IMAGE_URI $SCIKIT_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorFlow Training Script\n",
    "One down, three to go! Now we will develop a TensorFlow training script. We will use the tf.data API to ingest the data from CSVs then build/train a neural network with the tf.keras Functional API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir tensorflow_trainer_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tensorflow_trainer_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tensorflow_trainer_image/train.py\n",
    "\n",
    "\"\"\"Census Tensorflow classifier trainer script.\"\"\"\n",
    "\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "import fire\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "CSV_COLUMNS = [\"age\",\n",
    "               \"workclass\",\n",
    "               \"education_num\",\n",
    "               \"occupation\",\n",
    "               \"hours_per_week\",\n",
    "               \"income_bracket\"]\n",
    "\n",
    "# Add string name for label column\n",
    "LABEL_COLUMN = \"income_bracket\"\n",
    "\n",
    "# Set default values for each CSV column as a list of lists.\n",
    "# Treat is_male and plurality as strings.\n",
    "DEFAULTS = [[18], [\"?\"], [4], [\"?\"], [20],[\"<=50K\"]]\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    cols = tf.io.decode_csv(row_data, record_defaults=DEFAULTS)\n",
    "    feats = {\n",
    "        'age': tf.reshape(cols[0], [1,]),\n",
    "        'workclass': tf.reshape(cols[1],[1,]),\n",
    "        'education_num': tf.reshape(cols[2],[1,]),\n",
    "        'occupation': tf.reshape(cols[3],[1,]),\n",
    "        'hours_per_week': tf.reshape(cols[4],[1,]),\n",
    "        'income_bracket': cols[5]\n",
    "    }\n",
    "    label = feats.pop('income_bracket')\n",
    "    label_int = tf.case([(tf.math.equal(label,tf.constant([' <=50K'])), lambda: 0),\n",
    "                        (tf.math.equal(label,tf.constant([' >50K'])), lambda: 1)])\n",
    "    \n",
    "    return feats, label_int\n",
    "\n",
    "def load_dataset(pattern, batch_size=1, mode='eval'):\n",
    "    # Make a CSV dataset\n",
    "    filelist = tf.io.gfile.glob(pattern)\n",
    "    dataset = tf.data.TextLineDataset(filelist).skip(1)\n",
    "    dataset = dataset.map(features_and_labels)\n",
    "\n",
    "    # Shuffle and repeat for training\n",
    "    if mode == 'train':\n",
    "        dataset = dataset.shuffle(buffer_size=10*batch_size).batch(batch_size).repeat()\n",
    "    else:\n",
    "        dataset = dataset.batch(10)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def train_evaluate(training_dataset_path, validation_dataset_path, batch_size, num_train_examples, num_evals, output_dir):\n",
    "    inputs = {\n",
    "        'age': tf.keras.layers.Input(name='age',shape=[None],dtype='int32'),\n",
    "        'workclass': tf.keras.layers.Input(name='workclass',shape=[None],dtype='string'),\n",
    "        'education_num': tf.keras.layers.Input(name='education_num',shape=[None],dtype='int32'),\n",
    "        'occupation': tf.keras.layers.Input(name='occupation',shape=[None],dtype='string'),\n",
    "        'hours_per_week': tf.keras.layers.Input(name='hours_per_week',shape=[None],dtype='int32')\n",
    "    }\n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "    num_train_examples = int(num_train_examples)\n",
    "    num_evals = int(num_evals)\n",
    "    \n",
    "    feat_cols = {\n",
    "        'age': tf.feature_column.numeric_column('age'),\n",
    "        'workclass': tf.feature_column.indicator_column(\n",
    "            tf.feature_column.categorical_column_with_hash_bucket(\n",
    "                key='workclass', hash_bucket_size=100\n",
    "            )\n",
    "        ),\n",
    "        'education_num': tf.feature_column.numeric_column('education_num'),\n",
    "        'occupation': tf.feature_column.indicator_column(\n",
    "            tf.feature_column.categorical_column_with_hash_bucket(\n",
    "                key='occupation', hash_bucket_size=100\n",
    "            )\n",
    "        ),\n",
    "        'hours_per_week': tf.feature_column.numeric_column('hours_per_week')\n",
    "    }\n",
    "    \n",
    "    dnn_inputs = tf.keras.layers.DenseFeatures(\n",
    "        feature_columns=feat_cols.values())(inputs)\n",
    "    h1 = tf.keras.layers.Dense(64, activation='relu')(dnn_inputs)\n",
    "    h2 = tf.keras.layers.Dense(128, activation='relu')(h1)\n",
    "    h3 = tf.keras.layers.Dense(64, activation='relu')(h2)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(h3)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inputs,outputs=output)\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    trainds = load_dataset(\n",
    "        pattern=training_dataset_path,\n",
    "        batch_size=batch_size,\n",
    "        mode='train')\n",
    "    \n",
    "    evalds = load_dataset(\n",
    "        pattern=validation_dataset_path,\n",
    "        mode='eval')\n",
    "    \n",
    "    \n",
    "    steps_per_epoch = num_train_examples // (batch_size * num_evals)\n",
    "    \n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        validation_steps=100,\n",
    "        epochs=num_evals,\n",
    "        steps_per_epoch=steps_per_epoch\n",
    "    )\n",
    "    \n",
    "    EXPORT_PATH = os.path.join(\n",
    "    output_dir, datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    tf.saved_model.save(\n",
    "        obj=model, export_dir=EXPORT_PATH)  # with default serving function\n",
    "    \n",
    "    print(\"Exported trained model to {}\".format(EXPORT_PATH))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package TensorFlow Training Script into a Docker Image\n",
    "Note that the dependencies in this Dockerfile are different than for the Scikit-learn one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tensorflow_trainer_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tensorflow_trainer_image/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu.py37\n",
    "RUN pip install -U fire tensorflow==2.1.1\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Tensorflow Trainer Image \n",
    "Build the image and push it to your project's container registry. Again, this will take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IMAGE_NAME='tensorflow_trainer_image'\n",
    "TF_IMAGE_TAG='latest'\n",
    "TF_IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, TF_IMAGE_NAME, TF_IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 2 file(s) totalling 4.4 KiB before compression.\n",
      "Uploading tarball of [tensorflow_trainer_image] to [gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715919985.040391-1c456f9e3bdc41868384a068621d4889.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-04-71169a1ff84c/locations/global/builds/f6a554d5-28a5-485f-b768-de3fee8f553e].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/f6a554d5-28a5-485f-b768-de3fee8f553e?project=871018649403 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"f6a554d5-28a5-485f-b768-de3fee8f553e\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715919985.040391-1c456f9e3bdc41868384a068621d4889.tgz#1715919985313121\n",
      "Copying gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715919985.040391-1c456f9e3bdc41868384a068621d4889.tgz#1715919985313121...\n",
      "/ [1 files][  1.8 KiB/  1.8 KiB]                                                \n",
      "Operation completed over 1 objects/1.8 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  7.168kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu.py37\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu.py37\n",
      "edaedc954fb5: Already exists\n",
      "30ec18464f16: Pulling fs layer\n",
      "b1210a962261: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "b372f52c3359: Pulling fs layer\n",
      "46820f8f141d: Pulling fs layer\n",
      "7e106c42709e: Pulling fs layer\n",
      "06336d59a562: Pulling fs layer\n",
      "41427056d78b: Pulling fs layer\n",
      "e48eb86068f2: Pulling fs layer\n",
      "56e204eacca1: Pulling fs layer\n",
      "1c8205fce1bf: Pulling fs layer\n",
      "9f79d1d1728e: Pulling fs layer\n",
      "dca84d2f2059: Pulling fs layer\n",
      "6ce62f5256c2: Pulling fs layer\n",
      "3642ea3e80d4: Pulling fs layer\n",
      "e16a032d433f: Pulling fs layer\n",
      "2478c9cd2ebf: Pulling fs layer\n",
      "f629568b334f: Pulling fs layer\n",
      "44cb10575b68: Pulling fs layer\n",
      "ed4ddcc30bc5: Pulling fs layer\n",
      "eaa97cab871a: Pulling fs layer\n",
      "f5fcd2c9d555: Pulling fs layer\n",
      "b372f52c3359: Waiting\n",
      "46820f8f141d: Waiting\n",
      "7e106c42709e: Waiting\n",
      "06336d59a562: Waiting\n",
      "41427056d78b: Waiting\n",
      "e48eb86068f2: Waiting\n",
      "56e204eacca1: Waiting\n",
      "1c8205fce1bf: Waiting\n",
      "9f79d1d1728e: Waiting\n",
      "dca84d2f2059: Waiting\n",
      "6ce62f5256c2: Waiting\n",
      "3642ea3e80d4: Waiting\n",
      "e16a032d433f: Waiting\n",
      "2478c9cd2ebf: Waiting\n",
      "f629568b334f: Waiting\n",
      "44cb10575b68: Waiting\n",
      "ed4ddcc30bc5: Waiting\n",
      "eaa97cab871a: Waiting\n",
      "f5fcd2c9d555: Waiting\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "b1210a962261: Verifying Checksum\n",
      "b1210a962261: Download complete\n",
      "30ec18464f16: Verifying Checksum\n",
      "30ec18464f16: Download complete\n",
      "7e106c42709e: Verifying Checksum\n",
      "7e106c42709e: Download complete\n",
      "06336d59a562: Verifying Checksum\n",
      "06336d59a562: Download complete\n",
      "41427056d78b: Verifying Checksum\n",
      "41427056d78b: Download complete\n",
      "e48eb86068f2: Verifying Checksum\n",
      "e48eb86068f2: Download complete\n",
      "46820f8f141d: Verifying Checksum\n",
      "46820f8f141d: Download complete\n",
      "1c8205fce1bf: Verifying Checksum\n",
      "1c8205fce1bf: Download complete\n",
      "9f79d1d1728e: Download complete\n",
      "dca84d2f2059: Verifying Checksum\n",
      "dca84d2f2059: Download complete\n",
      "b372f52c3359: Download complete\n",
      "6ce62f5256c2: Verifying Checksum\n",
      "6ce62f5256c2: Download complete\n",
      "3642ea3e80d4: Verifying Checksum\n",
      "3642ea3e80d4: Download complete\n",
      "30ec18464f16: Pull complete\n",
      "e16a032d433f: Verifying Checksum\n",
      "e16a032d433f: Download complete\n",
      "b1210a962261: Pull complete\n",
      "2478c9cd2ebf: Verifying Checksum\n",
      "2478c9cd2ebf: Download complete\n",
      "4f4fb700ef54: Pull complete\n",
      "f629568b334f: Verifying Checksum\n",
      "f629568b334f: Download complete\n",
      "56e204eacca1: Verifying Checksum\n",
      "56e204eacca1: Download complete\n",
      "44cb10575b68: Verifying Checksum\n",
      "44cb10575b68: Download complete\n",
      "ed4ddcc30bc5: Download complete\n",
      "f5fcd2c9d555: Verifying Checksum\n",
      "f5fcd2c9d555: Download complete\n",
      "eaa97cab871a: Verifying Checksum\n",
      "eaa97cab871a: Download complete\n",
      "b372f52c3359: Pull complete\n",
      "46820f8f141d: Pull complete\n",
      "7e106c42709e: Pull complete\n",
      "06336d59a562: Pull complete\n",
      "41427056d78b: Pull complete\n",
      "e48eb86068f2: Pull complete\n",
      "56e204eacca1: Pull complete\n",
      "1c8205fce1bf: Pull complete\n",
      "9f79d1d1728e: Pull complete\n",
      "dca84d2f2059: Pull complete\n",
      "6ce62f5256c2: Pull complete\n",
      "3642ea3e80d4: Pull complete\n",
      "e16a032d433f: Pull complete\n",
      "2478c9cd2ebf: Pull complete\n",
      "f629568b334f: Pull complete\n",
      "44cb10575b68: Pull complete\n",
      "ed4ddcc30bc5: Pull complete\n",
      "eaa97cab871a: Pull complete\n",
      "f5fcd2c9d555: Pull complete\n",
      "Digest: sha256:171807669808bf8f8ba8d7c80a3a05aa7942ee117c5fe84b23b8288cdde6eede\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu.py37:latest\n",
      " ---> 11bda29051df\n",
      "Step 2/5 : RUN pip install -U fire tensorflow==2.1.1\n",
      " ---> Running in 2b17cd6ec589\n",
      "Collecting fire\n",
      "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.4/88.4 kB 4.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tensorflow==2.1.1\n",
      "  Obtaining dependency information for tensorflow==2.1.1 from https://files.pythonhosted.org/packages/15/49/87dd3b5e7fa4cf5255433bc4ecce0cec6562f179e67a2fa47da4de8e532c/tensorflow-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl.metadata\n",
      "  Downloading tensorflow-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (2.0.0)\n",
      "Collecting astor>=0.6.0 (from tensorflow==2.1.1)\n",
      "  Obtaining dependency information for astor>=0.6.0 from https://files.pythonhosted.org/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting gast==0.2.2 (from tensorflow==2.1.1)\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow==2.1.1)\n",
      "  Obtaining dependency information for google-pasta>=0.1.6 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting keras-applications>=1.0.8 (from tensorflow==2.1.1)\n",
      "  Obtaining dependency information for keras-applications>=1.0.8 from https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl.metadata\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting keras-preprocessing==1.1.0 (from tensorflow==2.1.1)\n",
      "  Obtaining dependency information for keras-preprocessing==1.1.0 from https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.21.6)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.1.1)\n",
      "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (3.20.3)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0 (from tensorflow==2.1.1)\n",
      "  Obtaining dependency information for tensorboard<2.2.0,>=2.1.0 from https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.1.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0 (from tensorflow==2.1.1)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.2.0,>=2.1.0rc0 from https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.1.1)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/67/e1/434566ffce04448192369c1a282931cf4ae593e91907558eaecd2e9f2801/termcolor-2.3.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.15.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.16.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (1.58.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.1.1) (0.41.2)\n",
      "Collecting scipy==1.4.1 (from tensorflow==2.1.1)\n",
      "  Obtaining dependency information for scipy==1.4.1 from https://files.pythonhosted.org/packages/dd/82/c1fe128f3526b128cfd185580ba40d01371c5d299fcf7f77968e22dfcc2e/scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl.metadata\n",
      "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting h5py (from keras-applications>=1.0.8->tensorflow==2.1.1)\n",
      "  Obtaining dependency information for h5py from https://files.pythonhosted.org/packages/95/be/de1e591bec008ed92d3829b985757b8bc2d34179feef5e181530876a4f9d/h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1)\n",
      "  Obtaining dependency information for google-auth<2,>=1.6.3 from https://files.pythonhosted.org/packages/fb/7a/1b3eb54caee1b8c73c2c3645f78a382eca4805a301a30c64a078e736e446/google_auth-1.35.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1)\n",
      "  Obtaining dependency information for google-auth-oauthlib<0.5,>=0.4.1 from https://files.pythonhosted.org/packages/b1/0e/0636cc1448a7abc444fb1b3a63655e294e0d2d49092dc3de05241be6d43c/google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (68.2.2)\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1)\n",
      "  Obtaining dependency information for werkzeug>=0.11.15 from https://files.pythonhosted.org/packages/f6/f8/9da63c1617ae2a1dec2fbf6412f3a0cfe9d4ce029eccbda6e1e4258ca45f/Werkzeug-2.2.3-py3-none-any.whl.metadata\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1)\n",
      "  Obtaining dependency information for cachetools<5.0,>=2.0.0 from https://files.pythonhosted.org/packages/ea/c1/4740af52db75e6dbdd57fc7e9478439815bbac549c1c05881be27d19a17d/cachetools-4.2.4-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.11.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (4.7.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.1) (3.2.2)\n",
      "Downloading tensorflow-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 421.8/421.8 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.9/41.9 kB 190.1 kB/s eta 0:00:00\n",
      "Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.1/26.1 MB 40.3 MB/s eta 0:00:00\n",
      "Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 9.1 MB/s eta 0:00:00\n",
      "Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.7/50.7 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 9.9 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 71.3 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 449.0/449.0 kB 38.4 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.9/152.9 kB 23.2 MB/s eta 0:00:00\n",
      "Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.2/94.2 kB 15.4 MB/s eta 0:00:00\n",
      "Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.6/233.6 kB 29.8 MB/s eta 0:00:00\n",
      "Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 77.3 MB/s eta 0:00:00\n",
      "Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Building wheels for collected packages: gast, fire\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7538 sha256=7cb49fdd8266786b6970139c85fda4f0498dfd276fb8a6ad7d19ece1bc834c8b\n",
      "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117033 sha256=09e2b2ba1aaebec3e53b3562a5e42ef091de9741b5bcb8f0246c694b6838f20b\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/30/ed/af256a2ae473c32a0c76e72fa25d794b127087f6897df9362a\n",
      "Successfully built gast fire\n",
      "Installing collected packages: tensorflow-estimator, werkzeug, termcolor, scipy, opt-einsum, keras-preprocessing, h5py, google-pasta, gast, cachetools, astor, markdown, keras-applications, google-auth, fire, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.3\n",
      "    Uninstalling scipy-1.7.3:\n",
      "      Successfully uninstalled scipy-1.7.3\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.3.1\n",
      "    Uninstalling cachetools-5.3.1:\n",
      "      Successfully uninstalled cachetools-5.3.1\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.23.0\n",
      "    Uninstalling google-auth-2.23.0:\n",
      "      Successfully uninstalled google-auth-2.23.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.1.0\n",
      "    Uninstalling google-auth-oauthlib-1.1.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.1.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "phik 0.12.3 requires scipy>=1.5.2, but you have scipy 1.4.1 which is incompatible.\n",
      "\u001b[0mSuccessfully installed astor-0.8.1 cachetools-4.2.4 fire-0.6.0 gast-0.2.2 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 h5py-3.8.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 markdown-3.4.4 opt-einsum-3.3.0 scipy-1.4.1 tensorboard-2.1.1 tensorflow-2.1.1 tensorflow-estimator-2.1.0 termcolor-2.3.0 werkzeug-2.2.3\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 2b17cd6ec589\n",
      " ---> c86eee30e720\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in b3ba3e67b8bc\n",
      "Removing intermediate container b3ba3e67b8bc\n",
      " ---> b7038417b67f\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 2a3c6479751f\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in d22ac190944d\n",
      "Removing intermediate container d22ac190944d\n",
      " ---> 7c3261c9afa5\n",
      "Successfully built 7c3261c9afa5\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-04-71169a1ff84c/tensorflow_trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-04-71169a1ff84c/tensorflow_trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-04-71169a1ff84c/tensorflow_trainer_image]\n",
      "3b0db40ea140: Preparing\n",
      "f6b2fcefc97f: Preparing\n",
      "904e065486b4: Preparing\n",
      "5a7ed725a946: Preparing\n",
      "073a5f3bc8ec: Preparing\n",
      "f6face19c9dd: Preparing\n",
      "92affaab67e8: Preparing\n",
      "8c672b5f8a22: Preparing\n",
      "73066cf8673e: Preparing\n",
      "2ca62cf21221: Preparing\n",
      "b9a424722d3f: Preparing\n",
      "8a11b9d4bbb9: Preparing\n",
      "f2e60533bd85: Preparing\n",
      "f8eee1652f88: Preparing\n",
      "ddac17ce4d28: Preparing\n",
      "731f70dfb97d: Preparing\n",
      "dc8463ac1279: Preparing\n",
      "5520158f7ef1: Preparing\n",
      "152607a648ba: Preparing\n",
      "62bb75d94c35: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "1a1ffc16734e: Preparing\n",
      "8449535b89d9: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "9a2ad63887b6: Preparing\n",
      "bca2d3c0b431: Preparing\n",
      "954c82bdeb5f: Preparing\n",
      "f6face19c9dd: Waiting\n",
      "92affaab67e8: Waiting\n",
      "8c672b5f8a22: Waiting\n",
      "73066cf8673e: Waiting\n",
      "2ca62cf21221: Waiting\n",
      "b9a424722d3f: Waiting\n",
      "8a11b9d4bbb9: Waiting\n",
      "f2e60533bd85: Waiting\n",
      "f8eee1652f88: Waiting\n",
      "ddac17ce4d28: Waiting\n",
      "731f70dfb97d: Waiting\n",
      "dc8463ac1279: Waiting\n",
      "5520158f7ef1: Waiting\n",
      "152607a648ba: Waiting\n",
      "62bb75d94c35: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "1a1ffc16734e: Waiting\n",
      "8449535b89d9: Waiting\n",
      "9a2ad63887b6: Waiting\n",
      "bca2d3c0b431: Waiting\n",
      "954c82bdeb5f: Waiting\n",
      "073a5f3bc8ec: Layer already exists\n",
      "5a7ed725a946: Layer already exists\n",
      "f6face19c9dd: Layer already exists\n",
      "92affaab67e8: Layer already exists\n",
      "8c672b5f8a22: Layer already exists\n",
      "73066cf8673e: Layer already exists\n",
      "2ca62cf21221: Layer already exists\n",
      "8a11b9d4bbb9: Layer already exists\n",
      "b9a424722d3f: Layer already exists\n",
      "f2e60533bd85: Layer already exists\n",
      "f8eee1652f88: Layer already exists\n",
      "ddac17ce4d28: Layer already exists\n",
      "731f70dfb97d: Layer already exists\n",
      "dc8463ac1279: Layer already exists\n",
      "5520158f7ef1: Layer already exists\n",
      "152607a648ba: Layer already exists\n",
      "62bb75d94c35: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "1a1ffc16734e: Layer already exists\n",
      "8449535b89d9: Layer already exists\n",
      "bca2d3c0b431: Layer already exists\n",
      "9a2ad63887b6: Layer already exists\n",
      "954c82bdeb5f: Layer already exists\n",
      "3b0db40ea140: Pushed\n",
      "f6b2fcefc97f: Pushed\n",
      "904e065486b4: Pushed\n",
      "latest: digest: sha256:c0f10d86a1ab516aca5f069652b4188839caeb602f1899a300ef7baf263ee704 size: 5966\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                                  STATUS\n",
      "f6a554d5-28a5-485f-b768-de3fee8f553e  2024-05-17T04:26:25+00:00  5M30S     gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715919985.040391-1c456f9e3bdc41868384a068621d4889.tgz  gcr.io/qwiklabs-gcp-04-71169a1ff84c/tensorflow_trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $TF_IMAGE_URI $TF_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PyTorch Training Script\n",
    "Two down, two to go! Now we will develop a PyTorch training script. We will use Pandas DataFrames combined with PyTorch's Dataset and Dataloader to ingest the data from CSVs, build a model with torch.nn, and write a training loop to train this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir pytorch_trainer_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pytorch_trainer_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pytorch_trainer_image/train.py\n",
    "\n",
    "import os \n",
    "import subprocess\n",
    "import datetime\n",
    "import fire\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class TrainData(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        # 27 input features\n",
    "        self.h1 = nn.Linear(27, 64) \n",
    "        self.h2 = nn.Linear(64, 64)\n",
    "        self.output_layer = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.h1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.h2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "def binary_acc(y_pred, y_true):\n",
    "    \"\"\"Calculates accuracy\"\"\"\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_true).sum().float()\n",
    "    acc = correct_results_sum/y_true.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def train_evaluate(training_dataset_path, validation_dataset_path, batch_size, num_epochs, output_dir):\n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "    num_epochs = int(num_epochs)\n",
    "    \n",
    "    # Read in train/validation data and concat \n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "    df = pd.concat([df_train, df_validation])\n",
    "\n",
    "    categorical_features = ['workclass', 'occupation']\n",
    "    target='income_bracket'\n",
    "\n",
    "    # One-hot encode categorical variables \n",
    "    df = pd.get_dummies(df,columns=categorical_features)\n",
    "\n",
    "    # Change label to 0 if <=50K, 1 if >50K\n",
    "    df[target] = df[target].apply(lambda x: 0 if x==' <=50K' else 1)\n",
    "\n",
    "    # Split features and labels into 2 different vars\n",
    "    X_train = df.loc[:, df.columns != target]\n",
    "    y_train = np.array(df[target])\n",
    "\n",
    "    # Normalize features \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Training data\n",
    "    train_data = TrainData(torch.FloatTensor(X_train), \n",
    "                           torch.FloatTensor(y_train))\n",
    "\n",
    "    # Use torch DataLoader to feed data to model \n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "    # Instantiate model \n",
    "    model = BinaryClassifier()\n",
    "    \n",
    "    # Loss is binary crossentropy w/ logits. Must manually implement sigmoid for inference\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model.train()\n",
    "    for e in range(1, num_epochs+1):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "            acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "        print(f'Epoch {e}: Loss = {epoch_loss/len(train_loader):.5f} | Acc = {epoch_acc/len(train_loader):.3f}')\n",
    "\n",
    "    # Save the model locally\n",
    "    model_filename='model.pt'\n",
    "    torch.save(model.state_dict(), model_filename)\n",
    "\n",
    "    EXPORT_PATH = os.path.join(\n",
    "        output_dir, datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "\n",
    "    # Copy the model to GCS\n",
    "    gcs_model_path = '{}/{}'.format(EXPORT_PATH, model_filename)\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path])\n",
    "    print('Saved model in: {}'.format(gcs_model_path))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package PyTorch Training Script into a Docker Image\n",
    "Note the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pytorch_trainer_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pytorch_trainer_image/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu.py37\n",
    "RUN pip install -U fire torch==1.6.0 scikit-learn==0.23.2 pandas==1.1.1\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the PyTorch Trainer Image \n",
    "Build and push the PyTorch training image to your project's Container Registry. Again, this will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_IMAGE_NAME='pytorch_trainer_image'\n",
    "TORCH_IMAGE_TAG='latest'\n",
    "TORCH_IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, TORCH_IMAGE_NAME, TORCH_IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 2 file(s) totalling 4.1 KiB before compression.\n",
      "Uploading tarball of [pytorch_trainer_image] to [gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715920321.05788-e5d7c7b9953d44b8a2b132a47332abb8.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-04-71169a1ff84c/locations/global/builds/ebd196ef-8187-4dc2-92f9-516c1387edd0].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/ebd196ef-8187-4dc2-92f9-516c1387edd0?project=871018649403 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"ebd196ef-8187-4dc2-92f9-516c1387edd0\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715920321.05788-e5d7c7b9953d44b8a2b132a47332abb8.tgz#1715920321317799\n",
      "Copying gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715920321.05788-e5d7c7b9953d44b8a2b132a47332abb8.tgz#1715920321317799...\n",
      "/ [1 files][  1.9 KiB/  1.9 KiB]                                                \n",
      "Operation completed over 1 objects/1.9 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.656kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu.py37\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu.py37\n",
      "edaedc954fb5: Already exists\n",
      "30ec18464f16: Pulling fs layer\n",
      "b1210a962261: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "b372f52c3359: Pulling fs layer\n",
      "46820f8f141d: Pulling fs layer\n",
      "7e106c42709e: Pulling fs layer\n",
      "06336d59a562: Pulling fs layer\n",
      "41427056d78b: Pulling fs layer\n",
      "e48eb86068f2: Pulling fs layer\n",
      "56e204eacca1: Pulling fs layer\n",
      "1c8205fce1bf: Pulling fs layer\n",
      "9f79d1d1728e: Pulling fs layer\n",
      "dca84d2f2059: Pulling fs layer\n",
      "6ce62f5256c2: Pulling fs layer\n",
      "3642ea3e80d4: Pulling fs layer\n",
      "e16a032d433f: Pulling fs layer\n",
      "2478c9cd2ebf: Pulling fs layer\n",
      "f629568b334f: Pulling fs layer\n",
      "44cb10575b68: Pulling fs layer\n",
      "ed4ddcc30bc5: Pulling fs layer\n",
      "eaa97cab871a: Pulling fs layer\n",
      "f5fcd2c9d555: Pulling fs layer\n",
      "dca84d2f2059: Waiting\n",
      "6ce62f5256c2: Waiting\n",
      "3642ea3e80d4: Waiting\n",
      "e16a032d433f: Waiting\n",
      "2478c9cd2ebf: Waiting\n",
      "f629568b334f: Waiting\n",
      "b372f52c3359: Waiting\n",
      "46820f8f141d: Waiting\n",
      "7e106c42709e: Waiting\n",
      "06336d59a562: Waiting\n",
      "41427056d78b: Waiting\n",
      "e48eb86068f2: Waiting\n",
      "56e204eacca1: Waiting\n",
      "1c8205fce1bf: Waiting\n",
      "9f79d1d1728e: Waiting\n",
      "44cb10575b68: Waiting\n",
      "ed4ddcc30bc5: Waiting\n",
      "eaa97cab871a: Waiting\n",
      "f5fcd2c9d555: Waiting\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "b1210a962261: Verifying Checksum\n",
      "b1210a962261: Download complete\n",
      "30ec18464f16: Verifying Checksum\n",
      "30ec18464f16: Download complete\n",
      "7e106c42709e: Verifying Checksum\n",
      "7e106c42709e: Download complete\n",
      "06336d59a562: Verifying Checksum\n",
      "06336d59a562: Download complete\n",
      "41427056d78b: Verifying Checksum\n",
      "41427056d78b: Download complete\n",
      "e48eb86068f2: Verifying Checksum\n",
      "e48eb86068f2: Download complete\n",
      "46820f8f141d: Verifying Checksum\n",
      "46820f8f141d: Download complete\n",
      "1c8205fce1bf: Verifying Checksum\n",
      "1c8205fce1bf: Download complete\n",
      "9f79d1d1728e: Verifying Checksum\n",
      "9f79d1d1728e: Download complete\n",
      "dca84d2f2059: Verifying Checksum\n",
      "dca84d2f2059: Download complete\n",
      "6ce62f5256c2: Verifying Checksum\n",
      "6ce62f5256c2: Download complete\n",
      "3642ea3e80d4: Verifying Checksum\n",
      "3642ea3e80d4: Download complete\n",
      "e16a032d433f: Verifying Checksum\n",
      "e16a032d433f: Download complete\n",
      "2478c9cd2ebf: Verifying Checksum\n",
      "2478c9cd2ebf: Download complete\n",
      "f629568b334f: Verifying Checksum\n",
      "f629568b334f: Download complete\n",
      "b372f52c3359: Verifying Checksum\n",
      "b372f52c3359: Download complete\n",
      "44cb10575b68: Verifying Checksum\n",
      "44cb10575b68: Download complete\n",
      "ed4ddcc30bc5: Verifying Checksum\n",
      "ed4ddcc30bc5: Download complete\n",
      "56e204eacca1: Download complete\n",
      "30ec18464f16: Pull complete\n",
      "b1210a962261: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "f5fcd2c9d555: Verifying Checksum\n",
      "f5fcd2c9d555: Download complete\n",
      "eaa97cab871a: Verifying Checksum\n",
      "eaa97cab871a: Download complete\n",
      "b372f52c3359: Pull complete\n",
      "46820f8f141d: Pull complete\n",
      "7e106c42709e: Pull complete\n",
      "06336d59a562: Pull complete\n",
      "41427056d78b: Pull complete\n",
      "e48eb86068f2: Pull complete\n",
      "56e204eacca1: Pull complete\n",
      "1c8205fce1bf: Pull complete\n",
      "9f79d1d1728e: Pull complete\n",
      "dca84d2f2059: Pull complete\n",
      "6ce62f5256c2: Pull complete\n",
      "3642ea3e80d4: Pull complete\n",
      "e16a032d433f: Pull complete\n",
      "2478c9cd2ebf: Pull complete\n",
      "f629568b334f: Pull complete\n",
      "44cb10575b68: Pull complete\n",
      "ed4ddcc30bc5: Pull complete\n",
      "eaa97cab871a: Pull complete\n",
      "f5fcd2c9d555: Pull complete\n",
      "Digest: sha256:171807669808bf8f8ba8d7c80a3a05aa7942ee117c5fe84b23b8288cdde6eede\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu.py37:latest\n",
      " ---> 11bda29051df\n",
      "Step 2/5 : RUN pip install -U fire torch==1.6.0 scikit-learn==0.23.2 pandas==1.1.1\n",
      " ---> Running in d9e71d3def98\n",
      "Collecting fire\n",
      "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.4/88.4 kB 2.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting torch==1.6.0\n",
      "  Obtaining dependency information for torch==1.6.0 from https://files.pythonhosted.org/packages/5d/5e/35140615fc1f925023f489e71086a9ecc188053d263d3594237281284d82/torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl.metadata\n",
      "  Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl.metadata (22 kB)\n",
      "Collecting scikit-learn==0.23.2\n",
      "  Obtaining dependency information for scikit-learn==0.23.2 from https://files.pythonhosted.org/packages/f4/cb/64623369f348e9bfb29ff898a57ac7c91ed4921f228e9726546614d63ccb/scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl.metadata\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting pandas==1.1.1\n",
      "  Obtaining dependency information for pandas==1.1.1 from https://files.pythonhosted.org/packages/61/ed/10112535645ad7afd26c3b9defd20a32d9e42340b19c4f73ff26ccad06ee/pandas-1.1.1-cp37-cp37m-manylinux1_x86_64.whl.metadata\n",
      "  Downloading pandas-1.1.1-cp37-cp37m-manylinux1_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting future (from torch==1.6.0)\n",
      "  Obtaining dependency information for future from https://files.pythonhosted.org/packages/da/71/ae30dadffc90b9006d77af76b393cb9dfbfc9629f339fc1574a1c52e6806/future-1.0.0-py3-none-any.whl.metadata\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.6.0) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.1.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.1.1) (2023.3.post1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor (from fire)\n",
      "  Obtaining dependency information for termcolor from https://files.pythonhosted.org/packages/67/e1/434566ffce04448192369c1a282931cf4ae593e91907558eaecd2e9f2801/termcolor-2.3.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 748.8/748.8 MB 1.7 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 21.1 MB/s eta 0:00:00\n",
      "Downloading pandas-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.5/10.5 MB 21.0 MB/s eta 0:00:00\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 491.3/491.3 kB 23.3 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117033 sha256=775317b01c4b43613fc80dc23a06c4545bebfac6bdc63f0d88b1314dfd7d09c9\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/30/ed/af256a2ae473c32a0c76e72fa25d794b127087f6897df9362a\n",
      "Successfully built fire\n",
      "Installing collected packages: termcolor, future, torch, scikit-learn, pandas, fire\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "Successfully installed fire-0.6.0 future-1.0.0 pandas-1.1.1 scikit-learn-0.23.2 termcolor-2.3.0 torch-1.6.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container d9e71d3def98\n",
      " ---> 5756884309d0\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 668ab50ac04f\n",
      "Removing intermediate container 668ab50ac04f\n",
      " ---> ea6c59dc8a78\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> aa3e16f4025e\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 8e7784507f55\n",
      "Removing intermediate container 8e7784507f55\n",
      " ---> 5569fb398237\n",
      "Successfully built 5569fb398237\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-04-71169a1ff84c/pytorch_trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-04-71169a1ff84c/pytorch_trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-04-71169a1ff84c/pytorch_trainer_image]\n",
      "5c81d4169e8a: Preparing\n",
      "4920dbab2a10: Preparing\n",
      "673966a57a1c: Preparing\n",
      "5a7ed725a946: Preparing\n",
      "073a5f3bc8ec: Preparing\n",
      "f6face19c9dd: Preparing\n",
      "92affaab67e8: Preparing\n",
      "8c672b5f8a22: Preparing\n",
      "73066cf8673e: Preparing\n",
      "2ca62cf21221: Preparing\n",
      "b9a424722d3f: Preparing\n",
      "8a11b9d4bbb9: Preparing\n",
      "f2e60533bd85: Preparing\n",
      "f8eee1652f88: Preparing\n",
      "ddac17ce4d28: Preparing\n",
      "731f70dfb97d: Preparing\n",
      "dc8463ac1279: Preparing\n",
      "5520158f7ef1: Preparing\n",
      "152607a648ba: Preparing\n",
      "62bb75d94c35: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "1a1ffc16734e: Preparing\n",
      "8449535b89d9: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "9a2ad63887b6: Preparing\n",
      "bca2d3c0b431: Preparing\n",
      "954c82bdeb5f: Preparing\n",
      "ddac17ce4d28: Waiting\n",
      "731f70dfb97d: Waiting\n",
      "dc8463ac1279: Waiting\n",
      "5520158f7ef1: Waiting\n",
      "152607a648ba: Waiting\n",
      "62bb75d94c35: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "1a1ffc16734e: Waiting\n",
      "8449535b89d9: Waiting\n",
      "9a2ad63887b6: Waiting\n",
      "bca2d3c0b431: Waiting\n",
      "954c82bdeb5f: Waiting\n",
      "f6face19c9dd: Waiting\n",
      "92affaab67e8: Waiting\n",
      "8c672b5f8a22: Waiting\n",
      "73066cf8673e: Waiting\n",
      "2ca62cf21221: Waiting\n",
      "b9a424722d3f: Waiting\n",
      "8a11b9d4bbb9: Waiting\n",
      "f2e60533bd85: Waiting\n",
      "f8eee1652f88: Waiting\n",
      "5a7ed725a946: Layer already exists\n",
      "073a5f3bc8ec: Layer already exists\n",
      "92affaab67e8: Layer already exists\n",
      "f6face19c9dd: Layer already exists\n",
      "8c672b5f8a22: Layer already exists\n",
      "73066cf8673e: Layer already exists\n",
      "2ca62cf21221: Layer already exists\n",
      "b9a424722d3f: Layer already exists\n",
      "f2e60533bd85: Layer already exists\n",
      "8a11b9d4bbb9: Layer already exists\n",
      "f8eee1652f88: Layer already exists\n",
      "ddac17ce4d28: Layer already exists\n",
      "731f70dfb97d: Layer already exists\n",
      "dc8463ac1279: Layer already exists\n",
      "5520158f7ef1: Layer already exists\n",
      "152607a648ba: Layer already exists\n",
      "62bb75d94c35: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "1a1ffc16734e: Layer already exists\n",
      "8449535b89d9: Layer already exists\n",
      "9a2ad63887b6: Layer already exists\n",
      "954c82bdeb5f: Layer already exists\n",
      "bca2d3c0b431: Layer already exists\n",
      "4920dbab2a10: Pushed\n",
      "5c81d4169e8a: Pushed\n",
      "673966a57a1c: Pushed\n",
      "latest: digest: sha256:0e874826e9703c900226d5707b38d8fcc2f0d42c0737eace15d5c52fa5b18ebb size: 5967\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                     IMAGES                                                               STATUS\n",
      "ebd196ef-8187-4dc2-92f9-516c1387edd0  2024-05-17T04:32:01+00:00  5M35S     gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715920321.05788-e5d7c7b9953d44b8a2b132a47332abb8.tgz  gcr.io/qwiklabs-gcp-04-71169a1ff84c/pytorch_trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $TORCH_IMAGE_URI $TORCH_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create XGBoost Training Script\n",
    "Three down, one to go! Create the final training script. This script will ingest and preprocess data with Pandas, then train a [Gradient Boosted Tree model](https://en.wikipedia.org/wiki/Gradient_boosting). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir xgboost_trainer_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./xgboost_trainer_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./xgboost_trainer_image/train.py\n",
    "\n",
    "import os \n",
    "import subprocess\n",
    "import datetime\n",
    "import fire\n",
    "import pickle \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_evaluate(training_dataset_path, validation_dataset_path,max_depth,n_estimators,output_dir):\n",
    "    \n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "    df = pd.concat([df_train, df_validation])\n",
    "\n",
    "    categorical_features = ['workclass', 'occupation']\n",
    "    target='income_bracket'\n",
    "\n",
    "    # One-hot encode categorical variables \n",
    "    df = pd.get_dummies(df,columns=categorical_features)\n",
    "\n",
    "    # Change label to 0 if <=50K, 1 if >50K\n",
    "    df[target] = df[target].apply(lambda x: 0 if x==' <=50K' else 1)\n",
    "\n",
    "    # Split features and labels into 2 different vars\n",
    "    X_train = df.loc[:, df.columns != target]\n",
    "    y_train = np.array(df[target])\n",
    "\n",
    "    # Normalize features \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    \n",
    "    grid = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'n_estimators': int(n_estimators)\n",
    "    }\n",
    "    \n",
    "    model = XGBClassifier()\n",
    "    model.set_params(**grid)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    model_filename = 'xgb_model.pkl'\n",
    "    pickle.dump(model, open(model_filename, \"wb\"))\n",
    "        \n",
    "    EXPORT_PATH = os.path.join(\n",
    "        output_dir, datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    \n",
    "    gcs_model_path = '{}/{}'.format(EXPORT_PATH, model_filename)\n",
    "    subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path])\n",
    "    print('Saved model in: {}'.format(gcs_model_path))  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package XGBoost Training Script into a Docker Image\n",
    "Note the dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./xgboost_trainer_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./xgboost_trainer_image/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu.py37\n",
    "RUN pip install -U fire scikit-learn==0.23.2 pandas==1.1.1 xgboost==1.2.0\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the XGBoost Trainer Image\n",
    "Build and push the XGBoost training image. This will take a few minutes (this is the last one, woohoo!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_IMAGE_NAME='xgboost_trainer_image'\n",
    "XGB_IMAGE_TAG='latest'\n",
    "XGB_IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, XGB_IMAGE_NAME, XGB_IMAGE_TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary archive of 2 file(s) totalling 1.8 KiB before compression.\n",
      "Uploading tarball of [xgboost_trainer_image] to [gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715920663.178602-f4502ff022a544fc9f27690b4a95930d.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-04-71169a1ff84c/locations/global/builds/417869c1-a221-46b4-8feb-6237e5168f17].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/417869c1-a221-46b4-8feb-6237e5168f17?project=871018649403 ].\n",
      "Waiting for build to complete. Polling interval: 1 second(s).\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"417869c1-a221-46b4-8feb-6237e5168f17\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715920663.178602-f4502ff022a544fc9f27690b4a95930d.tgz#1715920663489705\n",
      "Copying gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715920663.178602-f4502ff022a544fc9f27690b4a95930d.tgz#1715920663489705...\n",
      "/ [1 files][  1.1 KiB/  1.1 KiB]                                                \n",
      "Operation completed over 1 objects/1.1 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  4.608kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu.py37\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu.py37\n",
      "edaedc954fb5: Already exists\n",
      "30ec18464f16: Pulling fs layer\n",
      "b1210a962261: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "b372f52c3359: Pulling fs layer\n",
      "46820f8f141d: Pulling fs layer\n",
      "7e106c42709e: Pulling fs layer\n",
      "06336d59a562: Pulling fs layer\n",
      "41427056d78b: Pulling fs layer\n",
      "e48eb86068f2: Pulling fs layer\n",
      "56e204eacca1: Pulling fs layer\n",
      "1c8205fce1bf: Pulling fs layer\n",
      "9f79d1d1728e: Pulling fs layer\n",
      "dca84d2f2059: Pulling fs layer\n",
      "6ce62f5256c2: Pulling fs layer\n",
      "3642ea3e80d4: Pulling fs layer\n",
      "e16a032d433f: Pulling fs layer\n",
      "2478c9cd2ebf: Pulling fs layer\n",
      "f629568b334f: Pulling fs layer\n",
      "44cb10575b68: Pulling fs layer\n",
      "ed4ddcc30bc5: Pulling fs layer\n",
      "eaa97cab871a: Pulling fs layer\n",
      "f5fcd2c9d555: Pulling fs layer\n",
      "9f79d1d1728e: Waiting\n",
      "dca84d2f2059: Waiting\n",
      "6ce62f5256c2: Waiting\n",
      "3642ea3e80d4: Waiting\n",
      "e16a032d433f: Waiting\n",
      "2478c9cd2ebf: Waiting\n",
      "f629568b334f: Waiting\n",
      "44cb10575b68: Waiting\n",
      "ed4ddcc30bc5: Waiting\n",
      "eaa97cab871a: Waiting\n",
      "f5fcd2c9d555: Waiting\n",
      "7e106c42709e: Waiting\n",
      "06336d59a562: Waiting\n",
      "41427056d78b: Waiting\n",
      "e48eb86068f2: Waiting\n",
      "56e204eacca1: Waiting\n",
      "1c8205fce1bf: Waiting\n",
      "b372f52c3359: Waiting\n",
      "46820f8f141d: Waiting\n",
      "4f4fb700ef54: Download complete\n",
      "b1210a962261: Verifying Checksum\n",
      "b1210a962261: Download complete\n",
      "30ec18464f16: Download complete\n",
      "7e106c42709e: Verifying Checksum\n",
      "7e106c42709e: Download complete\n",
      "06336d59a562: Verifying Checksum\n",
      "06336d59a562: Download complete\n",
      "41427056d78b: Verifying Checksum\n",
      "41427056d78b: Download complete\n",
      "e48eb86068f2: Verifying Checksum\n",
      "e48eb86068f2: Download complete\n",
      "46820f8f141d: Verifying Checksum\n",
      "46820f8f141d: Download complete\n",
      "1c8205fce1bf: Verifying Checksum\n",
      "1c8205fce1bf: Download complete\n",
      "9f79d1d1728e: Verifying Checksum\n",
      "9f79d1d1728e: Download complete\n",
      "dca84d2f2059: Download complete\n",
      "6ce62f5256c2: Verifying Checksum\n",
      "6ce62f5256c2: Download complete\n",
      "3642ea3e80d4: Verifying Checksum\n",
      "3642ea3e80d4: Download complete\n",
      "56e204eacca1: Verifying Checksum\n",
      "56e204eacca1: Download complete\n",
      "b372f52c3359: Verifying Checksum\n",
      "b372f52c3359: Download complete\n",
      "e16a032d433f: Verifying Checksum\n",
      "e16a032d433f: Download complete\n",
      "2478c9cd2ebf: Download complete\n",
      "f629568b334f: Verifying Checksum\n",
      "f629568b334f: Download complete\n",
      "30ec18464f16: Pull complete\n",
      "44cb10575b68: Verifying Checksum\n",
      "44cb10575b68: Download complete\n",
      "ed4ddcc30bc5: Verifying Checksum\n",
      "ed4ddcc30bc5: Download complete\n",
      "b1210a962261: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "f5fcd2c9d555: Verifying Checksum\n",
      "f5fcd2c9d555: Download complete\n",
      "eaa97cab871a: Verifying Checksum\n",
      "eaa97cab871a: Download complete\n",
      "b372f52c3359: Pull complete\n",
      "46820f8f141d: Pull complete\n",
      "7e106c42709e: Pull complete\n",
      "06336d59a562: Pull complete\n",
      "41427056d78b: Pull complete\n",
      "e48eb86068f2: Pull complete\n",
      "56e204eacca1: Pull complete\n",
      "1c8205fce1bf: Pull complete\n",
      "9f79d1d1728e: Pull complete\n",
      "dca84d2f2059: Pull complete\n",
      "6ce62f5256c2: Pull complete\n",
      "3642ea3e80d4: Pull complete\n",
      "e16a032d433f: Pull complete\n",
      "2478c9cd2ebf: Pull complete\n",
      "f629568b334f: Pull complete\n",
      "44cb10575b68: Pull complete\n",
      "ed4ddcc30bc5: Pull complete\n",
      "eaa97cab871a: Pull complete\n",
      "f5fcd2c9d555: Pull complete\n",
      "Digest: sha256:171807669808bf8f8ba8d7c80a3a05aa7942ee117c5fe84b23b8288cdde6eede\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu.py37:latest\n",
      " ---> 11bda29051df\n",
      "Step 2/5 : RUN pip install -U fire scikit-learn==0.23.2 pandas==1.1.1 xgboost==1.2.0\n",
      " ---> Running in 5ff52bbbc81c\n",
      "Collecting fire\n",
      "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.4/88.4 kB 2.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scikit-learn==0.23.2\n",
      "  Obtaining dependency information for scikit-learn==0.23.2 from https://files.pythonhosted.org/packages/f4/cb/64623369f348e9bfb29ff898a57ac7c91ed4921f228e9726546614d63ccb/scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl.metadata\n",
      "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting pandas==1.1.1\n",
      "  Obtaining dependency information for pandas==1.1.1 from https://files.pythonhosted.org/packages/61/ed/10112535645ad7afd26c3b9defd20a32d9e42340b19c4f73ff26ccad06ee/pandas-1.1.1-cp37-cp37m-manylinux1_x86_64.whl.metadata\n",
      "  Downloading pandas-1.1.1-cp37-cp37m-manylinux1_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting xgboost==1.2.0\n",
      "  Obtaining dependency information for xgboost==1.2.0 from https://files.pythonhosted.org/packages/f6/5c/1133b5b8f4f2fa740ff27abdd35b8e79ce6e1f8d6480a07e9bce1cdafea2/xgboost-1.2.0-py3-none-manylinux2010_x86_64.whl.metadata\n",
      "  Downloading xgboost-1.2.0-py3-none-manylinux2010_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2) (1.21.6)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.23.2) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.1.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.1.1) (2023.3.post1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor (from fire)\n",
      "  Obtaining dependency information for termcolor from https://files.pythonhosted.org/packages/67/e1/434566ffce04448192369c1a282931cf4ae593e91907558eaecd2e9f2801/termcolor-2.3.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 44.1 MB/s eta 0:00:00\n",
      "Downloading pandas-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (10.5 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.5/10.5 MB 75.8 MB/s eta 0:00:00\n",
      "Downloading xgboost-1.2.0-py3-none-manylinux2010_x86_64.whl (148.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.9/148.9 MB 6.1 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117033 sha256=dd7816c592cc55b64526e19de5c8795fea83fd03d232f4242b30ae4a7e1dae49\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/30/ed/af256a2ae473c32a0c76e72fa25d794b127087f6897df9362a\n",
      "Successfully built fire\n",
      "Installing collected packages: termcolor, xgboost, scikit-learn, pandas, fire\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.5\n",
      "    Uninstalling pandas-1.3.5:\n",
      "      Successfully uninstalled pandas-1.3.5\n",
      "Successfully installed fire-0.6.0 pandas-1.1.1 scikit-learn-0.23.2 termcolor-2.3.0 xgboost-1.2.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 5ff52bbbc81c\n",
      " ---> ae301420dfe5\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in b6fadbb371e0\n",
      "Removing intermediate container b6fadbb371e0\n",
      " ---> cdfd9505749d\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> ef80caa2884f\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in b4f67c6f8d06\n",
      "Removing intermediate container b4f67c6f8d06\n",
      " ---> 09ac53a3b0f2\n",
      "Successfully built 09ac53a3b0f2\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-04-71169a1ff84c/xgboost_trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-04-71169a1ff84c/xgboost_trainer_image:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-04-71169a1ff84c/xgboost_trainer_image]\n",
      "254055ea96b4: Preparing\n",
      "c1d33a5bd101: Preparing\n",
      "65f978f9da4f: Preparing\n",
      "5a7ed725a946: Preparing\n",
      "073a5f3bc8ec: Preparing\n",
      "f6face19c9dd: Preparing\n",
      "92affaab67e8: Preparing\n",
      "8c672b5f8a22: Preparing\n",
      "73066cf8673e: Preparing\n",
      "2ca62cf21221: Preparing\n",
      "b9a424722d3f: Preparing\n",
      "8a11b9d4bbb9: Preparing\n",
      "f2e60533bd85: Preparing\n",
      "f8eee1652f88: Preparing\n",
      "ddac17ce4d28: Preparing\n",
      "731f70dfb97d: Preparing\n",
      "dc8463ac1279: Preparing\n",
      "5520158f7ef1: Preparing\n",
      "152607a648ba: Preparing\n",
      "62bb75d94c35: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "1a1ffc16734e: Preparing\n",
      "8449535b89d9: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "9a2ad63887b6: Preparing\n",
      "bca2d3c0b431: Preparing\n",
      "954c82bdeb5f: Preparing\n",
      "f6face19c9dd: Waiting\n",
      "92affaab67e8: Waiting\n",
      "8c672b5f8a22: Waiting\n",
      "73066cf8673e: Waiting\n",
      "2ca62cf21221: Waiting\n",
      "b9a424722d3f: Waiting\n",
      "8a11b9d4bbb9: Waiting\n",
      "f2e60533bd85: Waiting\n",
      "f8eee1652f88: Waiting\n",
      "ddac17ce4d28: Waiting\n",
      "731f70dfb97d: Waiting\n",
      "dc8463ac1279: Waiting\n",
      "5520158f7ef1: Waiting\n",
      "152607a648ba: Waiting\n",
      "62bb75d94c35: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "1a1ffc16734e: Waiting\n",
      "8449535b89d9: Waiting\n",
      "9a2ad63887b6: Waiting\n",
      "bca2d3c0b431: Waiting\n",
      "954c82bdeb5f: Waiting\n",
      "5a7ed725a946: Layer already exists\n",
      "073a5f3bc8ec: Layer already exists\n",
      "92affaab67e8: Layer already exists\n",
      "f6face19c9dd: Layer already exists\n",
      "73066cf8673e: Layer already exists\n",
      "8c672b5f8a22: Layer already exists\n",
      "2ca62cf21221: Layer already exists\n",
      "b9a424722d3f: Layer already exists\n",
      "8a11b9d4bbb9: Layer already exists\n",
      "f2e60533bd85: Layer already exists\n",
      "f8eee1652f88: Layer already exists\n",
      "ddac17ce4d28: Layer already exists\n",
      "731f70dfb97d: Layer already exists\n",
      "dc8463ac1279: Layer already exists\n",
      "5520158f7ef1: Layer already exists\n",
      "152607a648ba: Layer already exists\n",
      "62bb75d94c35: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "1a1ffc16734e: Layer already exists\n",
      "8449535b89d9: Layer already exists\n",
      "9a2ad63887b6: Layer already exists\n",
      "bca2d3c0b431: Layer already exists\n",
      "954c82bdeb5f: Layer already exists\n",
      "254055ea96b4: Pushed\n",
      "c1d33a5bd101: Pushed\n",
      "65f978f9da4f: Pushed\n",
      "latest: digest: sha256:f1fa9f28c1c110c745d1c8a2a5c4eb643c7e5a0edcc8d815cf72763928b2d746 size: 5965\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                               STATUS\n",
      "417869c1-a221-46b4-8feb-6237e5168f17  2024-05-17T04:37:43+00:00  3M57S     gs://qwiklabs-gcp-04-71169a1ff84c_cloudbuild/source/1715920663.178602-f4502ff022a544fc9f27690b4a95930d.tgz  gcr.io/qwiklabs-gcp-04-71169a1ff84c/xgboost_trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $XGB_IMAGE_URI $XGB_IMAGE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop KubeFlow Pipeline\n",
    "Now that you have all four of your training applications as containers in your project's Container Registry, let's build a KubeFlow pipeline. \n",
    "\n",
    "The KubeFlow pipeline will have two BigQuery Ops. We will use the pre-built BigQuery Query component (no need to reinvent the wheel) to do the following: \n",
    "* Create a training split in our data and export to CSV\n",
    "* Create a validation split in our data and export to CSV\n",
    "\n",
    "The output of these BigQuery Ops will be the input data into four AI Platform Training Ops. For this we will also use a pre-built component. Each AI Platform Training Op will train one of our containerized models - Tensorflow, PyTorch, XGBoost, and Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./pipeline/census_training_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/census_training_pipeline.py\n",
    "\n",
    "import os\n",
    "import kfp\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "import kfp.components as comp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import json\n",
    "\n",
    "# We will use environment vars to set the trainer image names and bucket name\n",
    "TF_TRAINER_IMAGE = os.getenv('TF_TRAINER_IMAGE')\n",
    "SCIKIT_TRAINER_IMAGE = os.getenv('SCIKIT_TRAINER_IMAGE')\n",
    "TORCH_TRAINER_IMAGE = os.getenv('TORCH_TRAINER_IMAGE')\n",
    "XGB_TRAINER_IMAGE = os.getenv('XGB_TRAINER_IMAGE')\n",
    "BUCKET = os.getenv('BUCKET')\n",
    "\n",
    "# Paths to export the training/validation data from bigquery\n",
    "TRAINING_OUTPUT_PATH = BUCKET + '/census/data/training.csv'\n",
    "VALIDATION_OUTPUT_PATH = BUCKET + '/census/data/validation.csv'\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "# Load BigQuery and AI Platform Training op\n",
    "bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "\n",
    "def get_query(dataset='training'):\n",
    "    \"\"\"Function that returns either training or validation query\"\"\"\n",
    "    if dataset=='training':\n",
    "        split = \"MOD(ABS(FARM_FINGERPRINT(CAST(functional_weight AS STRING))), 100) < 80\"\n",
    "    elif dataset=='validation':\n",
    "        split = \"\"\"MOD(ABS(FARM_FINGERPRINT(CAST(functional_weight AS STRING))), 100) >= 80 \n",
    "        AND MOD(ABS(FARM_FINGERPRINT(CAST(functional_weight AS STRING))), 100) < 90\"\"\"\n",
    "    else:\n",
    "        split = \"MOD(ABS(FARM_FINGERPRINT(CAST(functional_weight AS STRING))), 100) >= 90\"\n",
    "        \n",
    "    query = \"\"\"SELECT age, workclass, education_num, occupation, hours_per_week,income_bracket \n",
    "    FROM census.data \n",
    "    WHERE {0}\"\"\".format(split)\n",
    "    \n",
    "    return query\n",
    "\n",
    "# We will use the training/validation queries as inputs to our pipeline\n",
    "# This lets us change the training/validation datasets if we wish by simply\n",
    "# Changing the query. \n",
    "TRAIN_QUERY = get_query(dataset='training')\n",
    "VALIDATION_QUERY=get_query(dataset='validation')\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Continuous Training with Multiple Frameworks',\n",
    "    description='Pipeline to create training/validation splits w/ BigQuery then launch multiple AI Platform Training Jobs'\n",
    ")\n",
    "def pipeline(\n",
    "    project_id,\n",
    "    train_query=TRAIN_QUERY,\n",
    "    validation_query=VALIDATION_QUERY,\n",
    "    region='us-central1'\n",
    "):\n",
    "    # Creating the training data split\n",
    "    create_training_split = bigquery_query_op(\n",
    "        query=train_query,\n",
    "        project_id=project_id,\n",
    "        output_gcs_path=TRAINING_OUTPUT_PATH\n",
    "    ).set_display_name('BQ Train Split')\n",
    "    \n",
    "    # Creating the validation data split\n",
    "    create_validation_split = bigquery_query_op(\n",
    "        query=validation_query,\n",
    "        project_id=project_id,\n",
    "        output_gcs_path=VALIDATION_OUTPUT_PATH\n",
    "    ).set_display_name('BQ Eval Split')\n",
    "    \n",
    "    # These are the output directories where our models will be saved\n",
    "    tf_output_dir = BUCKET + '/census/models/tf'\n",
    "    scikit_output_dir = BUCKET + '/census/models/scikit'\n",
    "    torch_output_dir = BUCKET + '/census/models/torch'\n",
    "    xgb_output_dir = BUCKET + '/census/models/xgb'\n",
    "    \n",
    "    # Training arguments to be passed to the TF Trainer\n",
    "    tf_args = [\n",
    "        '--training_dataset_path', create_training_split.outputs['output_gcs_path'],\n",
    "        '--validation_dataset_path', create_validation_split.outputs['output_gcs_path'],\n",
    "        '--output_dir', tf_output_dir,\n",
    "        '--batch_size', '32', \n",
    "        '--num_train_examples', '1000',\n",
    "        '--num_evals', '10'\n",
    "    ]\n",
    "    \n",
    "    # Training arguments to be passed to the Scikit-learn Trainer\n",
    "    scikit_args = [\n",
    "        '--training_dataset_path', create_training_split.outputs['output_gcs_path'],\n",
    "        '--validation_dataset_path', create_validation_split.outputs['output_gcs_path'],\n",
    "        '--output_dir', scikit_output_dir\n",
    "    ]\n",
    "    \n",
    "    # Training arguments to be passed to the PyTorch Trainer\n",
    "    torch_args = [\n",
    "        '--training_dataset_path', create_training_split.outputs['output_gcs_path'],\n",
    "        '--validation_dataset_path', create_validation_split.outputs['output_gcs_path'],\n",
    "        '--output_dir', torch_output_dir,\n",
    "        '--batch_size', '32', \n",
    "        '--num_epochs', '15',\n",
    "    ]\n",
    "    \n",
    "    # Training arguments to be passed to the XGBoost Trainer \n",
    "    xgb_args = [\n",
    "        '--training_dataset_path', create_training_split.outputs['output_gcs_path'],\n",
    "        '--validation_dataset_path', create_validation_split.outputs['output_gcs_path'],\n",
    "        '--output_dir', xgb_output_dir,\n",
    "        '--max_depth', '10', \n",
    "        '--n_estimators', '100'\n",
    "    ]\n",
    "    \n",
    "    # AI Platform Training Jobs with all 4 trainer images \n",
    "    \n",
    "    train_scikit = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=SCIKIT_TRAINER_IMAGE,\n",
    "        args=scikit_args).set_display_name('Scikit Model - AI Platform Training')\n",
    "    \n",
    "    train_tf = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TF_TRAINER_IMAGE,\n",
    "        args=tf_args).set_display_name('Tensorflow Model - AI Platform Training')\n",
    "    \n",
    "    train_torch = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TORCH_TRAINER_IMAGE,\n",
    "        args=torch_args).set_display_name('Pytorch Model - AI Platform Training')\n",
    "    \n",
    "    train_xgb = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=XGB_TRAINER_IMAGE,\n",
    "        args=xgb_args).set_display_name('XGBoost Model - AI Platform Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set environment variables for the different trainer image names as well as our bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG = 'latest'\n",
    "SCIKIT_TRAINER_IMAGE = 'gcr.io/{}/scikit_trainer_image:{}'.format(PROJECT_ID, TAG)\n",
    "TF_TRAINER_IMAGE = 'gcr.io/{}/tensorflow_trainer_image:{}'.format(PROJECT_ID, TAG)\n",
    "TORCH_TRAINER_IMAGE = 'gcr.io/{}/pytorch_trainer_image:{}'.format(PROJECT_ID, TAG)\n",
    "XGB_TRAINER_IMAGE = 'gcr.io/{}/xgboost_trainer_image:{}'.format(PROJECT_ID, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_TRAINER_IMAGE=gcr.io/qwiklabs-gcp-04-71169a1ff84c/tensorflow_trainer_image:latest\n",
      "env: SCIKIT_TRAINER_IMAGE=gcr.io/qwiklabs-gcp-04-71169a1ff84c/scikit_trainer_image:latest\n",
      "env: TORCH_TRAINER_IMAGE=gcr.io/qwiklabs-gcp-04-71169a1ff84c/pytorch_trainer_image:latest\n",
      "env: XGB_TRAINER_IMAGE=gcr.io/qwiklabs-gcp-04-71169a1ff84c/xgboost_trainer_image:latest\n",
      "env: BUCKET=gs://qwiklabs-gcp-04-71169a1ff84c\n"
     ]
    }
   ],
   "source": [
    "%env TF_TRAINER_IMAGE={TF_TRAINER_IMAGE}\n",
    "%env SCIKIT_TRAINER_IMAGE={SCIKIT_TRAINER_IMAGE}\n",
    "%env TORCH_TRAINER_IMAGE={TORCH_TRAINER_IMAGE}\n",
    "%env XGB_TRAINER_IMAGE={XGB_TRAINER_IMAGE}\n",
    "%env BUCKET={BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Pipeline\n",
    "Compile the pipeline with the CLI compiler. This will save a census_training_pipeline.yaml file locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py pipeline/census_training_pipeline.py --output census_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the head of the yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"apiVersion\": |-\n",
      "  argoproj.io/v1alpha1\n",
      "\"kind\": |-\n",
      "  Workflow\n",
      "\"metadata\":\n",
      "  \"annotations\":\n",
      "    \"pipelines.kubeflow.org/pipeline_spec\": |-\n",
      "      {\"description\": \"Pipeline to create training/validation splits w/ BigQuery then launch multiple AI Platform Training Jobs\", \"inputs\": [{\"name\": \"project_id\"}, {\"default\": \"SELECT age, workclass, education_num, occupation, hours_per_week,income_bracket \\n    FROM census.data \\n    WHERE MOD(ABS(FARM_FINGERPRINT(CAST(functional_weight AS STRING))), 100) < 80\", \"name\": \"train_query\", \"optional\": true}, {\"default\": \"SELECT age, workclass, education_num, occupation, hours_per_week,income_bracket \\n    FROM census.data \\n    WHERE MOD(ABS(FARM_FINGERPRINT(CAST(functional_weight AS STRING))), 100) >= 80 \\n        AND MOD(ABS(FARM_FINGERPRINT(CAST(functional_weight AS STRING))), 100) < 90\", \"name\": \"validation_query\", \"optional\": true}, {\"default\": \"us-central1\", \"name\": \"region\", \"optional\": true}], \"name\": \"Continuous Training with Multiple Frameworks\"}\n",
      "  \"generateName\": |-\n",
      "    continuous-training-with-multiple-frameworks-\n"
     ]
    }
   ],
   "source": [
    "!head census_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the command fields in the pipeline YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 's/\\\"command\\\": \\[\\]/\\\"command\\\": \\[python, -u, -m, kfp_component.launcher\\]/g' census_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      \"command\": [python, -u, -m, kfp_component.launcher]\n",
      "      \"command\": [python, -u, -m, kfp_component.launcher]\n",
      "      \"command\": [python, -u, -m, kfp_component.launcher]\n",
      "      \"command\": [python, -u, -m, kfp_component.launcher]\n",
      "      \"command\": [python, -u, -m, kfp_component.launcher]\n",
      "      \"command\": [python, -u, -m, kfp_component.launcher]\n"
     ]
    }
   ],
   "source": [
    "!cat census_training_pipeline.yaml | grep \"component.launcher\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see 6 lines in the output that were modified by the sed command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy your KubeFlow Pipeline\n",
    "Now let's deploy the KubeFlow pipeline. Prior to the lab you should have spun up an AI Platform Pipelines Instance. In the AI Platform Pipeline UI click 'settings' on your pipeline and copy the endpoint. Paste the endpoint as the value for the string variable ENDPOINT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Change ENDPOINT to the ENDPOINT for your AI Platform Pipelines Instance\n",
    "ENDPOINT = '21b500e44afdc1cf-dot-us-central1.pipelines.googleusercontent.com'\n",
    "PIPELINE_NAME = 'census_trainer_multiple_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 2214fdb4-ee00-4673-b0e9-a8e069b9d97a has been submitted\n",
      "\n",
      "Pipeline Details\n",
      "------------------\n",
      "ID           2214fdb4-ee00-4673-b0e9-a8e069b9d97a\n",
      "Name         census_trainer_multiple_models\n",
      "Description\n",
      "Uploaded at  2024-05-17T04:58:26+00:00\n",
      "+------------------+-------------------------------------------------------------------------------------+\n",
      "| Parameter Name   | Default Value                                                                       |\n",
      "+==================+=====================================================================================+\n",
      "| project_id       |                                                                                     |\n",
      "+------------------+-------------------------------------------------------------------------------------+\n",
      "| train_query      | SELECT age, workclass, education_num, occupation, hours_per_week,income_bracket     |\n",
      "|                  |     FROM census.data                                                                |\n",
      "|                  |     WHERE MOD(ABS(FARM_FINGERPRINT(CAST(functional_weight AS STRING))), 100) < 80   |\n",
      "+------------------+-------------------------------------------------------------------------------------+\n",
      "| validation_query | SELECT age, workclass, education_num, occupation, hours_per_week,income_bracket     |\n",
      "|                  |     FROM census.data                                                                |\n",
      "|                  |     WHERE MOD(ABS(FARM_FINGERPRINT(CAST(functional_weight AS STRING))), 100) >= 80  |\n",
      "|                  |         AND MOD(ABS(FARM_FINGERPRINT(CAST(functional_weight AS STRING))), 100) < 90 |\n",
      "+------------------+-------------------------------------------------------------------------------------+\n",
      "| region           | us-central1                                                                         |\n",
      "+------------------+-------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "./census_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Training: Create Pipeline Run, then Create Recurring Runs in the UI. \n",
    "Now that we deployed our KubeFlow pipeline, let's head over to the UI and launch a pipeline run. You can either click **Open Pipelines Dashboard** in the AI Platform Pipeline UI or run the Python cell below and copy/paste the output into the browser. Once in the UI, complete the following steps:\n",
    "* Select **Pipelines** on the left-hand navigation panel\n",
    "* Select **census_trainer_multiple_models** then click **Create Run**\n",
    "* In the **Experiment** field select **Default**\n",
    "* For **Run Type** select **One-Off**\n",
    "* Enter your **Project ID** and hit **Start**. You can now monitor your pipeline run in the UI (it will take about **10 minutes** to complete the run).\n",
    "\n",
    "\n",
    "**NOTE that your pipeline run may fail due to the bug in a BigQuery component that does not handle certain race conditions. If you observe the pipeline failure, please click on Retry on the top right in the KFP UI to re-run the failed steps.**\n",
    "\n",
    "\n",
    "Now let's set up a recurring run:\n",
    "* Select **Pipelines** then census_trainer_multiple_models and click **Create Run**\n",
    "* In the **Experiement** field select **Default**\n",
    "* For **Run Type** select **Recurring**\n",
    "* Configure the **Recurring Run** to start tomorrow at 5pm and run once weekly\n",
    "* Enter your **Project ID** and hit **Start**\n",
    "\n",
    "Now your pipeline will run weekly starting tomorrow, it's as easy as that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://21b500e44afdc1cf-dot-us-central1.pipelines.googleusercontent.com\n"
     ]
    }
   ],
   "source": [
    "print(f\"https://{ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Lab, Congrats!\n",
    "In this lab you created containerized training applications for TensorFlow, PyTorch, Scikit-learn, an XGBoost models. You then created a KubeFlow pipeline that used pre built components to create training/validation splits in BigQuery data, export that data as CSV files to GCS, and launch AI Platform Training Jobs with the four containerized training applications. Finally, you ran the pipeline from the UI and set up Continuous Training to re-run your pipeline once a week!"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu121.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
