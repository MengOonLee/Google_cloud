{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199da007-ba55-4d5e-8370-d71a9916176f",
   "metadata": {},
   "source": [
    "# SQL Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601f0e4-8443-418b-a709-e5beabb51060",
   "metadata": {},
   "source": [
    "## Cloud Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9357c-d7c6-4d98-b49b-1c7cef520863",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud auth list\n",
    "\n",
    "gcloud config list project\n",
    "\n",
    "gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeae231-88ff-4aaa-a7fd-cdbd416f0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install -y virtualenv\n",
    "\n",
    "virtualenv -p python3 venv\n",
    "\n",
    "source venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce164eb-00fb-47a5-977d-01eb49f56ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "export PROJECT_ID=$(gcloud info --format='value(config.project)')\n",
    "\n",
    "gsutil mb gs://${PROJECT_ID}-ml\n",
    "\n",
    "bq mk --project_id $PROJECT_ID flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a069816-28e7-486c-8007-1e2c11d504b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "export BUCKET=${PROJECT_ID}-ml\n",
    "\n",
    "gsutil cp airports.csv.gz \\\n",
    "gs://$BUCKET/flights/airports/airports.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6743d4-fe72-47a2-88dd-6157562d25da",
   "metadata": {},
   "outputs": [],
   "source": [
    "export ADDRESS=$(wget -qO - http://ipecho.net/plain)/32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddfc2a8-828b-4c6e-83b5-cc176d1ba719",
   "metadata": {},
   "outputs": [],
   "source": [
    "MYSQLIP=$(gcloud sql instances describe \\\n",
    "flights --format=\"value(ipAddresses.ipAddress)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6bef8-5635-453f-88f2-7b1bf499b55d",
   "metadata": {},
   "source": [
    "## SELECT, FROM, WHERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267cb176-6f65-4c83-8503-1ec0eafa8d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT * \n",
    "FROM \n",
    "    `bigquery-public-data.london_bicycles.cycle_hire` \n",
    "WHERE \n",
    "    duration>=1200;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5af339-c448-4cba-859b-44e481dac473",
   "metadata": {},
   "source": [
    "## GROUP BY, COUNT, AS, ORDER BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d851618-940c-47fb-9d7d-8107748a3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \n",
    "    start_station_name \n",
    "FROM \n",
    "    `bigquery-public-data.london_bicycles.cycle_hire` \n",
    "GROUP BY \n",
    "    start_station_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e5385-254f-40a2-b35c-1f9ad3895b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \n",
    "    start_station_name, \n",
    "    COUNT(*) AS num_starts \n",
    "FROM \n",
    "    `bigquery-public-data.london_bicycles.cycle_hire` \n",
    "GROUP BY \n",
    "    start_station_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32268528-3ef4-4d2d-8a64-366cda91e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT \n",
    "    start_station_name, \n",
    "    COUNT(*) AS num \n",
    "FROM \n",
    "    `bigquery-public-data.london_bicycles.cycle_hire` \n",
    "GROUP BY \n",
    "    start_station_name \n",
    "ORDER BY \n",
    "    num DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95341d-94a4-4ac4-a979-c63b03b8a07a",
   "metadata": {},
   "source": [
    "## Process data using Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54471be5-4e70-47c0-a022-4c31edb26004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import csv\n",
    "\n",
    "DATETIME_FORMAT='%Y-%m-%dT%H:%M:%S'\n",
    "\n",
    "def addtimezone(lat, lon):\n",
    "   try:\n",
    "      import timezonefinder\n",
    "      tf = timezonefinder.TimezoneFinder()\n",
    "      return (lat, lon, tf.timezone_at(lng=float(lon), lat=float(lat)))\n",
    "      #return (lat, lon, 'America/Los_Angeles') # FIXME\n",
    "   except ValueError:\n",
    "      return (lat, lon, 'TIMEZONE') # header\n",
    "\n",
    "def as_utc(date, hhmm, tzone):\n",
    "   try:\n",
    "      if len(hhmm) > 0 and tzone is not None:\n",
    "         import datetime, pytz\n",
    "         loc_tz = pytz.timezone(tzone)\n",
    "         loc_dt = loc_tz.localize(datetime.datetime.strptime(date,'%Y-%m-%d'), is_dst=False)\n",
    "         # can't just parse hhmm because the data contains 2400 and the like ...\n",
    "         loc_dt += datetime.timedelta(hours=int(hhmm[:2]), minutes=int(hhmm[2:]))\n",
    "         utc_dt = loc_dt.astimezone(pytz.utc)\n",
    "         return utc_dt.strftime(DATETIME_FORMAT), loc_dt.utcoffset().total_seconds()\n",
    "      else:\n",
    "         return '',0 # empty string corresponds to canceled flights\n",
    "   except ValueError as e:\n",
    "      print ('{} {} {}'.format(date, hhmm, tzone))\n",
    "      raise e\n",
    "\n",
    "def add_24h_if_before(arrtime, deptime):\n",
    "   import datetime\n",
    "   if len(arrtime) > 0 and len(deptime) > 0 and (arrtime < deptime):\n",
    "      adt = datetime.datetime.strptime(arrtime, DATETIME_FORMAT)\n",
    "      adt += datetime.timedelta(hours=24)\n",
    "      return adt.strftime(DATETIME_FORMAT)\n",
    "   else:\n",
    "      return arrtime\n",
    "\n",
    "def tz_correct(line, airport_timezones_dict):\n",
    "   def airport_timezone(airport_id):\n",
    "       if airport_id in airport_timezones_dict:\n",
    "          return airport_timezones_dict[airport_id]\n",
    "       else:\n",
    "          return ('37.52', '-92.17', u'America/Chicago') # population center of US\n",
    "   fields = line.split(',')\n",
    "   if fields[0] != 'FL_DATE' and len(fields) == 27:\n",
    "      # convert all times to UTC\n",
    "      dep_airport_id = fields[6]\n",
    "      arr_airport_id = fields[10]\n",
    "      dep_timezone = airport_timezone(dep_airport_id)[2] \n",
    "      arr_timezone = airport_timezone(arr_airport_id)[2]\n",
    "      \n",
    "      for f in [13, 14, 17]: #crsdeptime, deptime, wheelsoff\n",
    "         fields[f], deptz = as_utc(fields[0], fields[f], dep_timezone)\n",
    "      for f in [18, 20, 21]: #wheelson, crsarrtime, arrtime\n",
    "         fields[f], arrtz = as_utc(fields[0], fields[f], arr_timezone)\n",
    "      \n",
    "      for f in [17, 18, 20, 21]:\n",
    "         fields[f] = add_24h_if_before(fields[f], fields[14])\n",
    "\n",
    "      fields.extend(airport_timezone(dep_airport_id))\n",
    "      fields[-1] = str(deptz)\n",
    "      fields.extend(airport_timezone(arr_airport_id))\n",
    "      fields[-1] = str(arrtz)\n",
    "\n",
    "      yield fields\n",
    "\n",
    "def get_next_event(fields):\n",
    "    if len(fields[14]) > 0:\n",
    "       event = list(fields) # copy\n",
    "       event.extend(['departed', fields[14]])\n",
    "       for f in [16,17,18,19,21,22,25]:\n",
    "          event[f] = ''  # not knowable at departure time\n",
    "       yield event\n",
    "    if len(fields[17]) > 0:\n",
    "       event = list(fields) # copy\n",
    "       event.extend(['wheelsoff', fields[17]])\n",
    "       for f in [18,19,21,22,25]:\n",
    "          event[f] = ''  # not knowable at wheelsoff time\n",
    "       yield event\n",
    "    if len(fields[21]) > 0:\n",
    "       event = list(fields)\n",
    "       event.extend(['arrived', fields[21]])\n",
    "       yield event\n",
    "\n",
    "def create_row(fields):\n",
    "    header = 'FL_DATE,UNIQUE_CARRIER,AIRLINE_ID,CARRIER,FL_NUM,ORIGIN_AIRPORT_ID,ORIGIN_AIRPORT_SEQ_ID,ORIGIN_CITY_MARKET_ID,ORIGIN,DEST_AIRPORT_ID,DEST_AIRPORT_SEQ_ID,DEST_CITY_MARKET_ID,DEST,CRS_DEP_TIME,DEP_TIME,DEP_DELAY,TAXI_OUT,WHEELS_OFF,WHEELS_ON,TAXI_IN,CRS_ARR_TIME,ARR_TIME,ARR_DELAY,CANCELLED,CANCELLATION_CODE,DIVERTED,DISTANCE,DEP_AIRPORT_LAT,DEP_AIRPORT_LON,DEP_AIRPORT_TZOFFSET,ARR_AIRPORT_LAT,ARR_AIRPORT_LON,ARR_AIRPORT_TZOFFSET,EVENT,NOTIFY_TIME'.split(',')\n",
    "\n",
    "    featdict = {}\n",
    "    for name, value in zip(header, fields):\n",
    "        featdict[name] = value\n",
    "    featdict['EVENT_DATA'] = ','.join(fields)\n",
    "    return featdict\n",
    " \n",
    "def run(project, bucket, dataset, region):\n",
    "   argv = [\n",
    "      '--project={0}'.format(project),\n",
    "      '--job_name=ch04timecorr',\n",
    "      '--save_main_session',\n",
    "      '--staging_location=gs://{0}/flights/staging/'.format(bucket),\n",
    "      '--temp_location=gs://{0}/flights/temp/'.format(bucket),\n",
    "      '--setup_file=./setup.py',\n",
    "      '--max_num_workers=8',\n",
    "      '--region={}'.format(region),\n",
    "      '--autoscaling_algorithm=THROUGHPUT_BASED',\n",
    "      '--runner=DataflowRunner'\n",
    "   ]\n",
    "   airports_filename = 'gs://{}/flights/airports/airports.csv.gz'.format(bucket)\n",
    "   flights_raw_files = 'gs://{}/flights/raw/*.csv'.format(bucket)\n",
    "   flights_output = 'gs://{}/flights/tzcorr/all_flights'.format(bucket)\n",
    "   events_output = '{}:{}.simevents'.format(project, dataset)\n",
    "\n",
    "   pipeline = beam.Pipeline(argv=argv)\n",
    "   \n",
    "   airports = (pipeline \n",
    "      | 'airports:read' >> beam.io.ReadFromText(airports_filename)\n",
    "      | 'airports:fields' >> beam.Map(lambda line: next(csv.reader([line])))\n",
    "      | 'airports:tz' >> beam.Map(lambda fields: (fields[0], addtimezone(fields[21], fields[26])))\n",
    "   )\n",
    "\n",
    "   flights = (pipeline \n",
    "      | 'flights:read' >> beam.io.ReadFromText (flights_raw_files)\n",
    "      | 'flights:tzcorr' >> beam.FlatMap(tz_correct, beam.pvalue.AsDict(airports))\n",
    "   )\n",
    "\n",
    "   (flights \n",
    "      | 'flights:tostring' >> beam.Map(lambda fields: ','.join(fields)) \n",
    "      | 'flights:out' >> beam.io.textio.WriteToText(flights_output)\n",
    "   )\n",
    "\n",
    "   events = flights | beam.FlatMap(get_next_event)\n",
    "\n",
    "   schema = 'FL_DATE:date,UNIQUE_CARRIER:string,AIRLINE_ID:string,CARRIER:string,FL_NUM:string,ORIGIN_AIRPORT_ID:string,ORIGIN_AIRPORT_SEQ_ID:integer,ORIGIN_CITY_MARKET_ID:string,ORIGIN:string,DEST_AIRPORT_ID:string,DEST_AIRPORT_SEQ_ID:integer,DEST_CITY_MARKET_ID:string,DEST:string,CRS_DEP_TIME:timestamp,DEP_TIME:timestamp,DEP_DELAY:float,TAXI_OUT:float,WHEELS_OFF:timestamp,WHEELS_ON:timestamp,TAXI_IN:float,CRS_ARR_TIME:timestamp,ARR_TIME:timestamp,ARR_DELAY:float,CANCELLED:string,CANCELLATION_CODE:string,DIVERTED:string,DISTANCE:float,DEP_AIRPORT_LAT:float,DEP_AIRPORT_LON:float,DEP_AIRPORT_TZOFFSET:float,ARR_AIRPORT_LAT:float,ARR_AIRPORT_LON:float,ARR_AIRPORT_TZOFFSET:float,EVENT:string,NOTIFY_TIME:timestamp,EVENT_DATA:string'\n",
    "\n",
    "   (events \n",
    "      | 'events:totablerow' >> beam.Map(lambda fields: create_row(fields)) \n",
    "      | 'events:out' >> beam.io.WriteToBigQuery(\n",
    "                              events_output, schema=schema,\n",
    "                              write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "                              create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n",
    "   )\n",
    "\n",
    "   pipeline.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   import argparse\n",
    "   parser = argparse.ArgumentParser(description='Run pipeline on the cloud')\n",
    "   parser.add_argument('-p','--project', help='Unique project ID', required=True)\n",
    "   parser.add_argument('-b','--bucket', help='Bucket where your data were ingested in Chapter 2', required=True)\n",
    "   parser.add_argument('-r','--region', help='Region in which to run the Dataflow job. Choose the same region as your bucket.', required=True)\n",
    "   parser.add_argument('-d','--dataset', help='BigQuery dataset', default='flights')\n",
    "   args = vars(parser.parse_args())\n",
    "\n",
    "   print (\"Correcting timestamps and writing to BigQuery dataset {}\".format(args['dataset']))\n",
    "  \n",
    "   run(project=args['project'], bucket=args['bucket'], dataset=args['dataset'], region=args['region'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1e974-40c9-420f-ba08-55cf5263b507",
   "metadata": {},
   "source": [
    "## Inspect the processed data in Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107e0530-de4f-4acd-858f-3a4442750043",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT\n",
    "  FL_NUM,\n",
    "  ORIGIN,\n",
    "  DEP_TIME,\n",
    "  DEP_DELAY,\n",
    "  DEST,\n",
    "  ARR_TIME,\n",
    "  ARR_DELAY,\n",
    "  EVENT,\n",
    "  NOTIFY_TIME\n",
    "FROM\n",
    "  `flightssample.simevents`\n",
    "WHERE\n",
    "  (DEP_DELAY > 15 and ORIGIN = 'SEA') or\n",
    "  (ARR_DELAY > 15 and DEST = 'SEA')\n",
    "ORDER BY\n",
    "  NOTIFY_TIME ASC\n",
    "LIMIT\n",
    "  10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152473e-bd75-4935-bb9b-508139623afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT\n",
    "  EVENT,\n",
    "  NOTIFY_TIME,\n",
    "  EVENT_DATA\n",
    "FROM\n",
    "  `flightssample.simevents`\n",
    "WHERE\n",
    "  NOTIFY_TIME >= TIMESTAMP('2015-01-01 00:00:00 UTC')\n",
    "  AND NOTIFY_TIME < TIMESTAMP('2015-01-03 00:00:00 UTC')\n",
    "ORDER BY\n",
    "  NOTIFY_TIME ASC\n",
    "LIMIT\n",
    "  10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4358a7d-3444-497c-8f8f-0a96a6075383",
   "metadata": {},
   "source": [
    "## Create the real-time Google Dataflow stream processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34e090-82ec-4402-abaf-8cca8019799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pytz\n",
    "import logging\n",
    "import argparse\n",
    "import datetime\n",
    "from google.cloud import pubsub_v1 # Upgrading The Library\n",
    "import google.cloud.bigquery as bq\n",
    "\n",
    "TIME_FORMAT = '%Y-%m-%d %H:%M:%S %Z'\n",
    "RFC3339_TIME_FORMAT = '%Y-%m-%dT%H:%M:%S-00:00'\n",
    "\n",
    "def publish(publisher, topics, allevents, notify_time):\n",
    "   timestamp = notify_time.strftime(RFC3339_TIME_FORMAT)\n",
    "   for key in topics:  # 'departed', 'arrived', etc.\n",
    "      topic = topics[key]\n",
    "      events = allevents[key]\n",
    "      # the client automatically batches\n",
    "      logging.info('Publishing {} {} till {}'.format(len(events), key, timestamp))\n",
    "      for event_data in events:\n",
    "          publisher.publish(topic, event_data.encode(), EventTimeStamp=timestamp)\n",
    "\n",
    "def notify(publisher, topics, rows, simStartTime, programStart, speedFactor):\n",
    "   # sleep computation\n",
    "   def compute_sleep_secs(notify_time):\n",
    "        time_elapsed = (datetime.datetime.utcnow() - programStart).total_seconds()\n",
    "        sim_time_elapsed = (notify_time - simStartTime).total_seconds() / speedFactor\n",
    "        to_sleep_secs = sim_time_elapsed - time_elapsed\n",
    "        return to_sleep_secs\n",
    "\n",
    "   tonotify = {}\n",
    "   for key in topics:\n",
    "     tonotify[key] = list()\n",
    "\n",
    "   for row in rows:\n",
    "       event, notify_time, event_data = row\n",
    "\n",
    "       # how much time should we sleep?\n",
    "       if compute_sleep_secs(notify_time) > 1:\n",
    "          # notify the accumulated tonotify\n",
    "          publish(publisher, topics, tonotify, notify_time)\n",
    "          for key in topics:\n",
    "             tonotify[key] = list()\n",
    "\n",
    "          # recompute sleep, since notification takes a while\n",
    "          to_sleep_secs = compute_sleep_secs(notify_time)\n",
    "          if to_sleep_secs > 0:\n",
    "             logging.info('Sleeping {} seconds'.format(to_sleep_secs))\n",
    "             time.sleep(to_sleep_secs)\n",
    "       tonotify[event].append(event_data)\n",
    "\n",
    "   # left-over records; notify again\n",
    "   publish(publisher, topics, tonotify, notify_time)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   parser = argparse.ArgumentParser(description='Send simulated flight events to Cloud Pub/Sub')\n",
    "   parser.add_argument('--startTime', help='Example: 2015-05-01 00:00:00 UTC', required=True)\n",
    "   parser.add_argument('--endTime', help='Example: 2015-05-03 00:00:00 UTC', required=True)\n",
    "   parser.add_argument('--project', help='your project id, to create pubsub topic', required=True)\n",
    "   parser.add_argument('--speedFactor', help='Example: 60 implies 1 hour of data sent to Cloud Pub/Sub in 1 minute', required=True, type=float)\n",
    "   parser.add_argument('--jitter', help='type of jitter to add: None, uniform, exp  are the three options', default='None')\n",
    "\n",
    "   # set up BigQuery bqclient\n",
    "   logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO)\n",
    "   args = parser.parse_args()\n",
    "   bqclient = bq.Client(args.project)\n",
    "   dataset =  bqclient.get_dataset( bqclient.dataset('flights') )  # throws exception on failure\n",
    "\n",
    "   # jitter?\n",
    "   if args.jitter == 'exp':\n",
    "      jitter = 'CAST (-LN(RAND()*0.99 + 0.01)*30 + 90.5 AS INT64)'\n",
    "   elif args.jitter == 'uniform':\n",
    "      jitter = 'CAST(90.5 + RAND()*30 AS INT64)'\n",
    "   else:\n",
    "      jitter = '0'\n",
    "\n",
    "\n",
    "   # run the query to pull simulated events\n",
    "   querystr = \"\"\"\n",
    "SELECT\n",
    "  EVENT,\n",
    "  TIMESTAMP_ADD(NOTIFY_TIME, INTERVAL {} SECOND) AS NOTIFY_TIME,\n",
    "  EVENT_DATA\n",
    "FROM\n",
    "  flights.simevents\n",
    "WHERE\n",
    "  NOTIFY_TIME >= TIMESTAMP('{}')\n",
    "  AND NOTIFY_TIME < TIMESTAMP('{}')\n",
    "ORDER BY\n",
    "  NOTIFY_TIME ASC\n",
    "\"\"\"\n",
    "   rows = bqclient.query(querystr.format(jitter,\n",
    "                                         args.startTime,\n",
    "                                         args.endTime))\n",
    "\n",
    "   # create one Pub/Sub notification topic for each type of event\n",
    "   publisher = pubsub_v1.PublisherClient()\n",
    "   topics = {}\n",
    "   for event_type in ['wheelsoff', 'arrived', 'departed']:\n",
    "       topics[event_type] = publisher.topic_path(args.project, event_type)\n",
    "       try:\n",
    "         # Getting the new topics from PubSub\n",
    "          for topic in publisher.list_topics(request={\"project\": project_path}):\n",
    "                print(topic)\n",
    "       except:\n",
    "         #Creating New topics\n",
    "           publisher.create_topic(request={\"name\": topics[event_type]})\n",
    "\n",
    "   # notify about each row in the dataset\n",
    "   programStartTime = datetime.datetime.utcnow()\n",
    "   simStartTime = datetime.datetime.strptime(args.startTime, TIME_FORMAT).replace(tzinfo=pytz.UTC)\n",
    "   print('Simulation start time is {}'.format(simStartTime))\n",
    "   notify(publisher, topics, rows, simStartTime, programStartTime, args.speedFactor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c220a3bb-537d-43a5-8200-de50f317b1a6",
   "metadata": {},
   "source": [
    "## The delay data for all events that have happened within the last 30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b782286-85c5-4a04-b87b-0325f5108dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardsql\n",
    "SELECT\n",
    "  airport,\n",
    "  arr_delay,\n",
    "  dep_delay,\n",
    "  timestamp,\n",
    "  latitude,\n",
    "  longitude,\n",
    "  num_flights\n",
    "FROM\n",
    "  flights.streaming_delays\n",
    "WHERE\n",
    "  ABS(TIMESTAMP_DIFF(timestamp,\n",
    "      (\n",
    "      SELECT\n",
    "        MAX(timestamp) latest\n",
    "      FROM\n",
    "        flights.streaming_delays ),\n",
    "      MINUTE)) < 29\n",
    "  AND num_flights > 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9585ff-b902-4271-bb4e-084d054dae2f",
   "metadata": {},
   "source": [
    "## The aggregate arrival and delay times for all airports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb0c9a-875a-4565-9f6e-3d042cd90ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardSQL\n",
    "SELECT\n",
    "  airport,\n",
    "  last[safe_OFFSET(0)].*,\n",
    "  CONCAT(CAST(last[safe_OFFSET(0)].latitude AS STRING), \",\",\n",
    "        CAST(last[safe_OFFSET(0)].longitude AS STRING)) AS location\n",
    "FROM (\n",
    "  SELECT\n",
    "    airport,\n",
    "    ARRAY_AGG(STRUCT(arr_delay,\n",
    "        dep_delay,\n",
    "        timestamp,\n",
    "        latitude,\n",
    "        longitude,\n",
    "        num_flights)\n",
    "    ORDER BY\n",
    "      timestamp DESC\n",
    "    LIMIT\n",
    "      1) last\n",
    "  FROM\n",
    "    `[PROJECT_ID].flights.streaming_delays`\n",
    "  GROUP BY\n",
    "    airport )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
