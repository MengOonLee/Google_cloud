{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0ea8a9-00d9-42a6-9e40-4e9f0d7ceee8",
   "metadata": {},
   "source": [
    "## Cloud Composer\n",
    "\n",
    "Cloud Composer is a fully managed workflow orchestration service that empowers author, schedule, and monitor DAGs tasks in the `/dags` folder.\n",
    "\n",
    "## Apache Airflow\n",
    "\n",
    "Apache Airflow is an open source tool used to programatically author, schedule, and monitor workflows. There are a few key terms as follows:\n",
    "- `DAG` (Directed Acyclic Graph), also called workflows, is a collection of organized tasks you schedule and run, organized in a way that reflects their relationships and dependencies.\n",
    "- `Operator` describes a single task in a workflow\n",
    "- `Task` is a parameterised instance of an `Operator`.\n",
    "- `Task Instance` is a specific run of a `task`; characterized as a `DAG`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02544a84-ff66-4084-bbbe-439fe2edb9a0",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82074476-ec0b-4fcc-8456-b4389e403c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile codelab.py\n",
    "\"\"\"Example Airflow DAG that checks if a local file exists, creates a Cloud Dataproc cluster, runs the Hadoop\n",
    "wordcount example, and deletes the cluster.\n",
    "This DAG relies on three Airflow variables\n",
    "https://airflow.apache.org/concepts.html#variables\n",
    "* gcp_project - Google Cloud Project to use for the Cloud Dataproc cluster.\n",
    "* gce_zone - Google Compute Engine zone where Cloud Dataproc cluster should be\n",
    "  created.\n",
    "* gcs_bucket - Google Cloud Storage bucket to use for result of Hadoop job.\n",
    "  See https://cloud.google.com/storage/docs/creating-buckets for creating a\n",
    "  bucket.\n",
    "\"\"\"\n",
    "import datetime\n",
    "import os\n",
    "from airflow import models\n",
    "from airflow.contrib.operators import dataproc_operator\n",
    "from airflow.operators import BashOperator\n",
    "from airflow.utils import trigger_rule\n",
    "\n",
    "# Output file for Cloud Dataproc job.\n",
    "output_file = os.path.join(\n",
    "    models.Variable.get('gcs_bucket'), 'wordcount',\n",
    "    datetime.datetime.now().strftime('%Y%m%d-%H%M%S')) + os.sep\n",
    "# Path to Hadoop wordcount example available on every Dataproc cluster.\n",
    "WORDCOUNT_JAR = (\n",
    "    'file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'\n",
    ")\n",
    "# Path to input file for Hadoop job.\n",
    "input_file = '/home/airflow/gcs/data/rose.txt'\n",
    "# Arguments to pass to Cloud Dataproc job.\n",
    "wordcount_args = ['wordcount', input_file, output_file]\n",
    "\n",
    "yesterday = datetime.datetime.combine(\n",
    "    datetime.datetime.today() - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    "\n",
    "default_dag_args = {\n",
    "    # Setting start date as yesterday starts the DAG immediately when it is\n",
    "    # detected in the Cloud Storage bucket.\n",
    "    'start_date': yesterday,\n",
    "    # To email on failure or retry set 'email' arg to your email and enable\n",
    "    # emailing here.\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    # If a task fails, retry it once after waiting at least 5 minutes\n",
    "    'retries': 1,\n",
    "    'retry_delay': datetime.timedelta(minutes=5),\n",
    "    'project_id': models.Variable.get('gcp_project')\n",
    "}\n",
    "\n",
    "with models.DAG(\n",
    "    'Composer_sample_quickstart',\n",
    "    # Continue to run DAG once per day\n",
    "    schedule_interval=datetime.timedelta(days=1),\n",
    "    default_args=default_dag_args\n",
    ") as dag:\n",
    "    # Check if the input file exists.\n",
    "    check_file_existence =  BashOperator(\n",
    "        task_id='check_file_existence',\n",
    "        bash_command='if [ ! -f \\\"{}\\\" ]; then exit 1; fi'.format(input_file)\n",
    "    )\n",
    "    # Create a Cloud Dataproc cluster.\n",
    "    create_dataproc_cluster = dataproc_operator.DataprocClusterCreateOperator(\n",
    "        task_id='create_dataproc_cluster',\n",
    "        # Give the cluster a unique name by appending the date scheduled.\n",
    "        # See https://airflow.apache.org/code.html#default-variables.\n",
    "        # {{ ds_nodash }} gets replaced with the execution_date of \n",
    "        # the DAG in YYYYMMDD format.\n",
    "        cluster_name='quickstart-cluster-{{ ds_nodash }}',\n",
    "        num_workers=2,\n",
    "        image_version='2.0',\n",
    "        zone=models.Variable.get('gce_zone'),\n",
    "        region='us-central1',\n",
    "        master_machine_type='n1-standard-2',\n",
    "        worker_machine_type='n1-standard-2'\n",
    "    )\n",
    "    # Run the Hadoop wordcount example installed on the Cloud Dataproc cluster\n",
    "    # master node.\n",
    "    run_dataproc_hadoop = dataproc_operator.DataProcHadoopOperator(\n",
    "        task_id='run_dataproc_hadoop',\n",
    "        region='us-central1',\n",
    "        main_jar=WORDCOUNT_JAR,\n",
    "        cluster_name='quickstart-cluster-{{ ds_nodash }}',\n",
    "        arguments=wordcount_args\n",
    "    )\n",
    "    # Delete Cloud Dataproc cluster.\n",
    "    delete_dataproc_cluster = dataproc_operator.DataprocClusterDeleteOperator(\n",
    "        task_id='delete_dataproc_cluster',\n",
    "        cluster_name='quickstart-cluster-{{ ds_nodash }}',\n",
    "        region='us-central1',\n",
    "        # Setting trigger_rule to ALL_DONE causes the cluster to be deleted\n",
    "        # even if the Dataproc job fails.\n",
    "        trigger_rule=trigger_rule.TriggerRule.ALL_DONE\n",
    "    )\n",
    "    # Define DAG dependencies.\n",
    "    check_file_existence >> create_dataproc_cluster >> run_dataproc_hadoop \\\n",
    "        >> delete_dataproc_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefc23f8-116d-466c-8400-165afa79b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create Cloud Composer environment\n",
    "gcloud composer environments create my-composer-environment \\\n",
    "--location us-central1 --zone us-central1-a\n",
    "\n",
    "# Create Cloud Storage bucket\n",
    "gsutil mb gs://<project-id>\n",
    "\n",
    "# Set a variables\n",
    "gcloud composer environments run my-composer-environment \\\n",
    "--location us-central1 variables -- \\\n",
    "--set gcp_project <your-project-id> \\\n",
    "--set gcs_bucket gs://<your-bucket-name> \\\n",
    "--set gce_zone us-central1-a \\\n",
    "--set dags_folder <your-dags-folder>\n",
    "\n",
    "# View a variable\n",
    "gcloud composer environments run my-composer-environment \\\n",
    "--location us-central1 variables -- \\\n",
    "--get gcp_project \\\n",
    "--get gcs_bucket \\\n",
    "--get gce_zone \\\n",
    "--get dags_folder\n",
    "\n",
    "# Copy DAG into /dags folder\n",
    "gsutil cp gs://cloud-training/composer-academy/codelab.py <your-dags-folder>\n",
    "\n",
    "# Upload data to Cloud Storage\n",
    "gcloud composer environments storage dags import \\\n",
    "--environment my-composer-environment \\\n",
    "--location us-central1 \\\n",
    "--source gs://pub/shakespeare/rose.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7679ccf-12c2-4c85-afcc-cef29125f1f4",
   "metadata": {},
   "source": [
    "A simple workflow that verifies the existence of a data file, creates a Cloud Dataproc cluster, runs an Apache Hadoop wordcount job on the Cloud Dataproc cluster, and deletes the Cloud Dataproc cluster afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d52f2f-75fd-4b27-bcac-1fac9ebf55fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile composer_hadoop_tutorial.py\n",
    "# [START composer_hadoop_tutorial]\n",
    "\"\"\"\n",
    "Example Airflow DAG that creates a Cloud Dataproc cluster, runs the Hadoop wordcount \n",
    "example, and deletes the cluster. This DAG relies on three Airflow variables\n",
    "https://airflow.apache.org/concepts.html#variables\n",
    "* gcp_project - Google Cloud Project to use for the Cloud Dataproc cluster.\n",
    "* gce_zone - Google Compute Engine zone where Cloud Dataproc cluster should be\n",
    "  created.\n",
    "* gcs_bucket - Google Cloud Storage bucket to used as output for the Hadoop jobs from \n",
    "    Dataproc. See https://cloud.google.com/storage/docs/creating-buckets for creating a\n",
    "    bucket.\n",
    "\"\"\"\n",
    "import datetime\n",
    "import os\n",
    "from airflow import models\n",
    "from airflow.contrib.operators import dataproc_operator\n",
    "from airflow.utils import trigger_rule\n",
    "\n",
    "# Output file for Cloud Dataproc job.\n",
    "output_file = os.path.join(\n",
    "    models.Variable.get('gcs_bucket'), 'wordcount',\n",
    "    datetime.datetime.now().strftime('%Y%m%d-%H%M%S')) + os.sep\n",
    "# Path to Hadoop wordcount example available on every Dataproc cluster.\n",
    "WORDCOUNT_JAR = (\n",
    "    'file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar'\n",
    ")\n",
    "# Arguments to pass to Cloud Dataproc job.\n",
    "wordcount_args = ['wordcount', 'gs://pub/shakespeare/rose.txt', output_file]\n",
    "\n",
    "yesterday = datetime.datetime.combine(\n",
    "    datetime.datetime.today() - datetime.timedelta(1),\n",
    "    datetime.datetime.min.time())\n",
    "\n",
    "default_dag_args = {\n",
    "    # Setting start date as yesterday starts the DAG immediately when it is\n",
    "    # detected in the Cloud Storage bucket.\n",
    "    'start_date': yesterday,\n",
    "    # To email on failure or retry set 'email' arg to your email and enable\n",
    "    # emailing here.\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    # If a task fails, retry it once after waiting at least 5 minutes\n",
    "    'retries': 1,\n",
    "    'retry_delay': datetime.timedelta(minutes=5),\n",
    "    'project_id': models.Variable.get('gcp_project')\n",
    "}\n",
    "\n",
    "# [START composer_hadoop_schedule]\n",
    "with models.DAG(\n",
    "        'composer_sample_quickstart',\n",
    "        # Continue to run DAG once per day\n",
    "        schedule_interval=datetime.timedelta(days=1),\n",
    "        default_args=default_dag_args) as dag:\n",
    "    # [END composer_hadoop_schedule]\n",
    "    \n",
    "    # Create a Cloud Dataproc cluster.\n",
    "    create_dataproc_cluster = dataproc_operator.DataprocClusterCreateOperator(\n",
    "        task_id='create_dataproc_cluster',\n",
    "        # Give the cluster a unique name by appending the date scheduled.\n",
    "        # See https://airflow.apache.org/code.html#default-variables\n",
    "        cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',\n",
    "        num_workers=2,\n",
    "        region='us-central1',\n",
    "        zone=models.Variable.get('gce_zone'),\n",
    "        image_version='2.0',\n",
    "        master_machine_type='n1-standard-2',\n",
    "        worker_machine_type='n1-standard-2')\n",
    "    \n",
    "    # Run the Hadoop wordcount example installed on the Cloud Dataproc cluster\n",
    "    # master node.\n",
    "    run_dataproc_hadoop = dataproc_operator.DataProcHadoopOperator(\n",
    "        task_id='run_dataproc_hadoop',\n",
    "        region='us-central1',\n",
    "        main_jar=WORDCOUNT_JAR,\n",
    "        cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',\n",
    "        arguments=wordcount_args)\n",
    "    \n",
    "    # Delete Cloud Dataproc cluster to avoid incurring ongoing Compute Engine charges.\n",
    "    delete_dataproc_cluster = dataproc_operator.DataprocClusterDeleteOperator(\n",
    "        task_id='delete_dataproc_cluster',\n",
    "        region='us-central1',\n",
    "        cluster_name='composer-hadoop-tutorial-cluster-{{ ds_nodash }}',\n",
    "        # Setting trigger_rule to ALL_DONE causes the cluster to be deleted\n",
    "        # even if the Dataproc job fails.\n",
    "        trigger_rule=trigger_rule.TriggerRule.ALL_DONE)\n",
    "    \n",
    "    # [START composer_hadoop_steps]\n",
    "    # Define DAG dependencies.\n",
    "    create_dataproc_cluster >> run_dataproc_hadoop >> delete_dataproc_cluster\n",
    "    # [END composer_hadoop_steps]\n",
    "    \n",
    "# [END composer_hadoop_tutorial]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
