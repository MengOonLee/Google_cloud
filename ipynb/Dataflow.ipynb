{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32ad7f9-8e83-4a99-8b1f-1e12fac4ab86",
   "metadata": {},
   "source": [
    "# Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5869c-abcd-4490-bb74-b238b9f68550",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup IAM and networking for Dataflow jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7bba0c-66e8-4dce-b4f4-e5ddf0e20847",
   "metadata": {},
   "source": [
    "### Create a Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b938b856-4151-4e68-bd14-e7c329868d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud auth list\n",
    "gcloud config list project\n",
    "\n",
    "PROJECT=`gcloud config list --format 'value(core.project)'`\n",
    "USER_EMAIL=`gcloud config list account --format 'value(core.account)'`\n",
    "REGION=us-central1\n",
    "gsutil mb -p $PROJECT -b on gs://$PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b431a2a-566c-4244-a8b9-dae6b78321ce",
   "metadata": {},
   "source": [
    "### Create a virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b02053-ac8d-4080-9237-896836e59581",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create and activate virtual environment\n",
    "sudo apt-get install -y python3-venv\n",
    "python3 -m venv df-env\n",
    "source df-env/bin/activate\n",
    "\n",
    "python3 -m pip install -q -U pip setuptools wheel\n",
    "python3 -m pip install -q -U apache-beam[gcp]\n",
    "\n",
    "# Dataflow API is enabled.\n",
    "gcloud services enable dataflow.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7925c-64e4-40ef-9465-8ce02ae08c5e",
   "metadata": {},
   "source": [
    "### Grant `Dataflow` role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7427e-7669-4509-bdfd-150a10998ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud projects get-iam-policy $PROJECT \\\n",
    "    --format='table(bindings.role)' \\\n",
    "    --flatten='bindings[].members' \\\n",
    "    --filter='bindings.members:$USER_EMAIL'\n",
    "\n",
    "gcloud projects add-iam-policy-binding $PROJECT \\\n",
    "    --member=user:$USER_EMAIL \\\n",
    "    --role=roles/dataflow.admin\n",
    "\n",
    "# Grant the dataflow.worker role to the Compute Engine service account\n",
    "PROJECT_ID=$(gcloud config get-value project)\n",
    "export PROJECT_NUMBER=$(gcloud projects list --filter=\"$PROJECT_ID\" \\\n",
    "    --format=\"value(PROJECT_NUMBER)\")\n",
    "export serviceAccount=\"\"$PROJECT_NUMBER\"-compute@developer.gserviceaccount.com\"\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=\"serviceAccount:${serviceAccount}\" \\\n",
    "    --role=\"roles/dataflow.worker\"\n",
    "\n",
    "python3 -m apache_beam.examples.wordcount \\\n",
    "    --input=gs://dataflow-samples/shakespeare/kinglear.txt \\\n",
    "    --output=gs://$PROJECT/results/outputs \\\n",
    "    --runner=DataflowRunner \\\n",
    "    --project=$PROJECT \\\n",
    "    --temp_location=gs://$PROJECT/tmp/ \\\n",
    "    --region=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c292c5-86c8-44a8-974c-001a3bd322a3",
   "metadata": {},
   "source": [
    "### Launch in private IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56927f-b2a8-48df-9d0d-348c98082b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud projects add-iam-policy-binding $PROJECT \\\n",
    "--member=user:$USER_EMAIL \\\n",
    "--role=roles/compute.networkAdmin\n",
    "\n",
    "\n",
    "gcloud compute networks subnets update default \\\n",
    "--region=$REGION \\\n",
    "--enable-private-ip-google-access\n",
    "\n",
    "\n",
    "python3 -m apache_beam.examples.wordcount \\\n",
    "--input=gs://dataflow-samples/shakespeare/kinglear.txt \\\n",
    "--output=gs://$PROJECT/results/outputs \\\n",
    "--runner=DataflowRunner \\\n",
    "--project=$PROJECT \\\n",
    "--temp_location=gs://$PROJECT/tmp/ \\\n",
    "--region=$REGION \\\n",
    "--no_use_public_ips \\\n",
    "--network default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a67ee61-76c8-433f-a3d0-1ea4d3bce3da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Extract-Transform-Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b8274-551e-4513-bfcf-28bee264b768",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8e595-8190-4d61-84ab-29a4884c3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--stagingLocation', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--tempLocation', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions()\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).staging_location = opts.stagingLocation\n",
    "    options.view_as(GoogleCloudOptions).temp_location = opts.tempLocation\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('my-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    # Static input and output\n",
    "    input = 'gs://{0}/events.json'.format(opts.project)\n",
    "    output = '{0}:logs.logs'.format(opts.project)\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"ip\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"user_id\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lat\",\n",
    "                \"type\": \"FLOAT\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lng\",\n",
    "                \"type\": \"FLOAT\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_request\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_response\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"num_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"user_agent\",\n",
    "                \"type\": \"STRING\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    '''\n",
    "    Steps:\n",
    "    1) Read something\n",
    "    2) Transform something\n",
    "    3) Write something\n",
    "    '''\n",
    "\n",
    "    (p\n",
    "        | 'ReadFromGCS' >> beam.io.ReadFromText(input)\n",
    "        | 'ParseJson' >> beam.Map(lambda line: json.loads(line))\n",
    "        | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n",
    "            output,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b4cde-39fe-40cc-b23c-8a1b4c31e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment variables\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "# Run the pipeline\n",
    "python3 my_pipeline.py \\\n",
    "--project=${PROJECT_ID} \\\n",
    "--region=us-central1 \\\n",
    "--stagingLocation=gs://$PROJECT_ID/staging/ \\\n",
    "--tempLocation=gs://$PROJECT_ID/temp/ \\\n",
    "--runner=DataflowRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a6227-734f-47a0-8700-ce22a628d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a full JSON object\n",
    "bq show --schema --format=prettyjson logs.logs | sed '1s/^/{\"BigQuery Schema\":/' | sed '$s/$/}/' > schema.json\n",
    "cat schema.json\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "gsutil cp schema.json gs://${PROJECT_ID}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8445147b-79c4-4a9b-83cb-f61f8e7d2656",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Windows, Watermarks, Triggers\n",
    "\n",
    "Three main concepts:\n",
    "1. Group data in windows.\n",
    "2. Watermark when the window is ready to produce results.\n",
    "3. Control when and number of times the window will emit ouptput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c310fdcf-f095-48f6-a940-dfb28cdeb977",
   "metadata": {},
   "source": [
    "### Windows\n",
    "\n",
    "- Windowing divides data into time-based, finite chunks.\n",
    "- Required when doing aggregations over unbounded data using Beam primitives (GroupByKey, Combiners).\n",
    "\n",
    "Three types of windows:\n",
    "1. Fixed\n",
    "2. Sliding\n",
    "3. Session\n",
    "4. Single global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ddb85-1707-46ca-9f2d-e21bff23109d",
   "metadata": {},
   "source": [
    "### Watermarks\n",
    "\n",
    "A watermark is the systemâ€™s heuristic-based notion of when all data up to a certain point in event time can be expected to have arrived in the pipeline. Once the watermark progresses past the end of a window, any further element that arrives with a timestamp in that window is considered late data and is simply dropped.\n",
    "\n",
    "**Lag time** = the difference in time from when data was expected to arrive and when it actually arrived\n",
    "\n",
    "Lag is problematic when windowing using event time (as opposed to processing time) because it introduces uncertainty.\n",
    "\n",
    "**Data freshness** = the amount of time between real time and the output watermark.\n",
    "\n",
    "**System latency** = the current maximum duration that an item of data has been processing or awaiting processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabdadb-2458-4a00-a3b0-8461e98b9fe9",
   "metadata": {},
   "source": [
    "### Triggers\n",
    "\n",
    "1. Event time: AfterWatermark\n",
    "2. Processing time: AfterProcessingTime\n",
    "3. Composite\n",
    "4. Data-driven: AfterCount\n",
    "\n",
    "Window accumulation modes:\n",
    "1. Accumulate\n",
    "2. Discard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a22dc-7272-492e-9891-b5b08d9b8552",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcoll | WindowInto(\n",
    "    # Sliding window of 60 seconds, every 5 seconds\n",
    "    SlidingWindows(60, 5),\n",
    "    # Relative to the watermark, trigger:\n",
    "    trigger=AfterWatermark(\n",
    "        # fires 30 seconds after pipeline commences\n",
    "        early=AfterProcessingTime(delay=30),\n",
    "        # and for every late record (< allowedLateness)\n",
    "        late=AfterCount(1)\n",
    "    )\n",
    "    # the pane should have all the records\n",
    "    accumulation_mode=AccumulationMode.ACCUMULATING\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7259652-a745-4b22-b428-bbb534d82895",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcoll | WindowInto(\n",
    "    # Fixed window of 60 seconds\n",
    "    FixedWindows(60),\n",
    "    # Set up a composite trigger that triggers\n",
    "    trigger=Repeatedly(\n",
    "        # whenever either of these happens:\n",
    "        AfterAny(\n",
    "            # 100 elements accumulate\n",
    "            AfterCount(100),\n",
    "            # every 60 seconds (ignore watermark)\n",
    "            AfterProcessingTime(1*60)\n",
    "        )\n",
    "    ),\n",
    "    # the trigger should be with only new records 2 days\n",
    "    accumulation_mode=AccumulationMode.DISCARDING,\n",
    "    allowed_lateness=Duration(seconds=2*24*60*60)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb3bb98-e081-4c9d-b109-148e8b0fdb3a",
   "metadata": {},
   "source": [
    "### Aggregates site traffic by user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29313ca7-66e6-46eb-9b76-df29da094c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_user_traffic_pipeline.py\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import typing\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.transforms.combiners import CountCombineFn\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### functions and classes\n",
    "\n",
    "class CommonLog (typing.NamedTuple):\n",
    "    ip: str\n",
    "    user_id: str\n",
    "    lat: float\n",
    "    lng: float\n",
    "    timestamp: str\n",
    "    http_request: str\n",
    "    http_response: int\n",
    "    num_bytes: int\n",
    "    user_agent: str\n",
    "\n",
    "class PerUserAggregation(typing.NamedTuple):\n",
    "    user_id: str\n",
    "    page_views: int\n",
    "    total_bytes: int\n",
    "    max_bytes: int\n",
    "    min_bytes: int\n",
    "\n",
    "beam.coders.registry.register_coder(CommonLog, beam.coders.RowCoder)\n",
    "beam.coders.registry.register_coder(PerUserAggregation, beam.coders.RowCoder)\n",
    "\n",
    "def parse_json(element):\n",
    "    row = json.loads(element)\n",
    "    return CommonLog(**row)\n",
    "\n",
    "def to_dict(element):\n",
    "    return element._asdict()\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--staging_location', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--temp_location', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "    parser.add_argument('--input_path', required=True, help='Path to events.json')\n",
    "    parser.add_argument('--table_name', required=True, help='BigQuery table name')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(save_main_session=True)\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).staging_location = opts.staging_location\n",
    "    options.view_as(GoogleCloudOptions).temp_location = opts.temp_location\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('batch-user-traffic-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    input_path = opts.input_path\n",
    "    table_name = opts.table_name\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "\n",
    "            {\n",
    "                \"name\": \"user_id\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page_views\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"total_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"max_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"min_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    (p | 'ReadFromGCS' >> beam.io.ReadFromText(input_path)\n",
    "       | 'ParseJson' >> beam.Map(parse_json).with_output_types(CommonLog)\n",
    "       | 'PerUserAggregations' >> beam.GroupBy('user_id')\n",
    "            .aggregate_field('user_id', CountCombineFn(), 'page_views')\n",
    "            .aggregate_field('num_bytes', sum, 'total_bytes')\n",
    "            .aggregate_field('num_bytes', max, 'max_bytes')\n",
    "            .aggregate_field('num_bytes', min, 'min_bytes')\n",
    "            .with_output_types(PerUserAggregation)\n",
    "       | 'ToDict' >> beam.Map(to_dict)\n",
    "       | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n",
    "            table_name,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36ec8c-1e69-4b50-8651-e8541fb6b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "export REGION='us-central1'\n",
    "export BUCKET=gs://${PROJECT_ID}\n",
    "export PIPELINE_FOLDER=${BUCKET}\n",
    "export RUNNER=DataflowRunner\n",
    "export INPUT_PATH=${PIPELINE_FOLDER}/events.json\n",
    "export TABLE_NAME=${PROJECT_ID}:logs.user_traffic\n",
    "cd $BASE_DIR\n",
    "python3 batch_user_traffic_pipeline.py \\\n",
    "--project=${PROJECT_ID} \\\n",
    "--region=${REGION} \\\n",
    "--staging_location=${PIPELINE_FOLDER}/staging \\\n",
    "--temp_location=${PIPELINE_FOLDER}/temp \\\n",
    "--runner=${RUNNER} \\\n",
    "--input_path=${INPUT_PATH} \\\n",
    "--table_name=${TABLE_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbff649-40ee-42e0-b59f-6e76bbf97203",
   "metadata": {},
   "source": [
    "### Aggregates site traffic by minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b3878-c95e-486b-96e2-2079ffea57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_minute_traffic_pipeline.py\n",
    "mport argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import typing\n",
    "from datetime import datetime\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.transforms.combiners import CountCombineFn\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### functions and classes\n",
    "\n",
    "class CommonLog(typing.NamedTuple):\n",
    "    ip: str\n",
    "    user_id: str\n",
    "    lat: float\n",
    "    lng: float\n",
    "    timestamp: str\n",
    "    http_request: str\n",
    "    http_response: int\n",
    "    num_bytes: int\n",
    "    user_agent: str\n",
    "\n",
    "beam.coders.registry.register_coder(CommonLog, beam.coders.RowCoder)\n",
    "\n",
    "def parse_json(element):\n",
    "    row = json.loads(element)\n",
    "    return CommonLog(**row)\n",
    "\n",
    "def add_timestamp(element):\n",
    "    ts = datetime.strptime(element.timestamp[:-8], \"%Y-%m-%dT%H:%M:%S\").timestamp()\n",
    "    return beam.window.TimestampedValue(element, ts)\n",
    "\n",
    "class GetTimestampFn(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        window_start = window.start.to_utc_datetime().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        output = {'page_views': element, 'timestamp': window_start}\n",
    "        yield output\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--staging_location', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--temp_location', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "    parser.add_argument('--input_path', required=True, help='Path to events.json')\n",
    "    parser.add_argument('--table_name', required=True, help='BigQuery table name')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(save_main_session=True)\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).staging_location = opts.staging_location\n",
    "    options.view_as(GoogleCloudOptions).temp_location = opts.temp_location\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('batch-minute-traffic-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    input_path = opts.input_path\n",
    "    table_name = opts.table_name\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"page_views\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    (p | 'ReadFromGCS' >> beam.io.ReadFromText(input_path)\n",
    "       | 'ParseJson' >> beam.Map(parse_json).with_output_types(CommonLog)\n",
    "       | 'AddEventTimestamp' >> beam.Map(add_timestamp)\n",
    "       | \"WindowByMinute\" >> beam.WindowInto(beam.window.FixedWindows(60))\n",
    "       | \"CountPerMinute\" >> beam.CombineGlobally(CountCombineFn()).without_defaults()\n",
    "       | \"AddWindowTimestamp\" >> beam.ParDo(GetTimestampFn())\n",
    "       | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n",
    "            table_name,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc41cf-2f26-4d5c-bc60-282ba0d18cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "export REGION='us-central1'\n",
    "export BUCKET=gs://${PROJECT_ID}\n",
    "export PIPELINE_FOLDER=${BUCKET}\n",
    "export RUNNER=DataflowRunner\n",
    "export INPUT_PATH=${PIPELINE_FOLDER}/events.json\n",
    "export TABLE_NAME=${PROJECT_ID}:logs.minute_traffic\n",
    "cd $BASE_DIR\n",
    "python3 batch_minute_traffic_pipeline.py \\\n",
    "--project=${PROJECT_ID} \\\n",
    "--region=${REGION} \\\n",
    "--staging_location=${PIPELINE_FOLDER}/staging \\\n",
    "--temp_location=${PIPELINE_FOLDER}/temp \\\n",
    "--runner=${RUNNER} \\\n",
    "--input_path=${INPUT_PATH} \\\n",
    "--table_name=${TABLE_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd3babb-b7e5-4070-9886-95e1dfd4d775",
   "metadata": {},
   "source": [
    "### Streaming analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea33aef-24b7-4665-ad09-8d5fe650ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming_event_generator.py\n",
    "# This program reads a file representing web server logs in common log format and streams them into a PubSub topic\n",
    "# with lag characteristics as determined by command-line arguments\n",
    "\n",
    "import argparse\n",
    "from google.cloud import pubsub_v1\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import random\n",
    "from anytree.importer import DictImporter\n",
    "import json\n",
    "from multiprocessing import Process\n",
    "\n",
    "parser = argparse.ArgumentParser(__file__, description=\"event_generator\")\n",
    "parser.add_argument(\"--taxonomy\", \"-x\", dest=\"taxonomy_fp\",\n",
    "    help=\"A .json file representing a taxonomy of web resources\",\n",
    "    default=\"taxonomy.json\")\n",
    "parser.add_argument(\"--users_fp\", \"-u\", dest=\"users_fp\",\n",
    "    help=\"A .csv file of users\",\n",
    "    default=\"users.csv\")\n",
    "parser.add_argument(\"--off_to_on\", \"-off\", dest=\"off_to_on_prob\", type=float,\n",
    "    help=\"A float representing the probability that a user who is offline will come online\",\n",
    "    default=.25)\n",
    "parser.add_argument(\"--on_to_off\", \"-on\", dest=\"on_to_off_prob\", type=float,\n",
    "    help=\"A float representing the probability that a user who is online will go offline\",\n",
    "    default=.1)\n",
    "parser.add_argument(\"--max_lag_millis\", '-l', dest=\"max_lag_millis\", type=int,\n",
    "    help=\"An integer representing the maximum amount of lag in millisecond\", default=250)\n",
    "parser.add_argument(\"--project_id\", \"-p\", type=str, dest=\"project_id\", \n",
    "    help=\"A GCP Project ID\", required=True)\n",
    "parser.add_argument(\"--topic_name\", \"-t\", dest=\"topic_name\", type=str,\n",
    "    help=\"The name of the topic where the messages to be published\", required=True)\n",
    "\n",
    "\n",
    "avg_secs_between_events = 5\n",
    "args = parser.parse_args()\n",
    "taxonomy_fp = args.taxonomy_fp\n",
    "users_fp = args.users_fp\n",
    "online_to_offline_probability = args.on_to_off_prob\n",
    "offline_to_online_probability = args.off_to_on_prob\n",
    "max_lag_millis = args.max_lag_millis\n",
    "project_id = args.project_id\n",
    "topic_name = args.topic_name\n",
    "min_file_size_bytes = 100\n",
    "max_file_size_bytes = 500\n",
    "verbs = [\"GET\"]\n",
    "responses = [200]\n",
    "\n",
    "\n",
    "log_fields = [\"ip\", \"user_id\", \"lat\", \"lng\", \"timestamp\", \"http_request\",\n",
    "              \"http_response\", \"num_bytes\", \"user_agent\"]\n",
    "\n",
    "def extract_resources(taxonomy_filepath):\n",
    "    \"\"\"\n",
    "    Reads a .json representing a taxonomy and returns\n",
    "    a data structure representing their hierarchical relationship\n",
    "    :param taxonomy_file: a string representing a path to a .json file\n",
    "    :return: Node representing root of taxonomic tree\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(taxonomy_filepath, 'r') as fp:\n",
    "            json_str = fp.read()\n",
    "            json_data = json.loads(json_str)\n",
    "            root = DictImporter().import_(json_data)\n",
    "    finally:\n",
    "        fp.close()\n",
    "\n",
    "    return root\n",
    "\n",
    "\n",
    "def read_users(users_fp):\n",
    "    \"\"\"\n",
    "    Reads a .csv from @user_fp representing users into a list of dictionaries,\n",
    "    each elt of which represents a user\n",
    "    :param user_fp: a .csv file where each line represents a user\n",
    "    :return: a list of dictionaries\n",
    "    \"\"\"\n",
    "    users = []\n",
    "    with open(users_fp, 'r') as fp:\n",
    "        fields = fp.readline().rstrip().split(\",\")\n",
    "        for line in fp:\n",
    "            user = dict(zip(fields, line.rstrip().split(\",\")))\n",
    "            users.append(user)\n",
    "    return users\n",
    "\n",
    "def sleep_then_publish_burst(burst, publisher, topic_path):\n",
    "    \"\"\"\n",
    "    :param burst: a list of dictionaries, each representing an event\n",
    "    :param num_events_counter: an instance of Value shared by all processes\n",
    "    to track the number of published events\n",
    "    :param publisher: a PubSub publisher\n",
    "    :param topic_path: a topic path for PubSub\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    sleep_secs = random.uniform(0, max_lag_millis/1000)\n",
    "    time.sleep(sleep_secs)\n",
    "    publish_burst(burst, publisher, topic_path)\n",
    "\n",
    "def publish_burst(burst, publisher, topic_path):\n",
    "    \"\"\"\n",
    "    Publishes and prints each event\n",
    "    :param burst: a list of dictionaries, each representing an event\n",
    "    :param num_events_counter: an instance of Value shared by all processes to\n",
    "    track the number of published events\n",
    "    :param publisher: a PubSub publisher\n",
    "    :param topic_path: a topic path for PubSub\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for event_dict in burst:\n",
    "        json_str = json.dumps(event_dict)\n",
    "        data = json_str.encode('utf-8')\n",
    "        publisher.publish(topic_path, data=data, timestamp=event_dict['timestamp'])\n",
    "\n",
    "def create_user_process(user, root):\n",
    "    \"\"\"\n",
    "    Code for continuously-running process representing a user publishing\n",
    "    events to pubsub\n",
    "    :param user: a dictionary representing characteristics of the user\n",
    "    :param root: an instance of AnyNode representing the home page of a website\n",
    "    :param num_events_counter: a variable shared among all processes used to track the number of events published\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    publisher = pubsub_v1.PublisherClient()\n",
    "    topic_path = publisher.topic_path(project_id, topic_name)\n",
    "\n",
    "    user['page'] = root\n",
    "    user['is_online'] = True\n",
    "    user['offline_events'] = []\n",
    "\n",
    "    while True:\n",
    "        time_between_events = random.uniform(0, avg_secs_between_events * 2)\n",
    "        time.sleep(time_between_events)\n",
    "        prob = random.random()\n",
    "        event = generate_event(user)\n",
    "        if user['is_online']:\n",
    "            if prob < online_to_offline_probability:\n",
    "                user['is_online'] = False\n",
    "                user['offline_events'] = [event]\n",
    "            else:\n",
    "                sleep_then_publish_burst([event], publisher, topic_path)\n",
    "        else:\n",
    "            user['offline_events'].append(event)\n",
    "            if prob < offline_to_online_probability:\n",
    "                user['is_online'] = True\n",
    "                sleep_then_publish_burst(user['offline_events'], publisher, topic_path)\n",
    "                user['offline_events'] = []\n",
    "\n",
    "def generate_event(user):\n",
    "    \"\"\"\n",
    "    Returns a dictionary representing an event\n",
    "    :param user:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    user['page'] = get_next_page(user)\n",
    "    uri = str(user['page'].name)\n",
    "    event_time = datetime.now(tz=timezone.utc)\n",
    "    current_time_str = event_time.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    file_size_bytes = random.choice(range(min_file_size_bytes, max_file_size_bytes))\n",
    "    http_request = \"\\\"{} {} HTTP/1.0\\\"\".format(random.choice(verbs), uri)\n",
    "    http_response = random.choice(responses)\n",
    "    event_values = [user['ip'], user['id'], float(user['lat']), float(user['lng']), current_time_str, http_request,\n",
    "                    http_response, file_size_bytes, user['user_agent']]\n",
    "\n",
    "    return dict(zip(log_fields, event_values))\n",
    "\n",
    "def get_next_page(user):\n",
    "    \"\"\"\n",
    "    Consults the user's representation of the web site taxonomy to determine the next page that they visit\n",
    "    :param user:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    possible_next_pages = [user['page']]\n",
    "    if not user['page'].is_leaf:\n",
    "        possible_next_pages += list(user['page'].children)\n",
    "    if (user['page'].parent != None):\n",
    "        possible_next_pages += [user['page'].parent]\n",
    "    next_page = random.choice(possible_next_pages)\n",
    "    return next_page\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    users = read_users(users_fp)\n",
    "    root = extract_resources(taxonomy_fp)\n",
    "    processes = [Process(target=create_user_process, args=(user, root))\n",
    "                 for user in users]\n",
    "    [process.start() for process in processes]\n",
    "    while True:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c76fb8-ad6b-48eb-a68b-f502adcc5089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_streaming_events.sh\n",
    "#!/bin/#!/usr/bin/env bash\n",
    "echo \"Installing packages\"\n",
    "# Install modules\n",
    "sh ./install_packages.sh\n",
    "\n",
    "echo \"Generating synthetic users\"\n",
    "# Generate 10 fake web site users\n",
    "python3 user_generator.py --n=10\n",
    "\n",
    "echo \"Generating synthetic events\"\n",
    "use_lag=$1\n",
    "\n",
    "if [ \"$use_lag\" = true ] ; then\n",
    "    echo \"Using lag\"\n",
    "    python3 streaming_event_generator.py --project_id=$(gcloud config get-value project) -t=my_topic\n",
    "else\n",
    "    echo \"Not using lag\"\n",
    "    python3 streaming_event_generator.py --project_id=$(gcloud config get-value project) -t=my_topic -off=1. -on=0. -l=0\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc743608-ce2f-4112-8ba9-d342d4b1198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming_minute_traffic_pipeline.py\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import typing\n",
    "from datetime import datetime\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.transforms.combiners import CountCombineFn\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### functions and classes\n",
    "\n",
    "class CommonLog(typing.NamedTuple):\n",
    "    ip: str\n",
    "    user_id: str\n",
    "    lat: float\n",
    "    lng: float\n",
    "    timestamp: str\n",
    "    http_request: str\n",
    "    http_response: int\n",
    "    num_bytes: int\n",
    "    user_agent: str\n",
    "\n",
    "beam.coders.registry.register_coder(CommonLog, beam.coders.RowCoder)\n",
    "\n",
    "def parse_json(element):\n",
    "    row = json.loads(element.decode('utf-8'))\n",
    "    return CommonLog(**row)\n",
    "\n",
    "def add_processing_timestamp(element):\n",
    "    row = element._asdict()\n",
    "    row['event_timestamp'] = row.pop('timestamp')\n",
    "    row['processing_timestamp'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    return row\n",
    "\n",
    "class GetTimestampFn(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        window_start = window.start.to_utc_datetime().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        output = {'page_views': element, 'timestamp': window_start}\n",
    "        yield output\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json from Pub/Sub into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--staging_location', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--temp_location', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "    parser.add_argument('--input_topic', required=True, help='Input Pub/Sub Topic')\n",
    "    parser.add_argument('--agg_table_name', required=True, help='BigQuery table name for aggregate results')\n",
    "    parser.add_argument('--raw_table_name', required=True, help='BigQuery table name for raw inputs')\n",
    "    parser.add_argument('--window_duration', required=True, help='Window duration')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(save_main_session=True, streaming=True)\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).staging_location = opts.staging_location\n",
    "    options.view_as(GoogleCloudOptions).temp_location = opts.temp_location\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('streaming-minute-traffic-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    input_topic = opts.input_topic\n",
    "    raw_table_name = opts.raw_table_name\n",
    "    agg_table_name = opts.agg_table_name\n",
    "    window_duration = opts.window_duration\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    agg_table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"page_views\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    raw_table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"ip\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"user_id\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"user_agent\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lat\",\n",
    "                \"type\": \"FLOAT\",\n",
    "                \"mode\": \"NULLABLE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lng\",\n",
    "                \"type\": \"FLOAT\",\n",
    "                \"mode\": \"NULLABLE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"event_timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"processing_timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_request\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_response\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"num_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    parsed_msgs = (p | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(input_topic)\n",
    "                     | 'ParseJson' >> beam.Map(parse_json).with_output_types(CommonLog))\n",
    "\n",
    "    (parsed_msgs\n",
    "        | \"AddProcessingTimestamp\" >> beam.Map(add_processing_timestamp)\n",
    "        | 'WriteRawToBQ' >> beam.io.WriteToBigQuery(\n",
    "            raw_table_name,\n",
    "            schema=raw_table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            )\n",
    "        )\n",
    "\n",
    "    (parsed_msgs\n",
    "        | \"WindowByMinute\" >> beam.WindowInto(beam.window.FixedWindows(60))\n",
    "        | \"CountPerMinute\" >> beam.CombineGlobally(CountCombineFn()).without_defaults()\n",
    "        | \"AddWindowTimestamp\" >> beam.ParDo(GetTimestampFn())\n",
    "        | 'WriteAggToBQ' >> beam.io.WriteToBigQuery(\n",
    "            agg_table_name,\n",
    "            schema=agg_table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run().wait_until_finish()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2a888-68e8-4bd3-b397-e4740574e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "export REGION='us-central1'\n",
    "export BUCKET=gs://${PROJECT_ID}\n",
    "export PIPELINE_FOLDER=${BUCKET}\n",
    "export RUNNER=DataflowRunner\n",
    "export PUBSUB_TOPIC=projects/${PROJECT_ID}/topics/my_topic\n",
    "export WINDOW_DURATION=60\n",
    "export AGGREGATE_TABLE_NAME=${PROJECT_ID}:logs.windowed_traffic\n",
    "export RAW_TABLE_NAME=${PROJECT_ID}:logs.raw\n",
    "python3 streaming_minute_traffic_pipeline.py \\\n",
    "--project=${PROJECT_ID} \\\n",
    "--region=${REGION} \\\n",
    "--staging_location=${PIPELINE_FOLDER}/staging \\\n",
    "--temp_location=${PIPELINE_FOLDER}/temp \\\n",
    "--runner=${RUNNER} \\\n",
    "--input_topic=${PUBSUB_TOPIC} \\\n",
    "--window_duration=${WINDOW_DURATION} \\\n",
    "--agg_table_name=${AGGREGATE_TABLE_NAME} \\\n",
    "--raw_table_name=${RAW_TABLE_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4cfeb-a5c0-45ce-99af-8be4e9ea8f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gap between event time and processing time\n",
    "SELECT\n",
    "  UNIX_MILLIS(TIMESTAMP(event_timestamp)) - min_millis.min_event_millis AS event_millis,\n",
    "  UNIX_MILLIS(TIMESTAMP(processing_timestamp)) - min_millis.min_event_millis AS processing_millis,\n",
    "  user_id,\n",
    "  -- added as unique label so we see all the points\n",
    "  CAST(UNIX_MILLIS(TIMESTAMP(event_timestamp)) - min_millis.min_event_millis AS STRING) AS label\n",
    "FROM\n",
    "  `logs.raw`\n",
    "CROSS JOIN (\n",
    "  SELECT\n",
    "    MIN(UNIX_MILLIS(TIMESTAMP(event_timestamp))) AS min_event_millis\n",
    "  FROM\n",
    "    `logs.raw`) min_millis\n",
    "WHERE\n",
    "  event_timestamp IS NOT NULL\n",
    "ORDER BY\n",
    "  event_millis ASC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98a31e-b3e6-4191-9df8-be3b3c167577",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Sources and Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98757a76-5936-4e48-9796-bfc0d20adf17",
   "metadata": {},
   "source": [
    "### Text IO & File IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b98297f-5f44-465d-99f3-ac3b4d901c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text IO reading\n",
    "pcoll = (pipeline\n",
    "    | 'Create' >> Create([file_name])\n",
    "    | 'ReadAll' >> ReadAllFromText()\n",
    ")\n",
    "\n",
    "pcoll = pipeline | 'Read' >> ReadFromText(file_name)\n",
    "\n",
    "# File IO reading with filenames\n",
    "with beam.Pipeline() as p:\n",
    "    readable_files = (p\n",
    "        | fileio.MatchFiles('hdfs://path/to/*.txt') # Match file patter\n",
    "        | fileio.ReadMatches()\n",
    "        | beam.Reshuffle()\n",
    "    )\n",
    "    file_and_contents = (readable_files\n",
    "        | beam.Map(lambda x: (x.metadata.path, x.read_utf8())) # Access file metadata\n",
    "    )\n",
    "    \n",
    "# File IO processing files as they arrive\n",
    "with beam.Pipeline() as p:\n",
    "    readable_files = (p\n",
    "        | beam.io.ReadFromPubSub(...) # Parse PubSub message and yield filename\n",
    "    )\n",
    "    files_and_contents = (readable_files\n",
    "        | ReadAllFromText() # Used parsed filename to read \n",
    "    )\n",
    "    \n",
    "# Text IO writing\n",
    "transformed_data | \"write\" >> WriteToText(know_args.output, coder=JsonCoder())\n",
    "\n",
    "# Text IO writing with dynamic destinations\n",
    "pcoll | beam.io.fileio.WriteToFiles(\n",
    "    path='/path',\n",
    "    destination=lambda record: \n",
    "        'avro' if record['type']=='A' else 'csv', # Dynamic destination\n",
    "    sink=lambda dest: \n",
    "        AvroSink() if dest=='avro' else CsvSink(), # Write dynamic sink\n",
    "        file_naming=beam.io.fileio.destination_prefix_naming()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f865f-1901-4d58-9a07-7a690202aa1a",
   "metadata": {},
   "source": [
    "### BigQuery IO with BigQuery Storage API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6b44b-1969-4ee7-bba1-2fed464ea172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery IO reading with query\n",
    "pcoll = (p\n",
    "    | 'QueryTableStdSQL' >> beam.io.ReadFromBigQuery(\n",
    "        query='SELECT max_temperature '\\\n",
    "            'FROM `project.dataset.table`',\n",
    "        use_standard_sql=True\n",
    "    ) # Map results\n",
    "    | beam.Map(lambda elem: elem['max_temperature']) # Source using query\n",
    ")\n",
    "\n",
    "# BigQuery IO writing with dynamic destinations\n",
    "def table_fn(element, fictional_characters):\n",
    "    if element in fictional_characters:\n",
    "        return 'dataset.fictional_quotes'\n",
    "    else:\n",
    "        return 'dataset.real_quotes'\n",
    "    \n",
    "quotes | \"WriteWithDynamicDestination\" >> beam.io.WriteToBigQuery(\n",
    "    table_fn,\n",
    "    schema=table_schema, # Schema destination\n",
    "    table_side_inputs=(fictional_characters_view)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17db1b-0b17-4b3b-9626-aa092abc4206",
   "metadata": {},
   "source": [
    "### PubSub IO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97d5f6-b195-4fd5-8133-8053a1486e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PubSub IO reading\n",
    "class GroupWindowsIntoBatches(beam.PTransform):\n",
    "    return (pcoll\n",
    "        | beam.WindowInto(window.FixedWindows(self.window_size))\n",
    "    )\n",
    "\n",
    "pipeline \n",
    "    | \"Read PubSub Message\" >> beam.io.ReadFromPubSub(topic=input_topic)\n",
    "    | \"Window into\" >> GroupWindowIntoBatches(window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ee87e-d6a2-4d43-9a71-2d8d197c3429",
   "metadata": {},
   "source": [
    "### Kafka IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65a49e-d48c-4147-ab50-ddbb6dca60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka IO reading\n",
    "pipeline\n",
    "    | ReadFromKafka(\n",
    "        consumer_config={'bootstrap.servers': bootstrap_servers},\n",
    "        topic=[topic]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c3543-0b59-4368-8990-8a22b9b4009b",
   "metadata": {},
   "source": [
    "### Avro IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5038bb-e8de-4c2e-880c-66dea0c5a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avro IO reading multiple files\n",
    "with beam.Pipeline() as p:\n",
    "    records = p | \"Read\" >> beam.io.ReadFromAvro('/avrofiles*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a9678-bb48-42e2-8bea-0317655076c4",
   "metadata": {},
   "source": [
    "### Splittable DoFn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68030d1-0e66-4f35-ab14-3abf49090174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splittable DoFn custome source\n",
    "class FileToWordsRestrictionProvider(beam.io.RestrictionProvider):\n",
    "    def initial_restriction(self, file_name): # Initial restriction\n",
    "        return OffsetRange(0, os.stat(file_name).st_size)\n",
    "    \n",
    "    # Tracking subset of restriction completed\n",
    "    def create_tracker(self, restriction):\n",
    "        return beam.io.restriction_trackers.OffsetRestrictionTracker()\n",
    "    \n",
    "class FileToWordsFn(beam.DoFn):\n",
    "    def process(self, ...=FileToWordsRestrictionProvider()):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb7d42-9b86-4808-8dd0-c8a6c8b9d82e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Schemas\n",
    "\n",
    "Express structured data in Beam pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d622a0d0-79da-49ef-9e2c-5404b6c3f744",
   "metadata": {},
   "source": [
    "### Branching pipeline\n",
    "\n",
    "![Branch_pipeline](./img/branch_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc9c6f-769d-46b5-9b69-2c159c5ba8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_pipeline.py\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### functions and classes\n",
    "\n",
    "def parse_json(element):\n",
    "    return json.loads(element)\n",
    "\n",
    "def drop_fields(element):\n",
    "    element.pop('user_agent')\n",
    "    return element\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "    parser.add_argument('--inputPath', required=True, help='Path to events.json')\n",
    "    parser.add_argument('--outputPath', required=True, help='Path to coldline storage bucket')\n",
    "    parser.add_argument('--tableName', required=True, help='BigQuery table name')\n",
    "\n",
    "    opts, pipeline_opts = parser.parse_known_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(pipeline_opts)\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('my-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    input_path = opts.inputPath\n",
    "    output_path = opts.outputPath\n",
    "    table_name = opts.tableName\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"ip\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"user_id\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lat\",\n",
    "                \"type\": \"FLOAT\",\n",
    "                \"mode\": \"NULLABLE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lng\",\n",
    "                \"type\": \"FLOAT\",\n",
    "                \"mode\": \"NULLABLE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_request\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_response\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"num_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    '''\n",
    "    Steps:\n",
    "    1) Read something\n",
    "    2) Transform something\n",
    "    3) Write something\n",
    "    '''\n",
    "\n",
    "    # Read in lines to an initial PCollection that can then be branched off of\n",
    "    lines = p | 'ReadFromGCS' >> beam.io.ReadFromText(input_path)\n",
    "\n",
    "    # Write to Google Cloud Storage\n",
    "    lines | 'WriteRawToGCS' >> beam.io.WriteToText(output_path)\n",
    "\n",
    "    # Read elements from Json, filter out individual elements, and write to BigQuery\n",
    "    (lines\n",
    "        | 'ParseJson' >> beam.Map(parse_json)\n",
    "        | 'DropFields' >> beam.Map(drop_fields)\n",
    "        | 'FilterFn' >> beam.Filter(lambda row: row['num_bytes'] < 120)\n",
    "        | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n",
    "            table_name,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87477eff-e36f-4ed5-90d8-af45c12d7f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment variables\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "export REGION='us-central1'\n",
    "export BUCKET=gs://${PROJECT_ID}\n",
    "export COLDLINE_BUCKET=${BUCKET}-coldline\n",
    "export PIPELINE_FOLDER=${BUCKET}\n",
    "export RUNNER=DataflowRunner\n",
    "export INPUT_PATH=${PIPELINE_FOLDER}/events.json\n",
    "export OUTPUT_PATH=${PIPELINE_FOLDER}-coldline/pipeline_output\n",
    "export TABLE_NAME=${PROJECT_ID}:logs.logs_filtered\n",
    "cd $BASE_DIR\n",
    "python3 my_pipeline.py \\\n",
    "--project=${PROJECT_ID} \\\n",
    "--region=${REGION} \\\n",
    "--stagingLocation=${PIPELINE_FOLDER}/staging \\\n",
    "--tempLocation=${PIPELINE_FOLDER}/temp \\\n",
    "--runner=${RUNNER} \\\n",
    "--inputPath=${INPUT_PATH} \\\n",
    "--outputPath=${OUTPUT_PATH} \\\n",
    "--tableName=${TABLE_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320168d-6cd3-4904-835f-1a5e48592ed1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## State & Timers\n",
    "\n",
    "Two powerful features in `DoFn` to implement stateful transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde4f57-0a53-4745-980e-91167cf20871",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### State API\n",
    "\n",
    "Stateful `ParDo` introduces a persistent mutable state which is partitioned by key and window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c63da-e94f-4165-8b20-152aa3892dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulBufferingFn(beam.DoFn):\n",
    "    MAX_BUFFER_SIZE = 500;\n",
    "    BUFFER_STATE = BagStateSpec('buffer', EventCoder())\n",
    "    COUNT_STATE = CombiningValueStateSpec('count', \n",
    "        VarIntCoder(), combiners.SumCombineFn())\n",
    "    \n",
    "    def process(self, element, buffer_state=beam.DoFn.StateParam(BUFFER_STATE),\n",
    "            count_state=beam.DoFn.StateParam(COUNT_STATE)):\n",
    "        buffer_state.add(element)\n",
    "        # Increment count and add element to buffer\n",
    "        count_state.add(1) \n",
    "        count = count_state.read()\n",
    "        # When buffer size limit is reached, a request is sent to the external service\n",
    "        if count >= MAX_BUFFER_SIZE:\n",
    "            for event in buffer_state.read():\n",
    "                yield event\n",
    "            count_state.clear()\n",
    "            buffer_state.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8295e1-f2d7-4f51-9827-db5aa05b915c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Timer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a98533-3ee1-4d09-85ed-8dde9628f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulBufferingFn(beam.DoFn):\n",
    "    MAX_BUFFER_SIZE = 500;\n",
    "    BUFFER_STATE = BagStateSpec('buffer', EventCoder())\n",
    "    COUNT_STATE = CombiningValueStateSpec('count', \n",
    "        VarIntCoder(), combiners.SumCombineFn()),\n",
    "    EXPIRY_TIMER = TimerSpec('expiry', TimeDomain.WATERMARK)\n",
    "    \n",
    "    def process(self, element, w=beam.DoFn.WindowParam,\n",
    "            buffer_state=beam.DoFn.StateParam(BUFFER_STATE),\n",
    "            count_state=beam.DoFn.StateParam(COUNT_STATE),\n",
    "            expiry_timer=beam.DoFn.TimerParam(EXPIRY_TIMER)):\n",
    "        expiry_timer.set(w.end + ALLOWED_LATENESS)\n",
    "        buffer_state.add(element)\n",
    "        # Increment count and add element to buffer\n",
    "        count_state.add(1) \n",
    "        count = count_state.read()\n",
    "        # When buffer size limit is reached, a request is sent to the external service\n",
    "        if count >= MAX_BUFFER_SIZE:\n",
    "            for event in buffer_state.read():\n",
    "                yield event\n",
    "            count_state.clear()\n",
    "            buffer_state.clear()        \n",
    "\n",
    "    # Added an event time timer so that when the window expires,\n",
    "    # any events remaining in the buffer are processed.\n",
    "    @on_timer(EXPIRY_TIMER)\n",
    "    def expiry(self, \n",
    "            buffer_state=beam.DoFn.StateParam(BUFFER_STATE),\n",
    "            count_state=beam.DoFn.StateParam(COUNT_STATE)):\n",
    "        events = buffer_state.read()\n",
    "        for event in events:\n",
    "            yield event\n",
    "        count_state.clear()\n",
    "        buffer_state.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93c8d6-0c9a-4c0c-be98-5563be95a3f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Beam SQL & Beam DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9619189-546a-4a47-a1de-82cdca254632",
   "metadata": {},
   "source": [
    "### Beam SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7f5dd-6005-419e-a722-c3c05d2d2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a SQLTransform using ZetaSQL Dialect\n",
    "SqlTransform(query, dialect='zetasql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40fb698-f705-4268-9ee5-b530225e3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataflow SQL CLI\n",
    "gcloud dataflow sql query \"\"\"SQL statements\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0c375-0f80-47dd-9758-600b5ad10c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUMBLE (fixed windows)\n",
    "SELECT\n",
    "  productId,\n",
    "  COUNT(transactionId) AS num_purchases,\n",
    "  TUMBLE_START(\"INTERVAL 10 SECOND\") AS period_start\n",
    "FROM\n",
    "  pubsub.topic.`instant-insights`.`retaildemo-online-purchase-json` AS pr\n",
    "GROUP BY\n",
    "  productId,\n",
    "  TUMBLE(pr.event_timestamp, \"Interval 10 SECOND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae4cfd-0bca-4389-af26-087932a5eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOP (sliding windows)\n",
    "SELECT\n",
    "  productId,\n",
    "  COUNT(transactionId) AS num_purchases,\n",
    "  HOP_START(\"INTERVAL 10 SECOND\", \"INTERVAL 30 SECOND\") AS period_start,\n",
    "  HOP_END(\"INTERVAL 10 SECOND\", \"INTERVAL 30 SECOND\") AS period_end\n",
    "FROM\n",
    "  pubsub.topic.`instant-insights`.`retaildemo-online-purchase-json` AS pr\n",
    "GROUP BY\n",
    "  productId,\n",
    "  HOP(pr.event_timestamp, \"Interval 10 SECOND\", \"INTERVAL 30 SECOND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67622be5-f3c1-48d8-b0ef-40a684d0539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SESSION (session windows)\n",
    "SELECT\n",
    "  userId,\n",
    "  COUNT(transactionId) AS num_purchases,\n",
    "  SESSION_START(\"INTERVAL 10 MINUTE\") AS interval_start,\n",
    "  SESSION_END(\"INTERVAL 10 MINUTE\") AS interval_end\n",
    "FROM\n",
    "  pubsub.topic.`instant-insights`.`retaildemo-online-purchase-json` AS pr\n",
    "GROUP BY\n",
    "  userId,\n",
    "  SESSION(pr.event_timestamp, \"Interval 10 MINUTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267a113-8a11-4699-b06a-0a19eb2cfa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_user_traffic_SQL_pipeline.py\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import typing\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### functions and classes\n",
    "\n",
    "class CommonLog (typing.NamedTuple):\n",
    "    ip: str\n",
    "    user_id: str\n",
    "    lat: float\n",
    "    lng: float\n",
    "    timestamp: str\n",
    "    http_request: str\n",
    "    http_response: int\n",
    "    num_bytes: int\n",
    "    user_agent: str\n",
    "\n",
    "beam.coders.registry.register_coder(CommonLog, beam.coders.RowCoder)\n",
    "\n",
    "def parse_json(element):\n",
    "    row = json.loads(element)\n",
    "    return CommonLog(**row)\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--staging_location', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--temp_location', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "    parser.add_argument('--input_path', required=True, help='Path to events.json')\n",
    "    parser.add_argument('--raw_table_name', required=True, help='BigQuery table for raw data')\n",
    "    parser.add_argument('--agg_table_name', required=True, help='BigQuery table for aggregated data')\n",
    "\n",
    "    opts, pipeline_opts = parser.parse_known_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(pipeline_opts, save_main_session=True)\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).staging_location = opts.staging_location\n",
    "    options.view_as(GoogleCloudOptions).temp_location = opts.temp_location\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('batch-user-traffic-pipeline-sql-'\n",
    "                                                                   ,time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    input_path = opts.input_path\n",
    "    agg_table_name = opts.agg_table_name\n",
    "    raw_table_name = opts.raw_table_name\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    raw_table_schema = {\n",
    "            \"fields\": [\n",
    "                {\n",
    "                    \"name\": \"ip\",\n",
    "                    \"type\": \"STRING\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"user_id\",\n",
    "                    \"type\": \"STRING\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"lat\",\n",
    "                    \"type\": \"FLOAT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"lng\",\n",
    "                    \"type\": \"FLOAT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"timestamp\",\n",
    "                    \"type\": \"STRING\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"http_request\",\n",
    "                    \"type\": \"STRING\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"http_response\",\n",
    "                    \"type\": \"INTEGER\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"num_bytes\",\n",
    "                    \"type\": \"INTEGER\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"user_agent\",\n",
    "                    \"type\": \"STRING\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    agg_table_schema = {\n",
    "        \"fields\": [\n",
    "\n",
    "            {\n",
    "                \"name\": \"user_id\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page_views\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"total_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"max_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"min_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT user_id,\n",
    "        COUNT(*) AS page_views, SUM(num_bytes) as total_bytes,\n",
    "        MAX(num_bytes) AS max_bytes, MIN(num_bytes) as min_bytes\n",
    "        FROM PCOLLECTION\n",
    "        GROUP BY user_id\n",
    "        \"\"\"\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    logs = (p | 'ReadFromGCS' >> beam.io.ReadFromText(input_path)\n",
    "              | 'ParseJson' >> beam.Map(parse_json).with_output_types(CommonLog))\n",
    "\n",
    "    (logs | 'RawToDict' >> beam.Map(lambda row : row._asdict())\n",
    "          | 'WriteRawToBQ' >> beam.io.WriteToBigQuery(\n",
    "           raw_table_name,\n",
    "           schema=raw_table_schema,\n",
    "           create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "           write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "      ))\n",
    "\n",
    "    (logs | 'PerUserAggregations' >> SqlTransform(query, dialect='zetasql')\n",
    "          | 'AggToDict' >> beam.Map(lambda row : row._asdict())\n",
    "          | 'WriteAggToBQ' >> beam.io.WriteToBigQuery(\n",
    "            agg_table_name,\n",
    "            schema=agg_table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f877d-897f-45b7-a4fb-528d199b10b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "export REGION='us-central1'\n",
    "export BUCKET=gs://${PROJECT_ID}\n",
    "export PIPELINE_FOLDER=${BUCKET}\n",
    "export RUNNER=DataflowRunner\n",
    "export INPUT_PATH=${PIPELINE_FOLDER}/events.json\n",
    "export TABLE_NAME=${PROJECT_ID}:logs.user_traffic\n",
    "export AGGREGATE_TABLE_NAME=${PROJECT_ID}:logs.user_traffic\n",
    "export RAW_TABLE_NAME=${PROJECT_ID}:logs.raw\n",
    "python3 batch_user_traffic_SQL_pipeline.py \\\n",
    "--project=${PROJECT_ID} \\\n",
    "--region=${REGION} \\\n",
    "--staging_location=${PIPELINE_FOLDER}/staging \\\n",
    "--temp_location=${PIPELINE_FOLDER}/temp \\\n",
    "--runner=${RUNNER} \\\n",
    "--experiments=use_runner_v2 \\\n",
    "--input_path=${INPUT_PATH} \\\n",
    "--agg_table_name=${AGGREGATE_TABLE_NAME} \\\n",
    "--raw_table_name=${RAW_TABLE_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6567b1e-535a-43c1-a591-6f0783342ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_minute_traffic_SQL_pipeline.py\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import typing\n",
    "from datetime import datetime\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### functions and classes\n",
    "\n",
    "class CommonLog(typing.NamedTuple):\n",
    "    ip: str\n",
    "    user_id: str\n",
    "    lat: float\n",
    "    lng: float\n",
    "    ts: str\n",
    "    http_request: str\n",
    "    http_response: int\n",
    "    num_bytes: int\n",
    "    user_agent: str\n",
    "\n",
    "beam.coders.registry.register_coder(CommonLog, beam.coders.RowCoder)\n",
    "\n",
    "def parse_json(element):\n",
    "    row = json.loads(element)\n",
    "    row['ts'] = row['timestamp']\n",
    "    row.pop('timestamp')\n",
    "    return CommonLog(**row)\n",
    "\n",
    "def format_timestamp(element):\n",
    "    ts = datetime.strptime(element.ts[:-8], \"%Y-%m-%dT%H:%M:%S\")\n",
    "    ts = datetime.strftime(ts, \"%Y-%m-%d %H:%M:%S\")\n",
    "    temp_dict = element._asdict()\n",
    "    temp_dict['ts'] = ts\n",
    "    return CommonLog(**temp_dict)\n",
    "\n",
    "def to_dict(row):\n",
    "    return {'page_views' : row.page_views,\n",
    "            'start_time' : row.start_time}\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--stagingLocation', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--tempLocation', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "    parser.add_argument('--inputPath', required=True, help='Path to events.json')\n",
    "    parser.add_argument('--tableName', required=True, help='BigQuery table name')\n",
    "\n",
    "    opts, pipeline_opts = parser.parse_known_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(pipeline_opts, save_main_session=True)\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).staging_location = opts.stagingLocation\n",
    "    options.view_as(GoogleCloudOptions).temp_location = opts.tempLocation\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('batch-minute-traffic-pipeline-sql'\n",
    "                                                                   ,time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    input_path = opts.inputPath\n",
    "    table_name = opts.tableName\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"page_views\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"start_time\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    query = '''\n",
    "        SELECT\n",
    "            COUNT(*) AS page_views,\n",
    "            STRING(window_start) AS start_time\n",
    "        FROM\n",
    "            TUMBLE(\n",
    "                (SELECT TIMESTAMP(ts) AS ts FROM PCOLLECTION),\n",
    "                DESCRIPTOR(ts),\n",
    "                'INTERVAL 1 MINUTE')\n",
    "        GROUP BY window_start\n",
    "    '''\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    (p | 'ReadFromGCS' >> beam.io.ReadFromText(input_path)\n",
    "       | 'ParseJson' >> beam.Map(parse_json).with_output_types(CommonLog)\n",
    "       | 'FormatTimestamp' >> beam.Map(format_timestamp).with_output_types(CommonLog)\n",
    "       | \"CountPerMinute\" >> SqlTransform(query, dialect='zetasql')\n",
    "       | \"ConvertToDict\" >> beam.Map(to_dict)\n",
    "       | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n",
    "            table_name,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f967b42-138a-47e4-996b-a992bd2fb840",
   "metadata": {},
   "outputs": [],
   "source": [
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "export REGION='us-central1'\n",
    "export BUCKET=gs://${PROJECT_ID}\n",
    "export PIPELINE_FOLDER=${BUCKET}\n",
    "export RUNNER=DataflowRunner\n",
    "export INPUT_PATH=${PIPELINE_FOLDER}/events.json\n",
    "export TABLE_NAME=${PROJECT_ID}:logs.minute_traffic\n",
    "python3 batch_minute_traffic_SQL_pipeline.py \\\n",
    "--project=${PROJECT_ID} \\\n",
    "--region=${REGION} \\\n",
    "--stagingLocation=${PIPELINE_FOLDER}/staging \\\n",
    "--tempLocation=${PIPELINE_FOLDER}/temp \\\n",
    "--runner=${RUNNER} \\\n",
    "--inputPath=${INPUT_PATH} \\\n",
    "--tableName=${TABLE_NAME} \\\n",
    "--experiments=use_runner_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a414c970-376b-41e2-a276-b669c57faa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming_minute_traffic_SQL_pipeline.py\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import typing\n",
    "from datetime import datetime\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### functions and classes\n",
    "\n",
    "class CommonLog(typing.NamedTuple):\n",
    "    ip: str\n",
    "    user_id: str\n",
    "    lat: float\n",
    "    lng: float\n",
    "    timestamp: str\n",
    "    event_timestamp: str\n",
    "    http_request: str\n",
    "    http_response: int\n",
    "    num_bytes: int\n",
    "    user_agent: str\n",
    "\n",
    "beam.coders.registry.register_coder(CommonLog, beam.coders.RowCoder)\n",
    "\n",
    "def parse_json(element):\n",
    "    row = json.loads(element.decode('utf-8'))\n",
    "    return row\n",
    "\n",
    "class GetEventTimestampFn(beam.DoFn):\n",
    "    def process(self, row, timestamp=beam.DoFn.TimestampParam):\n",
    "        event_ts = timestamp.to_utc_datetime().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        row['event_timestamp'] = event_ts\n",
    "        yield CommonLog(**row)\n",
    "\n",
    "class ParseAndGetEventTimestamp(beam.PTransform):\n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll\n",
    "            | 'ParseJson' >> beam.Map(parse_json)\n",
    "            | 'GetEventTimestamp' >> beam.ParDo(GetEventTimestampFn())\n",
    "            )\n",
    "\n",
    "def to_dict(row):\n",
    "    return {'page_views' : row.page_views,\n",
    "            'start_time' : row.start_time}\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json from Pub/Sub into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--staging_location', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--temp_location', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "    parser.add_argument('--input_topic', required=True, help='Input Pub/Sub Topic')\n",
    "    parser.add_argument('--table_name', required=True, help='BigQuery table name for aggregate results')\n",
    "\n",
    "\n",
    "    opts, pipeline_opts = parser.parse_known_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(pipeline_opts, save_main_session=True, streaming=True)\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).staging_location = opts.staging_location\n",
    "    options.view_as(GoogleCloudOptions).temp_location = opts.temp_location\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('streaming-minute-traffic-sql-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    input_topic = opts.input_topic\n",
    "    table_name = opts.table_name\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"page_views\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"start_time\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    query = '''\n",
    "        SELECT\n",
    "            COUNT(*) AS page_views,\n",
    "            STRING(window_start) AS start_time\n",
    "        FROM\n",
    "            TUMBLE(\n",
    "                (SELECT TIMESTAMP(event_timestamp) AS ts FROM PCOLLECTION),\n",
    "                DESCRIPTOR(ts),\n",
    "                'INTERVAL 1 MINUTE')\n",
    "        GROUP BY window_start\n",
    "    '''\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    (p | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(input_topic)\n",
    "       | 'ParseAndGetEventTimestamp' >> ParseAndGetEventTimestamp().with_output_types(CommonLog)\n",
    "       | \"CountPerMinute\" >> SqlTransform(query, dialect='zetasql')\n",
    "       | \"ConvertToDict\" >> beam.Map(to_dict)\n",
    "       | 'WriteAggToBQ' >> beam.io.WriteToBigQuery(\n",
    "            table_name,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run().wait_until_finish()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04acd8f1-4626-4d50-ae01-c9e9f4971257",
   "metadata": {},
   "outputs": [],
   "source": [
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "export REGION='us-central1'\n",
    "export BUCKET=gs://${PROJECT_ID}\n",
    "export PIPELINE_FOLDER=${BUCKET}\n",
    "export RUNNER=DataflowRunner\n",
    "export PUBSUB_TOPIC=projects/${PROJECT_ID}/topics/my_topic\n",
    "export TABLE_NAME=${PROJECT_ID}:logs.minute_traffic\n",
    "python3 streaming_minute_traffic_SQL_pipeline.py \\\n",
    "--project=${PROJECT_ID} \\\n",
    "--region=${REGION} \\\n",
    "--staging_location=${PIPELINE_FOLDER}/staging \\\n",
    "--temp_location=${PIPELINE_FOLDER}/temp \\\n",
    "--runner=${RUNNER} \\\n",
    "--input_topic=${PUBSUB_TOPIC} \\\n",
    "--table_name=${TABLE_NAME} \\\n",
    "--experiments=use_runner_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4d603-0cf6-4dbc-a130-0e8e0e4df83b",
   "metadata": {},
   "source": [
    "### Beam DataFrames\n",
    "\n",
    "- A more Pythonic expressive API compatible with Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac4fe0f-8754-4b31-a0cc-0ce6f151b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DataframeTransform\n",
    "def my_function(df):\n",
    "    df['C'] = df.A + 2*df.B\n",
    "    result = df.groupby('C').sum().filter('A < 0')\n",
    "    return result\n",
    "\n",
    "output = input | DataframeTransform(my_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb245cb9-2d0a-42f3-a8b3-60f0df592760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe/PCollection conversion\n",
    "with beam.Pipeline() as p:\n",
    "    df = to_dataframe(pc)\n",
    "    pc = to_pcollection(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abc533-7d62-4cec-8ccf-d441ef90bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count words\n",
    "words = (pipeline\n",
    "    | \"Split\" >> beam.FlatMap(lambda line: re.findall(r'[\\w]+', line))\n",
    "        .with_output_types(str)\n",
    "    # Map to Row objects to generate a schema suitable for conversion to a dataframe.\n",
    "    | \"ToRows\" >> beam.Map(lambda word beam.Row(word=word))\n",
    ")\n",
    "\n",
    "df = to_dataframe(words)\n",
    "df['count'] = 1\n",
    "counted = df.groupby('word').sum()\n",
    "counted.to_csv(known_args.output)\n",
    "\n",
    "# Deferred DatFrames can also be converted back to schema PCollections\n",
    "counted_pc = to_pcollection(counted, include_indexes=True)\n",
    "\n",
    "# Print out every word that occurred > 50 times\n",
    "_ = (counted_pc\n",
    "    | beam.Filter(lambda row: row.count > 50)\n",
    "    | beam.Map(lambda row: f'{row.word}: {row.count}')\n",
    "    | beam.Map(print)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7b77a-0864-4811-a752-5526fd483eba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Beam Notebooks\n",
    "\n",
    "Include the `interactive_runner` and `interactive_beam` modules in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f69a9-d187-4c72-9ad7-ca8ac0f73686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f5229-f4e5-4acb-a751-ba405f698dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the recording duration to 10 min\n",
    "ib.options.recording_duration = '10m'\n",
    "\n",
    "# Set the recording size limit to 1 GB\n",
    "ib.options.recording_size_limit = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72626746-7a6f-48de-8d60-2c6ab56c0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = p | \"read\" >> beam.io.ReadFromPubSub(topic=topic)\n",
    "\n",
    "windowed_words = (words | \"window\" >> beam.WindowInto(beam.window.FixedWindows(10)))\n",
    "\n",
    "windowed_words_counts = (windowed_words | \"count\" >> beam.combiners.Count.PerElement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fc1f4-3575-44e4-bd9f-4b3b01437d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materializes the resulting PCollection in a table\n",
    "ib.show(windowed_word_counts, include_window_info=True)\n",
    "\n",
    "# Load the output in a Pandas DataFrame\n",
    "ib.collect(windowed_word_counts, include_window_info=True)\n",
    "\n",
    "# Visualize the data in the Notebook\n",
    "ib.show(windowed_word_counts, include_window_info=True, visualize_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc019c-fdc5-476a-968e-bb55459dae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the production Dataflow runner\n",
    "from apache_beam.runners import DataflowRunner\n",
    "\n",
    "# Set up Apache Beam pipeline options\n",
    "options = pipeline_options.PipelineOptions()\n",
    "\n",
    "# Run the pipeline\n",
    "runner = DataflowRunner()\n",
    "runner.run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb32f53-bc0b-415a-9ce7-5e5444d172e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "storeSales = p | beam.io.ReadFromText(\"purchases-store\")\n",
    "               | beam.Map(lambda s: ...)\n",
    "\n",
    "onlineSales = p | beam.io.ReadFromText(\"purchase-online\")\n",
    "                | beam.Map(lambda s: ...)\n",
    "    \n",
    "topSales = (storeSales, onlineSales)\n",
    "                | beam.Flatten()\n",
    "                | beam.Combiners.Count.perKey()\n",
    "                | beam.Combiners.Top.of(10, key=lambda x: x[1])\n",
    "            \n",
    "topSales        | beam.io.WriteToBigQuery(topSales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7868bd-c1f0-4384-91b9-3239e6549756",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b2601-644e-4f1c-8976-8df81f33e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schemas\n",
    "class Purchase(typing.NamedTuple):\n",
    "    user_id: str # The id of the user who made the purchase.\n",
    "    item_id: int # The identifier of the item that was purchased.\n",
    "    shipping_address: ShippingAddress # The shipping address, a nested type.\n",
    "    cost_cents: int # The cost of the item\n",
    "    transactions: typing.Sequence[Transaction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd933891-d62f-43ed-8533-9308113e1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DoFn for micro batching\n",
    "class MyDoFn(beam.DoFn):\n",
    "    def setup(self):\n",
    "        pass\n",
    "    def start_bundle(self):\n",
    "        pass\n",
    "    def process(self, element):\n",
    "        pass\n",
    "    def finish_bundle(self):\n",
    "        pass\n",
    "    def teardown(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac0e6b0-e767-400a-b420-49ac1fb017fd",
   "metadata": {},
   "source": [
    "### Dealing with late data\n",
    "\n",
    "\n",
    "**Allowed lateness**\n",
    "\n",
    "`Allowed lateness` controls how long a window should retain its state; once the watermark reaches the end of the allowed lateness period, all state is dropped. A clean and concise way of doing this is by defining a horizon on the allowed lateness within the system, i.e. placing a bound on how late any given record may be (relative to the watermark) for the system to bother processing it; any data that arrives after this horizon is simply dropped.\n",
    "\n",
    "**Triggers**\n",
    "\n",
    "`Triggers` determine at what point during processing time results will be materialized. Triggers fire panes when the triggerâ€™s conditions are met. Set the trigger(s) for a PCollection by setting the `trigger` keyword argument of `WindowInto` PTransform:\n",
    "- `AfterWatermark` for firing when the watermark passes a timestamp determined from either the end of the window or the arrival of the first element in a pane.\n",
    "- `AfterProcessingTime` for firing after some amount of processing time has elapsed (typically since the first element in a pane).\n",
    "- `AfterCount` for firing when the number of elements in the window reaches a certain count."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9356b796-4a62-4bf2-8a42-9b0b646efc4b",
   "metadata": {},
   "source": [
    "### Dealing with malformed data\n",
    "\n",
    "Produce a branching pipeline to have a single transform produce multiple outputs while processing the input `PCollection` one time. Uses a class called `TaggedOutput` to key the outputs of the `DoFn` with multiple (possibly heterogeneous) outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d622892-a3a0-41ee-b5ff-a957f738fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming_minute_traffic_pipeline.py\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import typing\n",
    "from datetime import datetime\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import fileio\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.transforms.trigger import AfterWatermark, AfterCount, AfterProcessingTime\n",
    "from apache_beam.transforms.trigger import AccumulationMode\n",
    "from apache_beam.transforms.combiners import CountCombineFn\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### functions and classes\n",
    "\n",
    "class CommonLog(typing.NamedTuple):\n",
    "    ip: str\n",
    "    user_id: str\n",
    "    lat: float\n",
    "    lng: float\n",
    "    timestamp: str\n",
    "    http_request: str\n",
    "    http_response: int\n",
    "    num_bytes: int\n",
    "    user_agent: str\n",
    "\n",
    "beam.coders.registry.register_coder(CommonLog, beam.coders.RowCoder)\n",
    "\n",
    "class ConvertToCommonLogFn(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        try:\n",
    "            row = json.loads(element.decode('utf-8'))\n",
    "            yield beam.pvalue.TaggedOutput('parsed_row', CommonLog(**row))\n",
    "        except:\n",
    "            yield beam.pvalue.TaggedOutput('unparsed_row', element.decode('utf-8'))\n",
    "\n",
    "\n",
    "class GetTimestampFn(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        window_start = window.start.to_utc_datetime().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        output = {'page_views': element, 'timestamp': window_start}\n",
    "        yield output\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json from Pub/Sub into BigQuery')\n",
    "\n",
    "    # Google Cloud options\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--staging_location', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--temp_location', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "\n",
    "    # Pipeline-specific options\n",
    "    parser.add_argument('--window_duration', required=True, help='Window duration in seconds')\n",
    "    parser.add_argument('--table_name', required=True, help='Output BQ table')\n",
    "    parser.add_argument('--input_topic', required=True, help='Input Pub/Sub topic')\n",
    "    parser.add_argument('--allowed_lateness', required=True, help='Allowed lateness')\n",
    "    parser.add_argument('--dead_letter_bucket', required=True, help='GCS Bucket for unparsable Pub/Sub messages')\n",
    "\n",
    "    opts, pipeline_opts = parser.parse_known_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(pipeline_opts, save_main_session=True, streaming=True)\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).staging_location = opts.staging_location\n",
    "    options.view_as(GoogleCloudOptions).temp_location = opts.temp_location\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('streaming-minute-traffic-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    input_topic = opts.input_topic\n",
    "    table_name = opts.table_name\n",
    "    window_duration = opts.window_duration\n",
    "    allowed_lateness = opts.allowed_lateness\n",
    "    dead_letter_bucket = opts.dead_letter_bucket\n",
    "\n",
    "    output_path = dead_letter_bucket + '/deadletter/'\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"page_views\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "\n",
    "\n",
    "    rows = (p | 'ReadFromPubSub' >> beam.io.ReadFromPubSub(input_topic)\n",
    "              | 'ParseJson' >> beam.ParDo(ConvertToCommonLogFn())\n",
    "                    .with_outputs('parsed_row', 'unparsed_row')\n",
    "                    .with_output_types(CommonLog))\n",
    "\n",
    "    (rows.unparsed_row\n",
    "        | 'FireEvery120s' >> beam.WindowInto(beam.window.FixedWindows(120),\n",
    "            trigger=AfterProcessingTime(120),\n",
    "            accumulation_mode=AccumulationMode.DISCARDING)\n",
    "        # Dead-letter storage\n",
    "        | 'WriteUnparsedToGCS' >> fileio.WriteToFiles(output_path,\n",
    "            shards=1,\n",
    "            max_writers_per_bundle=0)\n",
    "    )\n",
    "\n",
    "    (rows.parsed_row\n",
    "        | \"WindowByMinute\" >> beam.WindowInto(beam.window.FixedWindows(int(window_duration)),\n",
    "            trigger=AfterWatermark(late=AfterCount(1)),\n",
    "            allowed_lateness=int(allowed_lateness),\n",
    "            accumulation_mode=AccumulationMode.ACCUMULATING)\n",
    "        | \"CountPerMinute\" >> beam.CombineGlobally(CountCombineFn()).without_defaults()\n",
    "        | \"AddWindowTimestamp\" >> beam.ParDo(GetTimestampFn())\n",
    "        | 'WriteAggToBQ' >> beam.io.WriteToBigQuery(\n",
    "            table_name,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run().wait_until_finish()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459430a-de0f-41d0-bdba-85834426d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "export REGION='us-central1'\n",
    "export BUCKET=gs://${PROJECT_ID}\n",
    "export PIPELINE_FOLDER=${BUCKET}\n",
    "export RUNNER=DataflowRunner\n",
    "export PUBSUB_TOPIC=projects/${PROJECT_ID}/topics/my_topic\n",
    "export WINDOW_DURATION=60\n",
    "export ALLOWED_LATENESS=1\n",
    "export OUTPUT_TABLE_NAME=${PROJECT_ID}:logs.minute_traffic\n",
    "export DEADLETTER_BUCKET=${BUCKET}\n",
    "cd $BASE_DIR\n",
    "python3 streaming_minute_traffic_pipeline.py \\\n",
    "--project=${PROJECT_ID} \\\n",
    "--region=${REGION} \\\n",
    "--staging_location=${PIPELINE_FOLDER}/staging \\\n",
    "--temp_location=${PIPELINE_FOLDER}/temp \\\n",
    "--runner=${RUNNER} \\\n",
    "--input_topic=${PUBSUB_TOPIC} \\\n",
    "--window_duration=${WINDOW_DURATION} \\\n",
    "--allowed_lateness=${ALLOWED_LATENESS} \\\n",
    "--table_name=${OUTPUT_TABLE_NAME} \\\n",
    "--dead_letter_bucket=${DEADLETTER_BUCKET} \\\n",
    "--allow_unsafe_triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874543f2-d09e-46cf-8ffd-b38a92f81090",
   "metadata": {},
   "outputs": [],
   "source": [
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "export REGION='us-central1'\n",
    "export BUCKET=gs://${PROJECT_ID}/deadletter\n",
    "gsutil ls $BUCKET\n",
    "gsutil cat $BUCKET/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc6c5e-df68-44ce-a70c-c95c5d4334f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Troubleshooting & Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5729b3e9-3735-4faa-8aef-d45b297a3d1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Adding exception handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9bf763-848d-4753-b22b-d4e27cad07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterMessagesFn(beam.DoFn):\n",
    "    BAD_MESSAGE_TAG = 'bad_message'\n",
    "    GOOD_MESSAGE_TAG = 'good_message'\n",
    "\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        try:\n",
    "            data = element.decode()\n",
    "            # tag the element accordingly\n",
    "            if 'bad' in data:\n",
    "                yield pvalue.TaggedOutput(self.BAD_MESSAGE_TAG, element)\n",
    "            else:\n",
    "                yield pvalue.TaggedOutput(self.GOOD_MESSAGE_TAG, element)\n",
    "                \n",
    "        # handle any exceptions in the processing\n",
    "        except Exception as exp:\n",
    "            logging.getLogger.warning(exp)\n",
    "            yield pvalue.TaggedOutput(self.BAD_MESSAGE_TAG, element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0571dd7-1feb-4059-b92a-9124550d8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_pipeline.py\n",
    "import argparse\n",
    "import logging\n",
    "import argparse, logging, os\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "class ReadGBK(beam.DoFn):\n",
    "    def process(self, e):\n",
    "        k, elems = e\n",
    "        for v in elems:\n",
    "            logging.info(f\"the element is {v}\")\n",
    "            yield v\n",
    "            \n",
    "def run(argv=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--output', dest='output', help='Output file to write results to.')\n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "    read_query = \"\"\"(\n",
    "                 SELECT\n",
    "                   version,\n",
    "                   block_hash,\n",
    "                   block_number\n",
    "                 FROM\n",
    "                   `bigquery-public-data.crypto_bitcoin.transactions`\n",
    "                 WHERE\n",
    "                   version = 1\n",
    "                 LIMIT\n",
    "                   1000000 )\n",
    "               UNION ALL (\n",
    "                 SELECT\n",
    "                   version,\n",
    "                   block_hash,\n",
    "                   block_number\n",
    "                 FROM\n",
    "                   `bigquery-public-data.crypto_bitcoin.transactions`\n",
    "                 WHERE\n",
    "                   version = 2\n",
    "                 LIMIT\n",
    "                   1000 ) ;\"\"\"\n",
    "    \n",
    "    p = beam.Pipeline(options=PipelineOptions(pipeline_args))\n",
    "    (p\n",
    "    | 'Read from BigQuery' >> beam.io.ReadFromBigQuery(\n",
    "        query=read_query, use_standard_sql=True)\n",
    "    | \"Add Hotkey\" >> beam.Map(lambda elem: (elem[\"version\"], elem))\n",
    "    | \"Groupby\" >> beam.GroupByKey()\n",
    "    | 'Print' >>  beam.ParDo(ReadGBK())\n",
    "    | 'Sink' >>  WriteToText(known_args.output))\n",
    "    result = p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger().setLevel(logging.INFO)\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70afa7d7-263f-4112-aea6-f9e95204608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a storage bucket\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "gsutil mb -l US gs://$PROJECT_ID\n",
    "\n",
    "# Attempt to launch the pipeline\n",
    "# Launch the pipeline\n",
    "python3 my_pipeline.py \\\n",
    "  --project=${PROJECT_ID} \\\n",
    "  --region=us-central1 \\\n",
    "  --output=gs://$PROJECT_ID/results/prefix \\\n",
    "  --tempLocation=gs://$PROJECT_ID/temp/ \\\n",
    "  --max_num_workers=5 \\\n",
    "  --runner=DataflowRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f2a831-f9bb-40b8-981d-37a8a893e3c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87468d5-e49e-4aba-a93a-ec7cef31741f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Graph Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e37bd6-9ebc-4ba6-8f8a-a40a51b40fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshuffle after ParDo\n",
    "_ = pcoll | beam.Reshuffle()\n",
    "\n",
    "# Side input\n",
    "_ = pcoll | beam.FlatMap(cross_join, rights=beam.pvalue.AsIter(side_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37bb52a-de2b-493d-b193-0764d313f1b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Disaster Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24e28b3-fe6c-4ab4-a6a9-6e20f0e3cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a snapshot of a subscription\n",
    "gcloud pubsub snapshots create my-snapshot \\\n",
    "--subscription=my-sub\n",
    "\n",
    "# Stop and drain Dataflow job\n",
    "gcloud dataflow jobs drain [job-id]\n",
    "\n",
    "# Seek subscription to the snapshot\n",
    "gcloud pubsub subscriptions seek my-sub --snapshot=my-snapshot\n",
    "\n",
    "# Resubmit the pipeline\n",
    "gcloud dataflow jobs run my-job-name \\\n",
    "--gcs_location=my_gcs_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d804f1-13ac-4809-9a0c-30cc460094f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CI/CD Testing\n",
    "\n",
    "Introduce frameworks and features available to streamline CI/CD workflow for Dataflow pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9779656a-8787-4cce-aea1-12577073724b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Unit Testing\n",
    "\n",
    "Performing unit tests for DoFns and PTransforms.\n",
    "`TestPipeline` is a special class included in the Beam SDK specifically for testing transforms and pipeline logic. Use the `assert_that` method to check that the output PCollection matches the expected output, and `equal_to` to verify that the output PCollection has the correct elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a23215-593d-4ded-aa7d-97e5de5c05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python PAssert\n",
    "from apache_beam.testing.util import assert_that\n",
    "from apache_beam.testing.util import equal_to\n",
    "\n",
    "# Python TestPipeline\n",
    "with TestPipeline as p:\n",
    "    INPUTS = [fake_input_1, fake_input_2]\n",
    "    test_output = p \n",
    "        | beam.Create(INPUTS) # Transforms to be tested\n",
    "    # Check whether a PCollection contains some elements in any order.\n",
    "        | assert_that(test_output, equal_to(EXPECTED_OUTPUTS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa8d76-ec61-4c23-849a-e99ce58b1fba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### In-flight Actions\n",
    "\n",
    "Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d04a9b-f131-4def-97a1-e4c04907644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m apache_beam.examples.wordcount \\\n",
    "--project $PROJECT \\\n",
    "--staging_location=gs://$BUCKET/tmp/ \\\n",
    "--input=gs://dataflow-samples/shakespeare/kinglear.txt \\\n",
    "--output=gs://$BUCKET/results/outputs \\\n",
    "--runner=DataflowRunner \\\n",
    "--update \\\n",
    "--job_name=[prior job name] \\\n",
    "--transform_name_mapping=='{\"oldTransform1\":\"newTransform1\", ...}' \\\n",
    "--region=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1ded7-e14d-4049-bf03-1a4bd46bf0d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Grant the `dataflow.worker` role to the Compute Engine default service account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350311c6-2d3e-4411-b8fd-28497ce8908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID=$(gcloud config get-value project)\n",
    "export PROJECT_NUMBER=$(gcloud projects list --filter=\"$PROJECT_ID\" \n",
    "    --format=\"value(PROJECT_NUMBER)\")\n",
    "export serviceAccount=\"\"$PROJECT_NUMBER\"-compute@developer.gserviceaccount.com\"\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "--member=\"serviceAccount:${serviceAccount}\" \\\n",
    "--role=\"roles/dataflow.worker\"\n",
    "\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "gsutil mb -l US gs://$PROJECT_ID\n",
    "gsutil cp testing.out gs://$PROJECT_ID/8a_Batch_Testing_Pipeline/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042071c-bbea-416f-86ae-6d47fbcc6337",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Performing unit tests for DoFns and PTransforms for a batch pipeline\n",
    "\n",
    "- Create a `TestPipeline`.\n",
    "- Create some test input data and use the `Create` transform to create a `PCollection` of your input data.\n",
    "- Apply your transform to the input `PCollection` and save the resulting `PCollection`.\n",
    "- Use the `assert_that` method from the `testing.util` module and its other methods to verify that the output `PCollection` contains the elements that you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b50924-a506-4603-b3ce-7ab70ef4f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_statistics_pipeline.py\n",
    "import json\n",
    "import typing\n",
    "import logging\n",
    "import apache_beam as beam\n",
    "\n",
    "class WeatherRecord(typing.NamedTuple):\n",
    "    loc_id: str\n",
    "    lat: float\n",
    "    lng: float\n",
    "    date: str\n",
    "    low_temp: float\n",
    "    high_temp: float\n",
    "    precip: float\n",
    "\n",
    "beam.coders.registry.register_coder(WeatherRecord, beam.coders.RowCoder)\n",
    "\n",
    "class ConvertCsvToWeatherRecord(beam.DoFn):\n",
    "\n",
    "    def process(self, line):\n",
    "        fields = 'loc_id,lat,lng,date,low_temp,high_temp,precip'.split(',')\n",
    "        values = line.split(',')\n",
    "        row = dict(zip(fields,values))\n",
    "        for num_field in ('lat', 'lng', 'low_temp', 'high_temp', 'precip'):\n",
    "            row[num_field] = float(row[num_field])\n",
    "        yield WeatherRecord(**row)\n",
    "\n",
    "class ConvertTempUnits(beam.DoFn):\n",
    "\n",
    "    def process(self, row):\n",
    "        row_dict = row._asdict()\n",
    "        for field in ('low_temp', 'high_temp'):\n",
    "            row_dict[field] = row_dict[field] * 1.8 + 32.0\n",
    "        yield WeatherRecord(**row_dict)\n",
    "\n",
    "class ConvertToJson(beam.DoFn):\n",
    "\n",
    "    def process(self, row):\n",
    "        line = json.dumps(row._asdict())\n",
    "        yield line\n",
    "\n",
    "class ComputeStatistics(beam.PTransform):\n",
    "\n",
    "    def expand(self, pcoll):\n",
    "    \n",
    "        results = (\n",
    "            pcoll | 'ComputeStatistics' >> beam.GroupBy('loc_id')\n",
    "                                        .aggregate_field('low_temp', min, 'record_low')\n",
    "                                        .aggregate_field('high_temp', max, 'record_high')\n",
    "                                        .aggregate_field('precip', sum, 'total_precip')\n",
    "                | 'ToJson' >> beam.ParDo(ConvertToJson())\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "\n",
    "class WeatherStats(beam.PTransform):\n",
    "\n",
    "    def expand(self, pcoll):\n",
    "\n",
    "        results = (\n",
    "            pcoll | \"ParseCSV\" >> beam.ParDo(ConvertCsvToWeatherRecord())\n",
    "                  | \"ConvertToF\" >> beam.ParDo(ConvertTempUnits())\n",
    "                  | \"ComputeStats\" >> ComputeStatistics()\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "def run():\n",
    "\n",
    "    p = beam.Pipeline()\n",
    "\n",
    "    (p | 'ReadCSV' >> beam.io.ReadFromText('./weather_data.csv')\n",
    "       | 'ComputeStatistics' >> WeatherStats()\n",
    "       | 'WriteJson' >> beam.io.WriteToText('./weather_stats', '.json')\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d88c67-1b4f-4a6d-b29e-33c0f3a5a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_statistics_pipeline_test.py\n",
    "import logging\n",
    "import json\n",
    "import unittest\n",
    "import sys\n",
    "\n",
    "from weather_statistics_pipeline import *\n",
    "from apache_beam.testing.test_pipeline import TestPipeline\n",
    "from apache_beam.testing.util import BeamAssertException\n",
    "from apache_beam.testing.util import assert_that, equal_to\n",
    "\n",
    "def main(out = sys.stderr, verbosity = 2):\n",
    "    loader = unittest.TestLoader()\n",
    "  \n",
    "    suite = loader.loadTestsFromModule(sys.modules[__name__])\n",
    "    unittest.TextTestRunner(out, verbosity = verbosity).run(suite)\n",
    "\n",
    "\n",
    "class ConvertToWeatherRecordTest(unittest.TestCase):\n",
    "\n",
    "    def test_convert_to_csv(self):\n",
    "\n",
    "        with TestPipeline() as p:\n",
    "\n",
    "            LINES = ['x,0.0,0.0,2/2/2021,1.0,2.0,0.1']\n",
    "            EXPECTED_OUTPUT = [WeatherRecord('x', 0.0, 0.0, '2/2/2021', 1.0, 2.0, 0.1)]\n",
    "\n",
    "            input_lines = p | beam.Create(LINES)\n",
    "\n",
    "            output = input_lines | beam.ParDo(ConvertCsvToWeatherRecord())\n",
    "\n",
    "            assert_that(output, equal_to(EXPECTED_OUTPUT))\n",
    "\n",
    "class ConvertTempUnitsTest(unittest.TestCase):\n",
    "\n",
    "    def test_convert_temp_units(self):\n",
    "\n",
    "        with TestPipeline() as p:\n",
    "\n",
    "            RECORDS = [WeatherRecord('x', 0.0, 0.0, '2/2/2021', 1.0, 2.0, 0.1),\n",
    "                       WeatherRecord('y', 0.0, 0.0, '2/2/2021', -3.0, -1.0, 0.3)]\n",
    "\n",
    "            EXPECTED_RECORDS = [WeatherRecord('x', 0.0, 0.0, '2/2/2021', 33.8, 35.6, 0.1),\n",
    "                               WeatherRecord('y', 0.0, 0.0, '2/2/2021', 26.6, 30.2, 0.3)]\n",
    "\n",
    "            input_records = p | beam.Create(RECORDS)\n",
    "\n",
    "            output = input_records | beam.ParDo(ConvertTempUnits())\n",
    "            \n",
    "            assert_that(output, equal_to(EXPECTED_RECORDS))\n",
    "\n",
    "class ComputeStatsTest(unittest.TestCase):\n",
    "    \n",
    "    def test_compute_statistics(self):\n",
    "\n",
    "        with TestPipeline() as p:\n",
    "\n",
    "            INPUT_RECORDS = [WeatherRecord('x', 0.0, 0.0, '2/2/2021', 33.8, 35.6, 0.1),\n",
    "                             WeatherRecord('x', 0.0, 0.0, '2/3/2021', 41.6, 65.3, 0.2),\n",
    "                             WeatherRecord('x', 0.0, 0.0, '2/4/2021', 45.3, 52.6, 0.2),\n",
    "                             WeatherRecord('y', 0.0, 0.0, '2/2/2021', 12.8, 23.6, 0.1),\n",
    "                             WeatherRecord('y', 0.0, 0.0, '2/3/2021', 26.6, 30.2, 0.3)]\n",
    "\n",
    "            EXPECTED_STATS = [json.dumps({'loc_id': 'x', 'record_low': 33.8, 'record_high': 65.3, 'total_precip': 0.5 }),\n",
    "                              json.dumps({'loc_id': 'y', 'record_low': 12.8, 'record_high': 30.2, 'total_precip': 0.4 })]\n",
    "\n",
    "            inputs = p | beam.Create(INPUT_RECORDS)\n",
    "\n",
    "            output = inputs | ComputeStatistics()\n",
    "\n",
    "            assert_that(output, equal_to(EXPECTED_STATS))\n",
    "\n",
    "class WeatherStatsTransformTest(unittest.TestCase):\n",
    "\n",
    "    def test_weather_stats_transform(self):\n",
    "\n",
    "        with TestPipeline() as p:\n",
    "\n",
    "            INPUT_STRINGS = [\"x,31.4,-39.2,2/2/21,4.0,7.5,0.1\",\n",
    "                             \"x,31.4,-39.2,2/2/21,3.5,6.0,0.3\",\n",
    "                             \"y,33.4,-49.2,2/2/21,12.5,17.5,0.5\"]\n",
    "\n",
    "            EXPECTED_STATS = [json.dumps({'loc_id': 'x', 'record_low': 38.3, 'record_high': 45.5, 'total_precip': 0.4 }),\n",
    "                              json.dumps({'loc_id': 'y', 'record_low': 54.5, 'record_high': 63.5, 'total_precip': 0.5 })]\n",
    "\n",
    "            inputs = p | beam.Create(INPUT_STRINGS)\n",
    "\n",
    "            output = inputs | WeatherStats()\n",
    "\n",
    "            assert_that(output, equal_to(EXPECTED_STATS))\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    with open('testing.out', 'w') as f:\n",
    "        main(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6654814-17cc-40f0-b916-655e22969f7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Perform unit testing for a streaming pipeline\n",
    "\n",
    "- Create a `TestPipeline`.\n",
    "- Use the `TestStream` class to generate streaming data. This includes generating a series of events, advancing the watermark, and advancing the processing time.\n",
    "- Use the `assert_that` method from the `testing.util` module and its other methods to verify that the output `PCollection` contains the elements that you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b2583-c2ec-4f01-97d7-0d4f8c1b7a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxi_streaming_pipeline.py\n",
    "import json\n",
    "import typing\n",
    "import logging\n",
    "import apache_beam as beam\n",
    "from apache_beam.transforms.trigger import AccumulationMode, AfterCount, AfterWatermark\n",
    "from apache_beam.transforms.combiners import CountCombineFn\n",
    "import argparse\n",
    "\n",
    "class TaxiRide(typing.NamedTuple):\n",
    "    ride_id: str\n",
    "    point_idx: int\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "    timestamp: str\n",
    "    meter_reading: float\n",
    "    meter_increment: float\n",
    "    ride_status: str\n",
    "    passenger_count: int\n",
    "\n",
    "beam.coders.registry.register_coder(TaxiRide, beam.coders.RowCoder)\n",
    "\n",
    "class JsonToTaxiRide(beam.DoFn):\n",
    "\n",
    "    def process(self, line):\n",
    "        row = json.loads(line)\n",
    "        yield TaxiRide(**row)\n",
    "\n",
    "class ConvertCountToDict(beam.DoFn):\n",
    "\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        window_start = window.start.to_utc_datetime().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        output = {\"taxi_rides\" : element, \"timestamp\": window_start}\n",
    "        yield output\n",
    "\n",
    "\n",
    "class TaxiCountTransform(beam.PTransform):\n",
    "\n",
    "    def expand(self, pcoll):\n",
    "        \n",
    "        output = (pcoll\n",
    "                    | \"ParseJson\" >> beam.ParDo(JsonToTaxiRide())\n",
    "                    | \"FilterForPickups\" >> beam.Filter(lambda x : x.ride_status == 'pickup')\n",
    "                    | \"WindowByMinute\" >> beam.WindowInto(beam.window.FixedWindows(60),\n",
    "                                              trigger=AfterWatermark(late=AfterCount(1)),\n",
    "                                              allowed_lateness=60,\n",
    "                                              accumulation_mode=AccumulationMode.ACCUMULATING)\n",
    "                    | \"CountPerMinute\" >> beam.CombineGlobally(CountCombineFn()).without_defaults()\n",
    "                 )\n",
    "\n",
    "        return output\n",
    "\n",
    "def run():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Load from Json from Pub/Sub into BigQuery')\n",
    "\n",
    "    parser.add_argument('--table_name', required=True, help='Output BQ table')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    table_name = opts['table_name']\n",
    "\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"taxi_rides\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    p = beam.Pipeline()\n",
    "\n",
    "    (p | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(topic=\"projects/pubsub-public-data/topics/taxirides-realtime\") \n",
    "       | \"TaxiPickupCount\" >> TaxiCountTransform()\n",
    "       | \"ConvertToDict\" >> beam.ParDo(ConvertCountToDict())\n",
    "       | 'WriteAggToBQ' >> beam.io.WriteToBigQuery(\n",
    "                table_name,\n",
    "                schema=table_schema,\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "                )\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120acaf9-4dd9-4fd9-bc2d-665ed4642628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxi_streaming_pipeline_test.py\n",
    "import logging\n",
    "import json\n",
    "import unittest\n",
    "import sys\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "from taxi_streaming_pipeline import *\n",
    "from apache_beam.testing.test_pipeline import TestPipeline\n",
    "from apache_beam.testing.util import BeamAssertException\n",
    "from apache_beam.testing.util import assert_that, equal_to_per_window\n",
    "from apache_beam.testing.test_stream import TestStream\n",
    "from apache_beam.transforms.window import TimestampedValue, IntervalWindow\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "\n",
    "def main(out = sys.stderr, verbosity = 2):\n",
    "    loader = unittest.TestLoader()\n",
    "  \n",
    "    suite = loader.loadTestsFromModule(sys.modules[__name__])\n",
    "    unittest.TextTestRunner(out, verbosity = verbosity).run(suite)\n",
    "\n",
    "\n",
    "class TaxiWindowingTest(unittest.TestCase):\n",
    "\n",
    "    def test_windowing_behavior(self):\n",
    "\n",
    "        options = PipelineOptions()\n",
    "        options.view_as(StandardOptions).streaming = True\n",
    "\n",
    "        with TestPipeline(options=options) as p:\n",
    "\n",
    "            base_json_pickup = \"{\\\"ride_id\\\":\\\"x\\\",\\\"point_idx\\\":1,\\\"latitude\\\":0.0,\\\"longitude\\\":0.0,\" \\\n",
    "                         \"\\\"timestamp\\\":\\\"00:00:00\\\",\\\"meter_reading\\\":1.0,\\\"meter_increment\\\":0.1,\" \\\n",
    "                         \"\\\"ride_status\\\":\\\"pickup\\\",\\\"passenger_count\\\":1}\" \n",
    "\n",
    "            base_json_enroute = \"{\\\"ride_id\\\":\\\"x\\\",\\\"point_idx\\\":1,\\\"latitude\\\":0.0,\\\"longitude\\\":0.0,\" \\\n",
    "                         \"\\\"timestamp\\\":\\\"00:00:00\\\",\\\"meter_reading\\\":1.0,\\\"meter_increment\\\":0.1,\" \\\n",
    "                         \"\\\"ride_status\\\":\\\"pickup\\\",\\\"passenger_count\\\":1}\" \n",
    "            \n",
    "\n",
    "            test_stream = TestStream().advance_watermark_to(0).add_elements([\n",
    "                TimestampedValue(base_json_pickup, 0),\n",
    "                TimestampedValue(base_json_pickup, 0),\n",
    "                TimestampedValue(base_json_enroute, 0),\n",
    "                TimestampedValue(base_json_pickup, 60)\n",
    "            ]).advance_watermark_to(60).advance_processing_time(60).add_elements([\n",
    "                TimestampedValue(base_json_pickup, 120)\n",
    "            ]).advance_watermark_to_infinity()\n",
    "\n",
    "            taxi_counts = (p | test_stream\n",
    "                             | TaxiCountTransform()\n",
    "                          )\n",
    "\n",
    "            EXPECTED_WINDOW_COUNTS = {IntervalWindow(0,60): [3],\n",
    "                                      IntervalWindow(60,120): [1],\n",
    "                                      IntervalWindow(120,180): [1]}\n",
    "\n",
    "            assert_that(taxi_counts, equal_to_per_window(EXPECTED_WINDOW_COUNTS),\n",
    "                        reify_windows=True)\n",
    "\n",
    "class TaxiLateDataTest(unittest.TestCase):\n",
    "\n",
    "        def test_late_data_behavior(self):\n",
    "\n",
    "            options = PipelineOptions()\n",
    "            options.view_as(StandardOptions).streaming = True\n",
    "\n",
    "            with TestPipeline(options=options) as p:\n",
    "\n",
    "                base_json_pickup = \"{\\\"ride_id\\\":\\\"x\\\",\\\"point_idx\\\":1,\\\"latitude\\\":0.0,\\\"longitude\\\":0.0,\" \\\n",
    "                            \"\\\"timestamp\\\":\\\"00:00:00\\\",\\\"meter_reading\\\":1.0,\\\"meter_increment\\\":0.1,\" \\\n",
    "                            \"\\\"ride_status\\\":\\\"pickup\\\",\\\"passenger_count\\\":1}\" \n",
    "\n",
    "                test_stream = TestStream().advance_watermark_to(0).add_elements([\n",
    "                    TimestampedValue(base_json_pickup, 0),\n",
    "                    TimestampedValue(base_json_pickup, 0),\n",
    "                ]).advance_watermark_to(60).advance_processing_time(60).add_elements([\n",
    "                    TimestampedValue(base_json_pickup, 0)\n",
    "                ]).advance_watermark_to(300).advance_processing_time(240).add_elements([\n",
    "                    TimestampedValue(base_json_pickup, 0)\n",
    "                ])\n",
    "\n",
    "                EXPECTED_RESULTS = {IntervalWindow(0,60): [2,3]}  #On Time and Late Result\n",
    "\n",
    "                taxi_counts_late = (p | test_stream\n",
    "                                      | TaxiCountTransform()\n",
    "                                   )\n",
    "\n",
    "                assert_that(taxi_counts_late, equal_to_per_window(EXPECTED_RESULTS),\n",
    "                            reify_windows=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('testing.out', 'w') as f:\n",
    "        main(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221327fe-64ae-4830-9b51-0021961a07fc",
   "metadata": {},
   "source": [
    "### The CI/CD pipeline\n",
    "\n",
    "![CI CD pipeline](./img/CI_CD_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4512a85-0272-4bf7-ac18-a534b66a26e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Cloud Composer environment\n",
    "gcloud composer environments create $COMPOSER_ENV_NAME \\\n",
    "--location $COMPOSER_REGION \\\n",
    "--zone $COMPOSER_ZONE_ID \\\n",
    "--machine-type n1-standard-1 \\\n",
    "--node-count 3 \\\n",
    "--disk-size 20 \\\n",
    "--python-version 3\n",
    "\n",
    "# Cloud Composer environment variable\n",
    "export COMPOSER_DAG_BUCKET=$(gcloud composer environments \\\n",
    "    describe $COMPOSER_ENV_NAME \\\n",
    "    --location $COMPOSER_REGION \\\n",
    "    --format=\"get(config.dagGcsPrefix)\")\n",
    "\n",
    "# Service account\n",
    "export COMPOSER_SERVICE_ACCOUNT=$(gcloud composer environments \\\n",
    "    describe $COMPOSER_ENV_NAME \\\n",
    "    --location $COMPOSER_REGION \\\n",
    "    --format=\"get(config.nodeConfig.serviceAccount)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c64e4d-ea2a-4646-b253-d7f12c21db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Source Repositories\n",
    "gcloud source repos create $SOURCE_CODE_REPO\n",
    "cp -r ~/ci-cd-for-data-processing-workflow/source-code ~/$SOURCE_CODE_REPO\n",
    "cd ~/$SOURCE_CODE_REPO\n",
    "git config --global credential.'https://source.developers.google.com'.helper gcloud.sh\n",
    "git config --global user.email $(gcloud config list --format 'value(core.account)')\n",
    "git config --global user.name $(gcloud config list --format 'value(core.account)')\n",
    "git init\n",
    "git remote add google \\\n",
    "    https://source.developers.google.com/p/$GCP_PROJECT_ID/r/$SOURCE_CODE_REPO\n",
    "git add .\n",
    "git commit -m 'initial commit'\n",
    "git push google master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a0c4d6-00e4-450b-854f-ca76721eef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Build pipeline\n",
    "cd ~/ci-cd-for-data-processing-workflow/source-code/build-pipeline\n",
    "gcloud builds submit --config=build_deploy_test.yaml --substitutions=\\\n",
    "REPO_NAME=$SOURCE_CODE_REPO,\\\n",
    "_DATAFLOW_JAR_BUCKET=$DATAFLOW_JAR_BUCKET_TEST,\\\n",
    "_COMPOSER_INPUT_BUCKET=$INPUT_BUCKET_TEST,\\\n",
    "_COMPOSER_REF_BUCKET=$REF_BUCKET_TEST,\\\n",
    "_COMPOSER_DAG_BUCKET=$COMPOSER_DAG_BUCKET,\\\n",
    "_COMPOSER_ENV_NAME=$COMPOSER_ENV_NAME,\\\n",
    "_COMPOSER_REGION=$COMPOSER_REGION,\\\n",
    "_COMPOSER_DAG_NAME_TEST=$COMPOSER_DAG_NAME_TEST\n",
    "\n",
    "# Get the URL to Cloud Composer web interface\n",
    "gcloud composer environments describe $COMPOSER_ENV_NAME \\\n",
    "--location $COMPOSER_REGION \\\n",
    "--format=\"get(config.airflowUri)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06734420-26df-42d0-821b-82fb276e11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Composer variable for the JAR filename\n",
    "export DATAFLOW_JAR_FILE_LATEST=$(gcloud composer environments run $COMPOSER_ENV_NAME \\\n",
    "--location $COMPOSER_REGION variables -- \\\n",
    "--get dataflow_jar_file_test 2>&1 | grep -i '.jar')\n",
    "\n",
    "# Build pipeline configuration file\n",
    "cd ~/ci-cd-for-data-processing-workflow/source-code/build-pipeline\n",
    "gcloud builds submit --config=deploy_prod.yaml --substitutions=\\\n",
    "REPO_NAME=$SOURCE_CODE_REPO,\\\n",
    "_DATAFLOW_JAR_BUCKET_TEST=$DATAFLOW_JAR_BUCKET_TEST,\\\n",
    "_DATAFLOW_JAR_FILE_LATEST=$DATAFLOW_JAR_FILE_LATEST,\\\n",
    "_DATAFLOW_JAR_BUCKET_PROD=$DATAFLOW_JAR_BUCKET_PROD,\\\n",
    "_COMPOSER_INPUT_BUCKET=$INPUT_BUCKET_PROD,\\\n",
    "_COMPOSER_ENV_NAME=$COMPOSER_ENV_NAME,\\\n",
    "_COMPOSER_REGION=$COMPOSER_REGION,\\\n",
    "_COMPOSER_DAG_BUCKET=$COMPOSER_DAG_BUCKET,\\\n",
    "_COMPOSER_DAG_NAME_PROD=$COMPOSER_DAG_NAME_PROD\n",
    "\n",
    "# Get the URL for Cloud Composer UI\n",
    "gcloud composer environments describe $COMPOSER_ENV_NAME \\\n",
    "--location $COMPOSER_REGION \\\n",
    "--format=\"get(config.airflowUri)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc60bc5-4a94-4b8a-a889-2b2464b336d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Flex Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e385fb-883e-457c-be51-11b28aa5e974",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a custom Dataflow Flex Template container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4311e6-db5c-4533-86d5-12435d36c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_pipeline.py\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### functions and classes\n",
    "\n",
    "def parse_json(element):\n",
    "    return json.loads(element)\n",
    "\n",
    "def drop_fields(element):\n",
    "    element.pop('user_agent')\n",
    "    return element\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "    parser.add_argument('--inputPath', required=True, help='Path to events.json')\n",
    "    parser.add_argument('--outputPath', required=True, help='Path to coldline storage bucket')\n",
    "    parser.add_argument('--tableName', required=True, help='BigQuery table name')\n",
    "\n",
    "    opts, pipeline_opts = parser.parse_known_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(pipeline_opts)\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('my-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    input_path = opts.inputPath\n",
    "    output_path = opts.outputPath\n",
    "    table_name = opts.tableName\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"ip\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"user_id\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lat\",\n",
    "                \"type\": \"FLOAT\",\n",
    "                \"mode\": \"NULLABLE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lng\",\n",
    "                \"type\": \"FLOAT\",\n",
    "                \"mode\": \"NULLABLE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_request\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_response\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"num_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    '''\n",
    "    Steps:\n",
    "    1) Read something\n",
    "    2) Transform something\n",
    "    3) Write something\n",
    "    '''\n",
    "\n",
    "    # Read in lines to an initial PCollection that can then be branched off of\n",
    "    lines = p | 'ReadFromGCS' >> beam.io.ReadFromText(input_path)\n",
    "\n",
    "    # Write to Google Cloud Storage\n",
    "    lines | 'WriteRawToGCS' >> beam.io.WriteToText(output_path)\n",
    "\n",
    "    # Read elements from Json, filter out individual elements, and write to BigQuery\n",
    "    (lines\n",
    "        | 'ParseJson' >> beam.Map(parse_json)\n",
    "        | 'DropFields' >> beam.Map(drop_fields)\n",
    "        | 'FilterFn' >> beam.Filter(lambda row: row['num_bytes'] < 120)\n",
    "        | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n",
    "            table_name,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36fce7-2bbf-4229-9656-1b2bc9331ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dockerfile\n",
    "FROM gcr.io/dataflow-templates-base/python3-template-launcher-base\n",
    "ARG WORKDIR=/dataflow/template\n",
    "RUN mkdir -p ${WORKDIR}\n",
    "WORKDIR ${WORKDIR}\n",
    "RUN apt-get update && apt-get install -y libffi-dev && rm -rf /var/lib/apt/lists/*\n",
    "COPY my_pipeline.py .\n",
    "ENV FLEX_TEMPLATE_PYTHON_PY_FILE=\"${WORKDIR}/my_pipeline.py\"\n",
    "RUN python3 -m pip install apache-beam[gcp]==2.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a40baf-3bba-42de-9297-82479c35be95",
   "metadata": {},
   "source": [
    "First, enable Kaniko cache use by default. Kaniko caches container build artifacts, so using this option speeds up subsequent builds. We will also use `pip3 freeze` to record the packages and their versions being used in our environment.\n",
    "\n",
    "Cloud Build to build the container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d8a55-85a1-4694-8390-d3bec8c7f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud config set builds/use_kaniko True\n",
    "\n",
    "export TEMPLATE_IMAGE=\"gcr.io/$PROJECT_ID/dataflow/my_pipeline:latest\"\n",
    "gcloud builds submit --tag $TEMPLATE_IMAGE ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090b0c5-ec47-47a7-baf0-30cdd8de786f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create and stage the flex template\n",
    "\n",
    "Create a template spec file in a Cloud Storage containing all of the necessary information to run the job, such as the SDK information and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedbaa91-0df0-446c-b942-42d36f401734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata.json\n",
    "{\n",
    "  \"name\": \"My Branching Pipeline\",\n",
    "  \"description\": \"A branching pipeline that writes raw to GCS Coldline, and filtered data to BQ\",\n",
    "  \"parameters\": [\n",
    "    {\n",
    "      \"name\": \"inputPath\",\n",
    "      \"label\": \"Input file path.\",\n",
    "      \"helpText\": \"Path to events.json file.\",\n",
    "      \"regexes\": [\n",
    "        \".*\\\\.json\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"outputPath\",\n",
    "      \"label\": \"Output file location\",\n",
    "      \"helpText\": \"GCS Coldline Bucket location for raw data\",\n",
    "      \"regexes\": [\n",
    "        \"gs:\\\\/\\\\/[a-zA-z0-9\\\\-\\\\_\\\\/]+\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"tableName\",\n",
    "      \"label\": \"BigQuery output table\",\n",
    "      \"helpText\": \"BigQuery table spec to write to, in the form 'project:dataset.table'.\",\n",
    "      \"regexes\": [\n",
    "        \"[^:]+:[^.]+[.].+\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673bfa8-0f8b-47bc-8e81-b6d2d3c8c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "export TEMPLATE_PATH=\"gs://${PROJECT_ID}/templates/mytemplate.json\"\n",
    "\n",
    "gcloud dataflow flex-template build $TEMPLATE_PATH \\\n",
    "--image=$TEMPLATE_IMAGE \\\n",
    "--sdk-language=\"PYTHON\" \\\n",
    "--metadata-file=metadata.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6535ec-9069-4ef7-83fb-92821f162136",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Execute the flex template\n",
    "\n",
    "One of the benefits of using Dataflow templates is the ability to execute them from a wider variety of contexts, other than a development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b5596-9f0d-4e7d-84f9-a62be5f5d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcloud\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "export REGION='us-central1'\n",
    "export JOB_NAME=mytemplate-$(date +%Y%m%d-%H%M%S)\n",
    "export TEMPLATE_LOC=gs://${PROJECT_ID}/templates/mytemplate.json\n",
    "export INPUT_PATH=gs://${PROJECT_ID}/events.json\n",
    "export OUTPUT_PATH=gs://${PROJECT_ID}-coldline/template_output/\n",
    "export BQ_TABLE=${PROJECT_ID}:logs.logs_filtered\n",
    "gcloud dataflow flex-template run ${JOB_NAME} \\\n",
    "--region=$REGION \\\n",
    "--template-file-gcs-location ${TEMPLATE_LOC} \\\n",
    "--parameters \"inputPath=${INPUT_PATH},outputPath=${OUTPUT_PATH},tableName=${BQ_TABLE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f7d26-f110-47bd-b004-63fcb811da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcloud\n",
    "gcloud dataflow flex-template run \"job-name-`date +%Y%m%d-%H%M%S`\" \\\n",
    "--template-file-gcs-location \"$TEMPLATE_PATH\" \\\n",
    "--parameters inputSubscription=\"$SUBSCRIPTION\" \\\n",
    "--parameters outputTable=\"$PROJECT:$DATASET.$TABLE\" \\\n",
    "--region=\"$REGION\"\n",
    "\n",
    "# REST API\n",
    "curl -X POST \"https://dataflow.googleapis.com/v1b3/projects/$PROJECT/locations/${REGION}/flexTemplates:launch\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-d '{\n",
    "  \"launch_parameter\": {\n",
    "    \"jobName\": \"job-name-`date +%Y%m%d-%H%M%S`\",\n",
    "    \"parameters\": {\n",
    "      \"inputSubscription\": \"'$SUBSCRIPTION'\",\n",
    "      \"outputTable\": \"'$PROJECT:$DATASET.$TABLE'\"\n",
    "    },\n",
    "    \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\"\n",
    "  }\n",
    "}'\n",
    "\n",
    "# Cloud Scheduler\n",
    "gcloud scheduler jobs create http scheduler-job \\\n",
    "--schedule=\"*/30 * * * *\" \\\n",
    "--uri=\"https://dataflow.googleapis.com/v1b3/projects/$PROJECT/locations/${REGION}/flexTemplates:launch\" \\\n",
    "--http-method=POST \\\n",
    "--headers Content-Type=application/json \\\n",
    "--oauth-service-account-email=email@project.iam.gserviceaccount.com \\\n",
    "--message-body='{\n",
    "    \"launch_parameter\": {\n",
    "      \"jobName\":\"job-name\"\n",
    "      \"parameters\": {\n",
    "        \"inputSubscription\": \"'$SUBSCRIPTION'\",\n",
    "        \"outputTable\": \"'$PROJECT:$DATASET.$TABLE'\"\n",
    "      },\n",
    "      \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\"\n",
    "    }\n",
    "}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
