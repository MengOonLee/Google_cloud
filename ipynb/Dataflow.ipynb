{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32ad7f9-8e83-4a99-8b1f-1e12fac4ab86",
   "metadata": {},
   "source": [
    "# Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5869c-abcd-4490-bb74-b238b9f68550",
   "metadata": {},
   "source": [
    "## Setup IAM and networking for Dataflow jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7bba0c-66e8-4dce-b4f4-e5ddf0e20847",
   "metadata": {},
   "source": [
    "### Create a Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b938b856-4151-4e68-bd14-e7c329868d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud auth list\n",
    "gcloud config list project\n",
    "\n",
    "\n",
    "PROJECT=`gcloud config list --format 'value(core.project)'`\n",
    "USER_EMAIL=`gcloud config list account --format 'value(core.account)'`\n",
    "REGION=us-central1\n",
    "\n",
    "gsutil mb -p $PROJECT -b on gs://$PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b431a2a-566c-4244-a8b9-dae6b78321ce",
   "metadata": {},
   "source": [
    "### Create a virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b02053-ac8d-4080-9237-896836e59581",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install -y python3-venv\n",
    "python3 -m venv df-env\n",
    "source df-env/bin/activate\n",
    "\n",
    "python3 -m pip install -q --upgrade pip setuptools wheel\n",
    "python3 -m pip install apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7925c-64e4-40ef-9465-8ce02ae08c5e",
   "metadata": {},
   "source": [
    "### Launch a Dataflow job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7427e-7669-4509-bdfd-150a10998ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud projects get-iam-policy $PROJECT \\\n",
    "--format='table(bindings.role)' \\\n",
    "--flatten='bindings[].members' \\\n",
    "--filter='bindings.members:$USER_EMAIL'\n",
    "\n",
    "\n",
    "gcloud projects add-iam-policy-binding $PROJECT \\\n",
    "--member=user:$USER_EMAIL \\\n",
    "--role=roles/dataflow.admin\n",
    "\n",
    "\n",
    "python3 -m apache_beam.examples.wordcount \\\n",
    "--input=gs://dataflow-samples/shakespeare/kinglear.txt \\\n",
    "--output=gs://$PROJECT/results/outputs \\\n",
    "--runner=DataflowRunner \\\n",
    "--project=$PROJECT \\\n",
    "--temp_location=gs://$PROJECT/tmp/ \\\n",
    "--region=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c292c5-86c8-44a8-974c-001a3bd322a3",
   "metadata": {},
   "source": [
    "### Launch in private IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56927f-b2a8-48df-9d0d-348c98082b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud projects add-iam-policy-binding $PROJECT \\\n",
    "--member=user:$USER_EMAIL \\\n",
    "--role=roles/compute.networkAdmin\n",
    "\n",
    "\n",
    "gcloud compute networks subnets update default \\\n",
    "--region=$REGION \\\n",
    "--enable-private-ip-google-access\n",
    "\n",
    "\n",
    "python3 -m apache_beam.examples.wordcount \\\n",
    "--input=gs://dataflow-samples/shakespeare/kinglear.txt \\\n",
    "--output=gs://$PROJECT/results/outputs \\\n",
    "--runner=DataflowRunner \\\n",
    "--project=$PROJECT \\\n",
    "--temp_location=gs://$PROJECT/tmp/ \\\n",
    "--region=$REGION \\\n",
    "--no_use_public_ips \\\n",
    "--network default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a67ee61-76c8-433f-a3d0-1ea4d3bce3da",
   "metadata": {},
   "source": [
    "## Extract-Transform-Load pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61722054-6608-46e2-9c09-55c14af32967",
   "metadata": {},
   "source": [
    "### Setting up virtual environment and dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ceb33d-31b5-460d-81be-2bcd50e741fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n",
    "cd ~/training-data-analyst/quests/dataflow_python/\n",
    "\n",
    "# Change directory into the lab\n",
    "cd 1_Basic_ETL/lab\n",
    "export BASE_DIR=$(pwd)\n",
    "\n",
    "sudo apt-get install -y python3-venv\n",
    "python3 -m venv df-env\n",
    "source df-env/bin/activate\n",
    "\n",
    "python3 -m pip install -q --upgrade pip setuptools wheel\n",
    "python3 -m pip install apache-beam[gcp]\n",
    "\n",
    "# Dataflow API is enabled.\n",
    "gcloud services enable dataflow.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b8274-551e-4513-bfcf-28bee264b768",
   "metadata": {},
   "source": [
    "### First pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249b4c39-c54c-4e36-848f-07fb7c96455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the directory containing the relevant code\n",
    "cd $BASE_DIR/../..\n",
    "# Create GCS buckets and BQ dataset\n",
    "source create_batch_sinks.sh\n",
    "# Run a script to generate a batch of web server log events\n",
    "bash generate_batch_events.sh\n",
    "# Examine some sample events\n",
    "head events.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8e595-8190-4d61-84ab-29a4884c3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile 1_Basic_ETL/lab/my_pipeline.py\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--stagingLocation', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--tempLocation', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions()\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).staging_location = opts.stagingLocation\n",
    "    options.view_as(GoogleCloudOptions).temp_location = opts.tempLocation\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('my-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    # Static input and output\n",
    "    input = 'gs://{0}/events.json'.format(opts.project)\n",
    "    output = '{0}:logs.logs'.format(opts.project)\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"ip\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"user_id\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lat\",\n",
    "                \"type\": \"FLOAT\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lng\",\n",
    "                \"type\": \"FLOAT\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_request\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_response\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"num_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"user_agent\",\n",
    "                \"type\": \"STRING\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    '''\n",
    "    Steps:\n",
    "    1) Read something\n",
    "    2) Transform something\n",
    "    3) Write something\n",
    "    '''\n",
    "\n",
    "    (p\n",
    "        | 'ReadFromGCS' >> beam.io.ReadFromText(input)\n",
    "        | 'ParseJson' >> beam.Map(lambda line: json.loads(line))\n",
    "        | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n",
    "            output,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b4cde-39fe-40cc-b23c-8a1b4c31e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $BASE_DIR\n",
    "# Set up environment variables\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "# Run the pipeline\n",
    "python3 my_pipeline.py \\\n",
    "  --project=${PROJECT_ID} \\\n",
    "  --region=us-central1 \\\n",
    "  --stagingLocation=gs://$PROJECT_ID/staging/ \\\n",
    "  --tempLocation=gs://$PROJECT_ID/temp/ \\\n",
    "  --runner=DataflowRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7b77a-0864-4811-a752-5526fd483eba",
   "metadata": {},
   "source": [
    "## Beam Notebooks\n",
    "\n",
    "Include the `interactive_runner` and `interactive_beam` modules in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f69a9-d187-4c72-9ad7-ca8ac0f73686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f5229-f4e5-4acb-a751-ba405f698dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the recording duration to 10 min\n",
    "ib.options.recording_duration = '10m'\n",
    "\n",
    "# Set the recording size limit to 1 GB\n",
    "ib.options.recording_size_limit = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72626746-7a6f-48de-8d60-2c6ab56c0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = p | \"read\" >> beam.io.ReadFromPubSub(topic=topic)\n",
    "\n",
    "windowed_words = (words | \"window\" >> beam.WindowInto(beam.window.FixedWindows(10)))\n",
    "\n",
    "windowed_words_counts = (windowed_words | \"count\" >> beam.combiners.Count.PerElement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fc1f4-3575-44e4-bd9f-4b3b01437d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materializes the resulting PCollection in a table\n",
    "ib.show(windowed_word_counts, include_window_info=True)\n",
    "\n",
    "# Load the output in a Pandas DataFrame\n",
    "ib.collect(windowed_word_counts, include_window_info=True)\n",
    "\n",
    "# Visualize the data in the Notebook\n",
    "ib.show(windowed_word_counts, include_window_info=True, visualize_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc019c-fdc5-476a-968e-bb55459dae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the production Dataflow runner\n",
    "from apache_beam.runners import DataflowRunner\n",
    "\n",
    "# Set up Apache Beam pipeline options\n",
    "options = pipeline_options.PipelineOptions()\n",
    "\n",
    "# Run the pipeline\n",
    "runner = DataflowRunner()\n",
    "runner.run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb32f53-bc0b-415a-9ce7-5e5444d172e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "storeSales = p | beam.io.ReadFromText(\"purchases-store\")\n",
    "               | beam.Map(lambda s: ...)\n",
    "\n",
    "onlineSales = p | beam.io.ReadFromText(\"purchase-online\")\n",
    "                | beam.Map(lambda s: ...)\n",
    "    \n",
    "topSales = (storeSales, onlineSales)\n",
    "                | beam.Flatten()\n",
    "                | beam.Combiners.Count.perKey()\n",
    "                | beam.Combiners.Top.of(10, key=lambda x: x[1])\n",
    "            \n",
    "topSales        | beam.io.WriteToBigQuery(topSales)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
