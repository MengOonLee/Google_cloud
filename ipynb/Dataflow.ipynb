{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32ad7f9-8e83-4a99-8b1f-1e12fac4ab86",
   "metadata": {},
   "source": [
    "# Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5869c-abcd-4490-bb74-b238b9f68550",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup IAM and networking for Dataflow jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7bba0c-66e8-4dce-b4f4-e5ddf0e20847",
   "metadata": {},
   "source": [
    "### Create a Cloud Storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b938b856-4151-4e68-bd14-e7c329868d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud auth list\n",
    "gcloud config list project\n",
    "\n",
    "PROJECT=`gcloud config list --format 'value(core.project)'`\n",
    "USER_EMAIL=`gcloud config list account --format 'value(core.account)'`\n",
    "REGION=us-central1\n",
    "gsutil mb -p $PROJECT -b on gs://$PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b431a2a-566c-4244-a8b9-dae6b78321ce",
   "metadata": {},
   "source": [
    "### Create a virtual environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b02053-ac8d-4080-9237-896836e59581",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create and activate virtual environment\n",
    "sudo apt-get install -y python3-venv\n",
    "python3 -m venv df-env\n",
    "source df-env/bin/activate\n",
    "\n",
    "python3 -m pip install -q --upgrade pip setuptools wheel\n",
    "python3 -m pip install apache-beam[gcp]\n",
    "\n",
    "# Dataflow API is enabled.\n",
    "gcloud services enable dataflow.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7925c-64e4-40ef-9465-8ce02ae08c5e",
   "metadata": {},
   "source": [
    "### Launch a Dataflow job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee7427e-7669-4509-bdfd-150a10998ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud projects get-iam-policy $PROJECT \\\n",
    "--format='table(bindings.role)' \\\n",
    "--flatten='bindings[].members' \\\n",
    "--filter='bindings.members:$USER_EMAIL'\n",
    "\n",
    "gcloud projects add-iam-policy-binding $PROJECT \\\n",
    "--member=user:$USER_EMAIL \\\n",
    "--role=roles/dataflow.admin\n",
    "\n",
    "python3 -m apache_beam.examples.wordcount \\\n",
    "--input=gs://dataflow-samples/shakespeare/kinglear.txt \\\n",
    "--output=gs://$PROJECT/results/outputs \\\n",
    "--runner=DataflowRunner \\\n",
    "--project=$PROJECT \\\n",
    "--temp_location=gs://$PROJECT/tmp/ \\\n",
    "--region=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c292c5-86c8-44a8-974c-001a3bd322a3",
   "metadata": {},
   "source": [
    "### Launch in private IPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56927f-b2a8-48df-9d0d-348c98082b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud projects add-iam-policy-binding $PROJECT \\\n",
    "--member=user:$USER_EMAIL \\\n",
    "--role=roles/compute.networkAdmin\n",
    "\n",
    "\n",
    "gcloud compute networks subnets update default \\\n",
    "--region=$REGION \\\n",
    "--enable-private-ip-google-access\n",
    "\n",
    "\n",
    "python3 -m apache_beam.examples.wordcount \\\n",
    "--input=gs://dataflow-samples/shakespeare/kinglear.txt \\\n",
    "--output=gs://$PROJECT/results/outputs \\\n",
    "--runner=DataflowRunner \\\n",
    "--project=$PROJECT \\\n",
    "--temp_location=gs://$PROJECT/tmp/ \\\n",
    "--region=$REGION \\\n",
    "--no_use_public_ips \\\n",
    "--network default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a67ee61-76c8-433f-a3d0-1ea4d3bce3da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Extract-Transform-Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b8274-551e-4513-bfcf-28bee264b768",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8e595-8190-4d61-84ab-29a4884c3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--stagingLocation', required=True, help='Specify Cloud Storage bucket for staging')\n",
    "    parser.add_argument('--tempLocation', required=True, help='Specify Cloud Storage bucket for temp')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions()\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).staging_location = opts.stagingLocation\n",
    "    options.view_as(GoogleCloudOptions).temp_location = opts.tempLocation\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('my-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    # Static input and output\n",
    "    input = 'gs://{0}/events.json'.format(opts.project)\n",
    "    output = '{0}:logs.logs'.format(opts.project)\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"ip\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"user_id\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lat\",\n",
    "                \"type\": \"FLOAT\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lng\",\n",
    "                \"type\": \"FLOAT\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_request\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_response\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"num_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"user_agent\",\n",
    "                \"type\": \"STRING\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    '''\n",
    "    Steps:\n",
    "    1) Read something\n",
    "    2) Transform something\n",
    "    3) Write something\n",
    "    '''\n",
    "\n",
    "    (p\n",
    "        | 'ReadFromGCS' >> beam.io.ReadFromText(input)\n",
    "        | 'ParseJson' >> beam.Map(lambda line: json.loads(line))\n",
    "        | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n",
    "            output,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b4cde-39fe-40cc-b23c-8a1b4c31e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment variables\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "# Run the pipeline\n",
    "python3 my_pipeline.py \\\n",
    "--project=${PROJECT_ID} \\\n",
    "--region=us-central1 \\\n",
    "--stagingLocation=gs://$PROJECT_ID/staging/ \\\n",
    "--tempLocation=gs://$PROJECT_ID/temp/ \\\n",
    "--runner=DataflowRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249a6227-734f-47a0-8700-ce22a628d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a full JSON object\n",
    "bq show --schema --format=prettyjson logs.logs | sed '1s/^/{\"BigQuery Schema\":/' | sed '$s/$/}/' > schema.json\n",
    "cat schema.json\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "gsutil cp schema.json gs://${PROJECT_ID}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98a31e-b3e6-4191-9df8-be3b3c167577",
   "metadata": {},
   "source": [
    "## Sources and Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98757a76-5936-4e48-9796-bfc0d20adf17",
   "metadata": {},
   "source": [
    "### Text IO & File IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b98297f-5f44-465d-99f3-ac3b4d901c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text IO reading\n",
    "pcoll = (pipeline\n",
    "    | 'Create' >> Create([file_name])\n",
    "    | 'ReadAll' >> ReadAllFromText()\n",
    ")\n",
    "\n",
    "pcoll = pipeline | 'Read' >> ReadFromText(file_name)\n",
    "\n",
    "# File IO reading with filenames\n",
    "with beam.Pipeline() as p:\n",
    "    readable_files = (p\n",
    "        | fileio.MatchFiles('hdfs://path/to/*.txt') # Match file patter\n",
    "        | fileio.ReadMatches()\n",
    "        | beam.Reshuffle()\n",
    "    )\n",
    "    file_and_contents = (readable_files\n",
    "        | beam.Map(lambda x: (x.metadata.path, x.read_utf8())) # Access file metadata\n",
    "    )\n",
    "    \n",
    "# File IO processing files as they arrive\n",
    "with beam.Pipeline() as p:\n",
    "    readable_files = (p\n",
    "        | beam.io.ReadFromPubSub(...) # Parse PubSub message and yield filename\n",
    "    )\n",
    "    files_and_contents = (readable_files\n",
    "        | ReadAllFromText() # Used parsed filename to read \n",
    "    )\n",
    "    \n",
    "# Text IO writing\n",
    "transformed_data | \"write\" >> WriteToText(know_args.output, coder=JsonCoder())\n",
    "\n",
    "# Text IO writing with dynamic destinations\n",
    "pcoll | beam.io.fileio.WriteToFiles(\n",
    "    path='/path',\n",
    "    destination=lambda record: \n",
    "        'avro' if record['type']=='A' else 'csv', # Dynamic destination\n",
    "    sink=lambda dest: \n",
    "        AvroSink() if dest=='avro' else CsvSink(), # Write dynamic sink\n",
    "        file_naming=beam.io.fileio.destination_prefix_naming()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f865f-1901-4d58-9a07-7a690202aa1a",
   "metadata": {},
   "source": [
    "### BigQuery IO with BigQuery Storage API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6b44b-1969-4ee7-bba1-2fed464ea172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery IO reading with query\n",
    "pcoll = (p\n",
    "    | 'QueryTableStdSQL' >> beam.io.ReadFromBigQuery(\n",
    "        query='SELECT max_temperature '\\\n",
    "            'FROM `project.dataset.table`',\n",
    "        use_standard_sql=True\n",
    "    ) # Map results\n",
    "    | beam.Map(lambda elem: elem['max_temperature']) # Source using query\n",
    ")\n",
    "\n",
    "# BigQuery IO writing with dynamic destinations\n",
    "def table_fn(element, fictional_characters):\n",
    "    if element in fictional_characters:\n",
    "        return 'dataset.fictional_quotes'\n",
    "    else:\n",
    "        return 'dataset.real_quotes'\n",
    "    \n",
    "quotes | \"WriteWithDynamicDestination\" >> beam.io.WriteToBigQuery(\n",
    "    table_fn,\n",
    "    schema=table_schema, # Schema destination\n",
    "    table_side_inputs=(fictional_characters_view)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17db1b-0b17-4b3b-9626-aa092abc4206",
   "metadata": {},
   "source": [
    "## PubSub IO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff97d5f6-b195-4fd5-8133-8053a1486e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PubSub IO reading\n",
    "class GroupWindowsIntoBatches(beam.PTransform):\n",
    "    return (pcoll\n",
    "        | beam.WindowInto(window.FixedWindows(self.window_size))\n",
    "    )\n",
    "\n",
    "pipeline \n",
    "    | \"Read PubSub Message\" >> beam.io.ReadFromPubSub(topic=input_topic)\n",
    "    | \"Window into\" >> GroupWindowIntoBatches(window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ee87e-d6a2-4d43-9a71-2d8d197c3429",
   "metadata": {},
   "source": [
    "## Kafka IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65a49e-d48c-4147-ab50-ddbb6dca60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka IO reading\n",
    "pipeline\n",
    "    | ReadFromKafka(\n",
    "        consumer_config={'bootstrap.servers': bootstrap_servers},\n",
    "        topic=[topic]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c3543-0b59-4368-8990-8a22b9b4009b",
   "metadata": {},
   "source": [
    "## Avro IO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5038bb-e8de-4c2e-880c-66dea0c5a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avro IO reading multiple files\n",
    "with beam.Pipeline() as p:\n",
    "    records = p | \"Read\" >> beam.io.ReadFromAvro('/avrofiles*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a9678-bb48-42e2-8bea-0317655076c4",
   "metadata": {},
   "source": [
    "## Splittable DoFn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68030d1-0e66-4f35-ab14-3abf49090174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splittable DoFn custome source\n",
    "class FileToWordsRestrictionProvider(beam.io.RestrictionProvider):\n",
    "    def initial_restriction(self, file_name): # Initial restriction\n",
    "        return OffsetRange(0, os.stat(file_name).st_size)\n",
    "    \n",
    "    # Tracking subset of restriction completed\n",
    "    def create_tracker(self, restriction):\n",
    "        return beam.io.restriction_trackers.OffsetRestrictionTracker()\n",
    "    \n",
    "class FileToWordsFn(beam.DoFn):\n",
    "    def process(self, ...=FileToWordsRestrictionProvider()):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7b77a-0864-4811-a752-5526fd483eba",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Beam Notebooks\n",
    "\n",
    "Include the `interactive_runner` and `interactive_beam` modules in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023f69a9-d187-4c72-9ad7-ca8ac0f73686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import apache_beam.runners.interactive.interactive_beam as ib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f5229-f4e5-4acb-a751-ba405f698dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the recording duration to 10 min\n",
    "ib.options.recording_duration = '10m'\n",
    "\n",
    "# Set the recording size limit to 1 GB\n",
    "ib.options.recording_size_limit = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72626746-7a6f-48de-8d60-2c6ab56c0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = p | \"read\" >> beam.io.ReadFromPubSub(topic=topic)\n",
    "\n",
    "windowed_words = (words | \"window\" >> beam.WindowInto(beam.window.FixedWindows(10)))\n",
    "\n",
    "windowed_words_counts = (windowed_words | \"count\" >> beam.combiners.Count.PerElement())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fc1f4-3575-44e4-bd9f-4b3b01437d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materializes the resulting PCollection in a table\n",
    "ib.show(windowed_word_counts, include_window_info=True)\n",
    "\n",
    "# Load the output in a Pandas DataFrame\n",
    "ib.collect(windowed_word_counts, include_window_info=True)\n",
    "\n",
    "# Visualize the data in the Notebook\n",
    "ib.show(windowed_word_counts, include_window_info=True, visualize_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc019c-fdc5-476a-968e-bb55459dae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the production Dataflow runner\n",
    "from apache_beam.runners import DataflowRunner\n",
    "\n",
    "# Set up Apache Beam pipeline options\n",
    "options = pipeline_options.PipelineOptions()\n",
    "\n",
    "# Run the pipeline\n",
    "runner = DataflowRunner()\n",
    "runner.run_pipeline(p, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb32f53-bc0b-415a-9ce7-5e5444d172e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "storeSales = p | beam.io.ReadFromText(\"purchases-store\")\n",
    "               | beam.Map(lambda s: ...)\n",
    "\n",
    "onlineSales = p | beam.io.ReadFromText(\"purchase-online\")\n",
    "                | beam.Map(lambda s: ...)\n",
    "    \n",
    "topSales = (storeSales, onlineSales)\n",
    "                | beam.Flatten()\n",
    "                | beam.Combiners.Count.perKey()\n",
    "                | beam.Combiners.Top.of(10, key=lambda x: x[1])\n",
    "            \n",
    "topSales        | beam.io.WriteToBigQuery(topSales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc6c5e-df68-44ce-a70c-c95c5d4334f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Troubleshooting & Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5729b3e9-3735-4faa-8aef-d45b297a3d1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Adding exception handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9bf763-848d-4753-b22b-d4e27cad07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterMessagesFn(beam.DoFn):\n",
    "    BAD_MESSAGE_TAG = 'bad_message'\n",
    "    GOOD_MESSAGE_TAG = 'good_message'\n",
    "\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        try:\n",
    "            data = element.decode()\n",
    "            # tag the element accordingly\n",
    "            if 'bad' in data:\n",
    "                yield pvalue.TaggedOutput(self.BAD_MESSAGE_TAG, element)\n",
    "            else:\n",
    "                yield pvalue.TaggedOutput(self.GOOD_MESSAGE_TAG, element)\n",
    "                \n",
    "        # handle any exceptions in the processing\n",
    "        except Exception as exp:\n",
    "            logging.getLogger.warning(exp)\n",
    "            yield pvalue.TaggedOutput(self.BAD_MESSAGE_TAG, element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0571dd7-1feb-4059-b92a-9124550d8d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_pipeline.py\n",
    "import argparse\n",
    "import logging\n",
    "import argparse, logging, os\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "class ReadGBK(beam.DoFn):\n",
    "    def process(self, e):\n",
    "        k, elems = e\n",
    "        for v in elems:\n",
    "            logging.info(f\"the element is {v}\")\n",
    "            yield v\n",
    "            \n",
    "def run(argv=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--output', dest='output', help='Output file to write results to.')\n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "    read_query = \"\"\"(\n",
    "                 SELECT\n",
    "                   version,\n",
    "                   block_hash,\n",
    "                   block_number\n",
    "                 FROM\n",
    "                   `bigquery-public-data.crypto_bitcoin.transactions`\n",
    "                 WHERE\n",
    "                   version = 1\n",
    "                 LIMIT\n",
    "                   1000000 )\n",
    "               UNION ALL (\n",
    "                 SELECT\n",
    "                   version,\n",
    "                   block_hash,\n",
    "                   block_number\n",
    "                 FROM\n",
    "                   `bigquery-public-data.crypto_bitcoin.transactions`\n",
    "                 WHERE\n",
    "                   version = 2\n",
    "                 LIMIT\n",
    "                   1000 ) ;\"\"\"\n",
    "    \n",
    "    p = beam.Pipeline(options=PipelineOptions(pipeline_args))\n",
    "    (p\n",
    "    | 'Read from BigQuery' >> beam.io.ReadFromBigQuery(\n",
    "        query=read_query, use_standard_sql=True)\n",
    "    | \"Add Hotkey\" >> beam.Map(lambda elem: (elem[\"version\"], elem))\n",
    "    | \"Groupby\" >> beam.GroupByKey()\n",
    "    | 'Print' >>  beam.ParDo(ReadGBK())\n",
    "    | 'Sink' >>  WriteToText(known_args.output))\n",
    "    result = p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger().setLevel(logging.INFO)\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70afa7d7-263f-4112-aea6-f9e95204608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a storage bucket\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "gsutil mb -l US gs://$PROJECT_ID\n",
    "\n",
    "# Attempt to launch the pipeline\n",
    "# Launch the pipeline\n",
    "python3 my_pipeline.py \\\n",
    "  --project=${PROJECT_ID} \\\n",
    "  --region=us-central1 \\\n",
    "  --output=gs://$PROJECT_ID/results/prefix \\\n",
    "  --tempLocation=gs://$PROJECT_ID/temp/ \\\n",
    "  --max_num_workers=5 \\\n",
    "  --runner=DataflowRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f2a831-f9bb-40b8-981d-37a8a893e3c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87468d5-e49e-4aba-a93a-ec7cef31741f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Graph Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e37bd6-9ebc-4ba6-8f8a-a40a51b40fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshuffle after ParDo\n",
    "_ = pcoll | beam.Reshuffle()\n",
    "\n",
    "# Side input\n",
    "_ = pcoll | beam.FlatMap(cross_join, rights=beam.pvalue.AsIter(side_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37bb52a-de2b-493d-b193-0764d313f1b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Disaster Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24e28b3-fe6c-4ab4-a6a9-6e20f0e3cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a snapshot of a subscription\n",
    "gcloud pubsub snapshots create my-snapshot \\\n",
    "--subscription=my-sub\n",
    "\n",
    "# Stop and drain Dataflow job\n",
    "gcloud dataflow jobs drain [job-id]\n",
    "\n",
    "# Seek subscription to the snapshot\n",
    "gcloud pubsub subscriptions seek my-sub --snapshot=my-snapshot\n",
    "\n",
    "# Resubmit the pipeline\n",
    "gcloud dataflow jobs run my-job-name \\\n",
    "--gcs_location=my_gcs_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d804f1-13ac-4809-9a0c-30cc460094f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CI/CD Testing\n",
    "\n",
    "Introduce frameworks and features available to streamline CI/CD workflow for Dataflow pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9779656a-8787-4cce-aea1-12577073724b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Unit Testing\n",
    "\n",
    "Performing unit tests for DoFns and PTransforms.\n",
    "`TestPipeline` is a special class included in the Beam SDK specifically for testing transforms and pipeline logic. Use the `assert_that` method to check that the output PCollection matches the expected output, and `equal_to` to verify that the output PCollection has the correct elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a23215-593d-4ded-aa7d-97e5de5c05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python PAssert\n",
    "from apache_beam.testing.util import assert_that\n",
    "from apache_beam.testing.util import equal_to\n",
    "\n",
    "# Python TestPipeline\n",
    "with TestPipeline as p:\n",
    "    INPUTS = [fake_input_1, fake_input_2]\n",
    "    test_output = p \n",
    "        | beam.Create(INPUTS) # Transforms to be tested\n",
    "    # Check whether a PCollection contains some elements in any order.\n",
    "        | assert_that(test_output, equal_to(EXPECTED_OUTPUTS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa8d76-ec61-4c23-849a-e99ce58b1fba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### In-flight Actions\n",
    "\n",
    "Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d04a9b-f131-4def-97a1-e4c04907644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m apache_beam.examples.wordcount \\\n",
    "--project $PROJECT \\\n",
    "--staging_location=gs://$BUCKET/tmp/ \\\n",
    "--input=gs://dataflow-samples/shakespeare/kinglear.txt \\\n",
    "--output=gs://$BUCKET/results/outputs \\\n",
    "--runner=DataflowRunner \\\n",
    "--update \\\n",
    "--job_name=[prior job name] \\\n",
    "--transform_name_mapping=='{\"oldTransform1\":\"newTransform1\", ...}' \\\n",
    "--region=$REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1ded7-e14d-4049-bf03-1a4bd46bf0d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Grant the `dataflow.worker` role to the Compute Engine default service account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350311c6-2d3e-4411-b8fd-28497ce8908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID=$(gcloud config get-value project)\n",
    "export PROJECT_NUMBER=$(gcloud projects list --filter=\"$PROJECT_ID\" \n",
    "    --format=\"value(PROJECT_NUMBER)\")\n",
    "export serviceAccount=\"\"$PROJECT_NUMBER\"-compute@developer.gserviceaccount.com\"\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "--member=\"serviceAccount:${serviceAccount}\" \\\n",
    "--role=\"roles/dataflow.worker\"\n",
    "\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "gsutil mb -l US gs://$PROJECT_ID\n",
    "gsutil cp testing.out gs://$PROJECT_ID/8a_Batch_Testing_Pipeline/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042071c-bbea-416f-86ae-6d47fbcc6337",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Performing unit tests for DoFns and PTransforms for a batch pipeline\n",
    "\n",
    "- Create a `TestPipeline`.\n",
    "- Create some test input data and use the `Create` transform to create a `PCollection` of your input data.\n",
    "- Apply your transform to the input `PCollection` and save the resulting `PCollection`.\n",
    "- Use the `assert_that` method from the `testing.util` module and its other methods to verify that the output `PCollection` contains the elements that you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b50924-a506-4603-b3ce-7ab70ef4f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_statistics_pipeline.py\n",
    "import json\n",
    "import typing\n",
    "import logging\n",
    "import apache_beam as beam\n",
    "\n",
    "class WeatherRecord(typing.NamedTuple):\n",
    "    loc_id: str\n",
    "    lat: float\n",
    "    lng: float\n",
    "    date: str\n",
    "    low_temp: float\n",
    "    high_temp: float\n",
    "    precip: float\n",
    "\n",
    "beam.coders.registry.register_coder(WeatherRecord, beam.coders.RowCoder)\n",
    "\n",
    "class ConvertCsvToWeatherRecord(beam.DoFn):\n",
    "\n",
    "    def process(self, line):\n",
    "        fields = 'loc_id,lat,lng,date,low_temp,high_temp,precip'.split(',')\n",
    "        values = line.split(',')\n",
    "        row = dict(zip(fields,values))\n",
    "        for num_field in ('lat', 'lng', 'low_temp', 'high_temp', 'precip'):\n",
    "            row[num_field] = float(row[num_field])\n",
    "        yield WeatherRecord(**row)\n",
    "\n",
    "class ConvertTempUnits(beam.DoFn):\n",
    "\n",
    "    def process(self, row):\n",
    "        row_dict = row._asdict()\n",
    "        for field in ('low_temp', 'high_temp'):\n",
    "            row_dict[field] = row_dict[field] * 1.8 + 32.0\n",
    "        yield WeatherRecord(**row_dict)\n",
    "\n",
    "class ConvertToJson(beam.DoFn):\n",
    "\n",
    "    def process(self, row):\n",
    "        line = json.dumps(row._asdict())\n",
    "        yield line\n",
    "\n",
    "class ComputeStatistics(beam.PTransform):\n",
    "\n",
    "    def expand(self, pcoll):\n",
    "    \n",
    "        results = (\n",
    "            pcoll | 'ComputeStatistics' >> beam.GroupBy('loc_id')\n",
    "                                        .aggregate_field('low_temp', min, 'record_low')\n",
    "                                        .aggregate_field('high_temp', max, 'record_high')\n",
    "                                        .aggregate_field('precip', sum, 'total_precip')\n",
    "                | 'ToJson' >> beam.ParDo(ConvertToJson())\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "\n",
    "class WeatherStats(beam.PTransform):\n",
    "\n",
    "    def expand(self, pcoll):\n",
    "\n",
    "        results = (\n",
    "            pcoll | \"ParseCSV\" >> beam.ParDo(ConvertCsvToWeatherRecord())\n",
    "                  | \"ConvertToF\" >> beam.ParDo(ConvertTempUnits())\n",
    "                  | \"ComputeStats\" >> ComputeStatistics()\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "def run():\n",
    "\n",
    "    p = beam.Pipeline()\n",
    "\n",
    "    (p | 'ReadCSV' >> beam.io.ReadFromText('./weather_data.csv')\n",
    "       | 'ComputeStatistics' >> WeatherStats()\n",
    "       | 'WriteJson' >> beam.io.WriteToText('./weather_stats', '.json')\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d88c67-1b4f-4a6d-b29e-33c0f3a5a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_statistics_pipeline_test.py\n",
    "import logging\n",
    "import json\n",
    "import unittest\n",
    "import sys\n",
    "\n",
    "from weather_statistics_pipeline import *\n",
    "from apache_beam.testing.test_pipeline import TestPipeline\n",
    "from apache_beam.testing.util import BeamAssertException\n",
    "from apache_beam.testing.util import assert_that, equal_to\n",
    "\n",
    "def main(out = sys.stderr, verbosity = 2):\n",
    "    loader = unittest.TestLoader()\n",
    "  \n",
    "    suite = loader.loadTestsFromModule(sys.modules[__name__])\n",
    "    unittest.TextTestRunner(out, verbosity = verbosity).run(suite)\n",
    "\n",
    "\n",
    "class ConvertToWeatherRecordTest(unittest.TestCase):\n",
    "\n",
    "    def test_convert_to_csv(self):\n",
    "\n",
    "        with TestPipeline() as p:\n",
    "\n",
    "            LINES = ['x,0.0,0.0,2/2/2021,1.0,2.0,0.1']\n",
    "            EXPECTED_OUTPUT = [WeatherRecord('x', 0.0, 0.0, '2/2/2021', 1.0, 2.0, 0.1)]\n",
    "\n",
    "            input_lines = p | beam.Create(LINES)\n",
    "\n",
    "            output = input_lines | beam.ParDo(ConvertCsvToWeatherRecord())\n",
    "\n",
    "            assert_that(output, equal_to(EXPECTED_OUTPUT))\n",
    "\n",
    "class ConvertTempUnitsTest(unittest.TestCase):\n",
    "\n",
    "    def test_convert_temp_units(self):\n",
    "\n",
    "        with TestPipeline() as p:\n",
    "\n",
    "            RECORDS = [WeatherRecord('x', 0.0, 0.0, '2/2/2021', 1.0, 2.0, 0.1),\n",
    "                       WeatherRecord('y', 0.0, 0.0, '2/2/2021', -3.0, -1.0, 0.3)]\n",
    "\n",
    "            EXPECTED_RECORDS = [WeatherRecord('x', 0.0, 0.0, '2/2/2021', 33.8, 35.6, 0.1),\n",
    "                               WeatherRecord('y', 0.0, 0.0, '2/2/2021', 26.6, 30.2, 0.3)]\n",
    "\n",
    "            input_records = p | beam.Create(RECORDS)\n",
    "\n",
    "            output = input_records | beam.ParDo(ConvertTempUnits())\n",
    "            \n",
    "            assert_that(output, equal_to(EXPECTED_RECORDS))\n",
    "\n",
    "class ComputeStatsTest(unittest.TestCase):\n",
    "    \n",
    "    def test_compute_statistics(self):\n",
    "\n",
    "        with TestPipeline() as p:\n",
    "\n",
    "            INPUT_RECORDS = [WeatherRecord('x', 0.0, 0.0, '2/2/2021', 33.8, 35.6, 0.1),\n",
    "                             WeatherRecord('x', 0.0, 0.0, '2/3/2021', 41.6, 65.3, 0.2),\n",
    "                             WeatherRecord('x', 0.0, 0.0, '2/4/2021', 45.3, 52.6, 0.2),\n",
    "                             WeatherRecord('y', 0.0, 0.0, '2/2/2021', 12.8, 23.6, 0.1),\n",
    "                             WeatherRecord('y', 0.0, 0.0, '2/3/2021', 26.6, 30.2, 0.3)]\n",
    "\n",
    "            EXPECTED_STATS = [json.dumps({'loc_id': 'x', 'record_low': 33.8, 'record_high': 65.3, 'total_precip': 0.5 }),\n",
    "                              json.dumps({'loc_id': 'y', 'record_low': 12.8, 'record_high': 30.2, 'total_precip': 0.4 })]\n",
    "\n",
    "            inputs = p | beam.Create(INPUT_RECORDS)\n",
    "\n",
    "            output = inputs | ComputeStatistics()\n",
    "\n",
    "            assert_that(output, equal_to(EXPECTED_STATS))\n",
    "\n",
    "class WeatherStatsTransformTest(unittest.TestCase):\n",
    "\n",
    "    def test_weather_stats_transform(self):\n",
    "\n",
    "        with TestPipeline() as p:\n",
    "\n",
    "            INPUT_STRINGS = [\"x,31.4,-39.2,2/2/21,4.0,7.5,0.1\",\n",
    "                             \"x,31.4,-39.2,2/2/21,3.5,6.0,0.3\",\n",
    "                             \"y,33.4,-49.2,2/2/21,12.5,17.5,0.5\"]\n",
    "\n",
    "            EXPECTED_STATS = [json.dumps({'loc_id': 'x', 'record_low': 38.3, 'record_high': 45.5, 'total_precip': 0.4 }),\n",
    "                              json.dumps({'loc_id': 'y', 'record_low': 54.5, 'record_high': 63.5, 'total_precip': 0.5 })]\n",
    "\n",
    "            inputs = p | beam.Create(INPUT_STRINGS)\n",
    "\n",
    "            output = inputs | WeatherStats()\n",
    "\n",
    "            assert_that(output, equal_to(EXPECTED_STATS))\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    with open('testing.out', 'w') as f:\n",
    "        main(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6654814-17cc-40f0-b916-655e22969f7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Perform unit testing for a streaming pipeline\n",
    "\n",
    "- Create a `TestPipeline`.\n",
    "- Use the `TestStream` class to generate streaming data. This includes generating a series of events, advancing the watermark, and advancing the processing time.\n",
    "- Use the `assert_that` method from the `testing.util` module and its other methods to verify that the output `PCollection` contains the elements that you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b2583-c2ec-4f01-97d7-0d4f8c1b7a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxi_streaming_pipeline.py\n",
    "import json\n",
    "import typing\n",
    "import logging\n",
    "import apache_beam as beam\n",
    "from apache_beam.transforms.trigger import AccumulationMode, AfterCount, AfterWatermark\n",
    "from apache_beam.transforms.combiners import CountCombineFn\n",
    "import argparse\n",
    "\n",
    "class TaxiRide(typing.NamedTuple):\n",
    "    ride_id: str\n",
    "    point_idx: int\n",
    "    latitude: float\n",
    "    longitude: float\n",
    "    timestamp: str\n",
    "    meter_reading: float\n",
    "    meter_increment: float\n",
    "    ride_status: str\n",
    "    passenger_count: int\n",
    "\n",
    "beam.coders.registry.register_coder(TaxiRide, beam.coders.RowCoder)\n",
    "\n",
    "class JsonToTaxiRide(beam.DoFn):\n",
    "\n",
    "    def process(self, line):\n",
    "        row = json.loads(line)\n",
    "        yield TaxiRide(**row)\n",
    "\n",
    "class ConvertCountToDict(beam.DoFn):\n",
    "\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        window_start = window.start.to_utc_datetime().strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        output = {\"taxi_rides\" : element, \"timestamp\": window_start}\n",
    "        yield output\n",
    "\n",
    "\n",
    "class TaxiCountTransform(beam.PTransform):\n",
    "\n",
    "    def expand(self, pcoll):\n",
    "        \n",
    "        output = (pcoll\n",
    "                    | \"ParseJson\" >> beam.ParDo(JsonToTaxiRide())\n",
    "                    | \"FilterForPickups\" >> beam.Filter(lambda x : x.ride_status == 'pickup')\n",
    "                    | \"WindowByMinute\" >> beam.WindowInto(beam.window.FixedWindows(60),\n",
    "                                              trigger=AfterWatermark(late=AfterCount(1)),\n",
    "                                              allowed_lateness=60,\n",
    "                                              accumulation_mode=AccumulationMode.ACCUMULATING)\n",
    "                    | \"CountPerMinute\" >> beam.CombineGlobally(CountCombineFn()).without_defaults()\n",
    "                 )\n",
    "\n",
    "        return output\n",
    "\n",
    "def run():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Load from Json from Pub/Sub into BigQuery')\n",
    "\n",
    "    parser.add_argument('--table_name', required=True, help='Output BQ table')\n",
    "\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    table_name = opts['table_name']\n",
    "\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"taxi_rides\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    p = beam.Pipeline()\n",
    "\n",
    "    (p | \"ReadFromPubSub\" >> beam.io.ReadFromPubSub(topic=\"projects/pubsub-public-data/topics/taxirides-realtime\") \n",
    "       | \"TaxiPickupCount\" >> TaxiCountTransform()\n",
    "       | \"ConvertToDict\" >> beam.ParDo(ConvertCountToDict())\n",
    "       | 'WriteAggToBQ' >> beam.io.WriteToBigQuery(\n",
    "                table_name,\n",
    "                schema=table_schema,\n",
    "                create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "                )\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120acaf9-4dd9-4fd9-bc2d-665ed4642628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taxi_streaming_pipeline_test.py\n",
    "import logging\n",
    "import json\n",
    "import unittest\n",
    "import sys\n",
    "\n",
    "import apache_beam as beam\n",
    "\n",
    "from taxi_streaming_pipeline import *\n",
    "from apache_beam.testing.test_pipeline import TestPipeline\n",
    "from apache_beam.testing.util import BeamAssertException\n",
    "from apache_beam.testing.util import assert_that, equal_to_per_window\n",
    "from apache_beam.testing.test_stream import TestStream\n",
    "from apache_beam.transforms.window import TimestampedValue, IntervalWindow\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, StandardOptions\n",
    "\n",
    "def main(out = sys.stderr, verbosity = 2):\n",
    "    loader = unittest.TestLoader()\n",
    "  \n",
    "    suite = loader.loadTestsFromModule(sys.modules[__name__])\n",
    "    unittest.TextTestRunner(out, verbosity = verbosity).run(suite)\n",
    "\n",
    "\n",
    "class TaxiWindowingTest(unittest.TestCase):\n",
    "\n",
    "    def test_windowing_behavior(self):\n",
    "\n",
    "        options = PipelineOptions()\n",
    "        options.view_as(StandardOptions).streaming = True\n",
    "\n",
    "        with TestPipeline(options=options) as p:\n",
    "\n",
    "            base_json_pickup = \"{\\\"ride_id\\\":\\\"x\\\",\\\"point_idx\\\":1,\\\"latitude\\\":0.0,\\\"longitude\\\":0.0,\" \\\n",
    "                         \"\\\"timestamp\\\":\\\"00:00:00\\\",\\\"meter_reading\\\":1.0,\\\"meter_increment\\\":0.1,\" \\\n",
    "                         \"\\\"ride_status\\\":\\\"pickup\\\",\\\"passenger_count\\\":1}\" \n",
    "\n",
    "            base_json_enroute = \"{\\\"ride_id\\\":\\\"x\\\",\\\"point_idx\\\":1,\\\"latitude\\\":0.0,\\\"longitude\\\":0.0,\" \\\n",
    "                         \"\\\"timestamp\\\":\\\"00:00:00\\\",\\\"meter_reading\\\":1.0,\\\"meter_increment\\\":0.1,\" \\\n",
    "                         \"\\\"ride_status\\\":\\\"pickup\\\",\\\"passenger_count\\\":1}\" \n",
    "            \n",
    "\n",
    "            test_stream = TestStream().advance_watermark_to(0).add_elements([\n",
    "                TimestampedValue(base_json_pickup, 0),\n",
    "                TimestampedValue(base_json_pickup, 0),\n",
    "                TimestampedValue(base_json_enroute, 0),\n",
    "                TimestampedValue(base_json_pickup, 60)\n",
    "            ]).advance_watermark_to(60).advance_processing_time(60).add_elements([\n",
    "                TimestampedValue(base_json_pickup, 120)\n",
    "            ]).advance_watermark_to_infinity()\n",
    "\n",
    "            taxi_counts = (p | test_stream\n",
    "                             | TaxiCountTransform()\n",
    "                          )\n",
    "\n",
    "            EXPECTED_WINDOW_COUNTS = {IntervalWindow(0,60): [3],\n",
    "                                      IntervalWindow(60,120): [1],\n",
    "                                      IntervalWindow(120,180): [1]}\n",
    "\n",
    "            assert_that(taxi_counts, equal_to_per_window(EXPECTED_WINDOW_COUNTS),\n",
    "                        reify_windows=True)\n",
    "\n",
    "class TaxiLateDataTest(unittest.TestCase):\n",
    "\n",
    "        def test_late_data_behavior(self):\n",
    "\n",
    "            options = PipelineOptions()\n",
    "            options.view_as(StandardOptions).streaming = True\n",
    "\n",
    "            with TestPipeline(options=options) as p:\n",
    "\n",
    "                base_json_pickup = \"{\\\"ride_id\\\":\\\"x\\\",\\\"point_idx\\\":1,\\\"latitude\\\":0.0,\\\"longitude\\\":0.0,\" \\\n",
    "                            \"\\\"timestamp\\\":\\\"00:00:00\\\",\\\"meter_reading\\\":1.0,\\\"meter_increment\\\":0.1,\" \\\n",
    "                            \"\\\"ride_status\\\":\\\"pickup\\\",\\\"passenger_count\\\":1}\" \n",
    "\n",
    "                test_stream = TestStream().advance_watermark_to(0).add_elements([\n",
    "                    TimestampedValue(base_json_pickup, 0),\n",
    "                    TimestampedValue(base_json_pickup, 0),\n",
    "                ]).advance_watermark_to(60).advance_processing_time(60).add_elements([\n",
    "                    TimestampedValue(base_json_pickup, 0)\n",
    "                ]).advance_watermark_to(300).advance_processing_time(240).add_elements([\n",
    "                    TimestampedValue(base_json_pickup, 0)\n",
    "                ])\n",
    "\n",
    "                EXPECTED_RESULTS = {IntervalWindow(0,60): [2,3]}  #On Time and Late Result\n",
    "\n",
    "                taxi_counts_late = (p | test_stream\n",
    "                                      | TaxiCountTransform()\n",
    "                                   )\n",
    "\n",
    "                assert_that(taxi_counts_late, equal_to_per_window(EXPECTED_RESULTS),\n",
    "                            reify_windows=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with open('testing.out', 'w') as f:\n",
    "        main(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221327fe-64ae-4830-9b51-0021961a07fc",
   "metadata": {},
   "source": [
    "### The CI/CD pipeline\n",
    "\n",
    "![CI CD pipeline](./img/CI_CD_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4512a85-0272-4bf7-ac18-a534b66a26e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Cloud Composer environment\n",
    "gcloud composer environments create $COMPOSER_ENV_NAME \\\n",
    "--location $COMPOSER_REGION \\\n",
    "--zone $COMPOSER_ZONE_ID \\\n",
    "--machine-type n1-standard-1 \\\n",
    "--node-count 3 \\\n",
    "--disk-size 20 \\\n",
    "--python-version 3\n",
    "\n",
    "# Cloud Composer environment variable\n",
    "export COMPOSER_DAG_BUCKET=$(gcloud composer environments \\\n",
    "    describe $COMPOSER_ENV_NAME \\\n",
    "    --location $COMPOSER_REGION \\\n",
    "    --format=\"get(config.dagGcsPrefix)\")\n",
    "\n",
    "# Service account\n",
    "export COMPOSER_SERVICE_ACCOUNT=$(gcloud composer environments \\\n",
    "    describe $COMPOSER_ENV_NAME \\\n",
    "    --location $COMPOSER_REGION \\\n",
    "    --format=\"get(config.nodeConfig.serviceAccount)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c64e4d-ea2a-4646-b253-d7f12c21db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Source Repositories\n",
    "gcloud source repos create $SOURCE_CODE_REPO\n",
    "cp -r ~/ci-cd-for-data-processing-workflow/source-code ~/$SOURCE_CODE_REPO\n",
    "cd ~/$SOURCE_CODE_REPO\n",
    "git config --global credential.'https://source.developers.google.com'.helper gcloud.sh\n",
    "git config --global user.email $(gcloud config list --format 'value(core.account)')\n",
    "git config --global user.name $(gcloud config list --format 'value(core.account)')\n",
    "git init\n",
    "git remote add google \\\n",
    "    https://source.developers.google.com/p/$GCP_PROJECT_ID/r/$SOURCE_CODE_REPO\n",
    "git add .\n",
    "git commit -m 'initial commit'\n",
    "git push google master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a0c4d6-00e4-450b-854f-ca76721eef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Build pipeline\n",
    "cd ~/ci-cd-for-data-processing-workflow/source-code/build-pipeline\n",
    "gcloud builds submit --config=build_deploy_test.yaml --substitutions=\\\n",
    "REPO_NAME=$SOURCE_CODE_REPO,\\\n",
    "_DATAFLOW_JAR_BUCKET=$DATAFLOW_JAR_BUCKET_TEST,\\\n",
    "_COMPOSER_INPUT_BUCKET=$INPUT_BUCKET_TEST,\\\n",
    "_COMPOSER_REF_BUCKET=$REF_BUCKET_TEST,\\\n",
    "_COMPOSER_DAG_BUCKET=$COMPOSER_DAG_BUCKET,\\\n",
    "_COMPOSER_ENV_NAME=$COMPOSER_ENV_NAME,\\\n",
    "_COMPOSER_REGION=$COMPOSER_REGION,\\\n",
    "_COMPOSER_DAG_NAME_TEST=$COMPOSER_DAG_NAME_TEST\n",
    "\n",
    "# Get the URL to Cloud Composer web interface\n",
    "gcloud composer environments describe $COMPOSER_ENV_NAME \\\n",
    "--location $COMPOSER_REGION \\\n",
    "--format=\"get(config.airflowUri)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06734420-26df-42d0-821b-82fb276e11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Composer variable for the JAR filename\n",
    "export DATAFLOW_JAR_FILE_LATEST=$(gcloud composer environments run $COMPOSER_ENV_NAME \\\n",
    "--location $COMPOSER_REGION variables -- \\\n",
    "--get dataflow_jar_file_test 2>&1 | grep -i '.jar')\n",
    "\n",
    "# Build pipeline configuration file\n",
    "cd ~/ci-cd-for-data-processing-workflow/source-code/build-pipeline\n",
    "gcloud builds submit --config=deploy_prod.yaml --substitutions=\\\n",
    "REPO_NAME=$SOURCE_CODE_REPO,\\\n",
    "_DATAFLOW_JAR_BUCKET_TEST=$DATAFLOW_JAR_BUCKET_TEST,\\\n",
    "_DATAFLOW_JAR_FILE_LATEST=$DATAFLOW_JAR_FILE_LATEST,\\\n",
    "_DATAFLOW_JAR_BUCKET_PROD=$DATAFLOW_JAR_BUCKET_PROD,\\\n",
    "_COMPOSER_INPUT_BUCKET=$INPUT_BUCKET_PROD,\\\n",
    "_COMPOSER_ENV_NAME=$COMPOSER_ENV_NAME,\\\n",
    "_COMPOSER_REGION=$COMPOSER_REGION,\\\n",
    "_COMPOSER_DAG_BUCKET=$COMPOSER_DAG_BUCKET,\\\n",
    "_COMPOSER_DAG_NAME_PROD=$COMPOSER_DAG_NAME_PROD\n",
    "\n",
    "# Get the URL for Cloud Composer UI\n",
    "gcloud composer environments describe $COMPOSER_ENV_NAME \\\n",
    "--location $COMPOSER_REGION \\\n",
    "--format=\"get(config.airflowUri)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc60bc5-4a94-4b8a-a889-2b2464b336d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Flex Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e385fb-883e-457c-be51-11b28aa5e974",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create a custom Dataflow Flex Template container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4311e6-db5c-4533-86d5-12435d36c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_pipeline.py\n",
    "import argparse\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import GoogleCloudOptions\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.runners import DataflowRunner, DirectRunner\n",
    "\n",
    "# ### functions and classes\n",
    "\n",
    "def parse_json(element):\n",
    "    return json.loads(element)\n",
    "\n",
    "def drop_fields(element):\n",
    "    element.pop('user_agent')\n",
    "    return element\n",
    "\n",
    "# ### main\n",
    "\n",
    "def run():\n",
    "    # Command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Load from Json into BigQuery')\n",
    "    parser.add_argument('--project',required=True, help='Specify Google Cloud project')\n",
    "    parser.add_argument('--region', required=True, help='Specify Google Cloud region')\n",
    "    parser.add_argument('--runner', required=True, help='Specify Apache Beam Runner')\n",
    "    parser.add_argument('--inputPath', required=True, help='Path to events.json')\n",
    "    parser.add_argument('--outputPath', required=True, help='Path to coldline storage bucket')\n",
    "    parser.add_argument('--tableName', required=True, help='BigQuery table name')\n",
    "\n",
    "    opts, pipeline_opts = parser.parse_known_args()\n",
    "\n",
    "    # Setting up the Beam pipeline options\n",
    "    options = PipelineOptions(pipeline_opts)\n",
    "    options.view_as(GoogleCloudOptions).project = opts.project\n",
    "    options.view_as(GoogleCloudOptions).region = opts.region\n",
    "    options.view_as(GoogleCloudOptions).job_name = '{0}{1}'.format('my-pipeline-',time.time_ns())\n",
    "    options.view_as(StandardOptions).runner = opts.runner\n",
    "\n",
    "    input_path = opts.inputPath\n",
    "    output_path = opts.outputPath\n",
    "    table_name = opts.tableName\n",
    "\n",
    "    # Table schema for BigQuery\n",
    "    table_schema = {\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"ip\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"user_id\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lat\",\n",
    "                \"type\": \"FLOAT\",\n",
    "                \"mode\": \"NULLABLE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"lng\",\n",
    "                \"type\": \"FLOAT\",\n",
    "                \"mode\": \"NULLABLE\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"timestamp\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_request\",\n",
    "                \"type\": \"STRING\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"http_response\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"num_bytes\",\n",
    "                \"type\": \"INTEGER\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Create the pipeline\n",
    "    p = beam.Pipeline(options=options)\n",
    "\n",
    "    '''\n",
    "    Steps:\n",
    "    1) Read something\n",
    "    2) Transform something\n",
    "    3) Write something\n",
    "    '''\n",
    "\n",
    "    # Read in lines to an initial PCollection that can then be branched off of\n",
    "    lines = p | 'ReadFromGCS' >> beam.io.ReadFromText(input_path)\n",
    "\n",
    "    # Write to Google Cloud Storage\n",
    "    lines | 'WriteRawToGCS' >> beam.io.WriteToText(output_path)\n",
    "\n",
    "    # Read elements from Json, filter out individual elements, and write to BigQuery\n",
    "    (lines\n",
    "        | 'ParseJson' >> beam.Map(parse_json)\n",
    "        | 'DropFields' >> beam.Map(drop_fields)\n",
    "        | 'FilterFn' >> beam.Filter(lambda row: row['num_bytes'] < 120)\n",
    "        | 'WriteToBQ' >> beam.io.WriteToBigQuery(\n",
    "            table_name,\n",
    "            schema=table_schema,\n",
    "            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "            write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE\n",
    "            )\n",
    "    )\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"Building pipeline ...\")\n",
    "\n",
    "    p.run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36fce7-2bbf-4229-9656-1b2bc9331ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dockerfile\n",
    "FROM gcr.io/dataflow-templates-base/python3-template-launcher-base\n",
    "ARG WORKDIR=/dataflow/template\n",
    "RUN mkdir -p ${WORKDIR}\n",
    "WORKDIR ${WORKDIR}\n",
    "RUN apt-get update && apt-get install -y libffi-dev && rm -rf /var/lib/apt/lists/*\n",
    "COPY my_pipeline.py .\n",
    "ENV FLEX_TEMPLATE_PYTHON_PY_FILE=\"${WORKDIR}/my_pipeline.py\"\n",
    "RUN python3 -m pip install apache-beam[gcp]==2.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a40baf-3bba-42de-9297-82479c35be95",
   "metadata": {},
   "source": [
    "First, enable Kaniko cache use by default. Kaniko caches container build artifacts, so using this option speeds up subsequent builds. We will also use `pip3 freeze` to record the packages and their versions being used in our environment.\n",
    "\n",
    "Cloud Build to build the container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d8a55-85a1-4694-8390-d3bec8c7f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud config set builds/use_kaniko True\n",
    "\n",
    "export TEMPLATE_IMAGE=\"gcr.io/$PROJECT_ID/dataflow/my_pipeline:latest\"\n",
    "gcloud builds submit --tag $TEMPLATE_IMAGE ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090b0c5-ec47-47a7-baf0-30cdd8de786f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create and stage the flex template\n",
    "\n",
    "Create a template spec file in a Cloud Storage containing all of the necessary information to run the job, such as the SDK information and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedbaa91-0df0-446c-b942-42d36f401734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata.json\n",
    "{\n",
    "  \"name\": \"My Branching Pipeline\",\n",
    "  \"description\": \"A branching pipeline that writes raw to GCS Coldline, and filtered data to BQ\",\n",
    "  \"parameters\": [\n",
    "    {\n",
    "      \"name\": \"inputPath\",\n",
    "      \"label\": \"Input file path.\",\n",
    "      \"helpText\": \"Path to events.json file.\",\n",
    "      \"regexes\": [\n",
    "        \".*\\\\.json\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"outputPath\",\n",
    "      \"label\": \"Output file location\",\n",
    "      \"helpText\": \"GCS Coldline Bucket location for raw data\",\n",
    "      \"regexes\": [\n",
    "        \"gs:\\\\/\\\\/[a-zA-z0-9\\\\-\\\\_\\\\/]+\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"tableName\",\n",
    "      \"label\": \"BigQuery output table\",\n",
    "      \"helpText\": \"BigQuery table spec to write to, in the form 'project:dataset.table'.\",\n",
    "      \"regexes\": [\n",
    "        \"[^:]+:[^.]+[.].+\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673bfa8-0f8b-47bc-8e81-b6d2d3c8c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "export TEMPLATE_PATH=\"gs://${PROJECT_ID}/templates/mytemplate.json\"\n",
    "\n",
    "gcloud dataflow flex-template build $TEMPLATE_PATH \\\n",
    "--image \"$TEMPLATE_IMAGE\" \\\n",
    "--sdk-language \"PYTHON\" \\\n",
    "--metadata-file \"metadata.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6535ec-9069-4ef7-83fb-92821f162136",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Execute the flex template\n",
    "\n",
    "One of the benefits of using Dataflow templates is the ability to execute them from a wider variety of contexts, other than a development environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b5596-9f0d-4e7d-84f9-a62be5f5d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcloud\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "export REGION='us-central1'\n",
    "export JOB_NAME=mytemplate-$(date +%Y%m%d-%H%M$S)\n",
    "export TEMPLATE_LOC=gs://${PROJECT_ID}/templates/mytemplate.json\n",
    "export INPUT_PATH=gs://${PROJECT_ID}/events.json\n",
    "export OUTPUT_PATH=gs://${PROJECT_ID}-coldline/template_output/\n",
    "export BQ_TABLE=${PROJECT_ID}:logs.logs_filtered\n",
    "gcloud dataflow flex-template run ${JOB_NAME} \\\n",
    "--region=$REGION \\\n",
    "--template-file-gcs-location ${TEMPLATE_LOC} \\\n",
    "--parameters \"inputPath=${INPUT_PATH},outputPath=${OUTPUT_PATH},tableName=${BQ_TABLE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f7d26-f110-47bd-b004-63fcb811da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcloud\n",
    "gcloud dataflow flex-template run \"job-name-`date +%Y%m%d-%H%M%S`\" \\\n",
    "--template-file-gcs-location \"$TEMPLATE_PATH\" \\\n",
    "--parameters inputSubscription=\"$SUBSCRIPTION\" \\\n",
    "--parameters outputTable=\"$PROJECT:$DATASET.$TABLE\" \\\n",
    "--region=\"$REGION\"\n",
    "\n",
    "# REST API\n",
    "curl -X POST \"https://dataflow.googleapis.com/v1b3/projects/$PROJECT/locations/${REGION}/flexTemplates:launch\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "-d '{\n",
    "  \"launch_parameter\": {\n",
    "    \"jobName\": \"job-name-`date +%Y%m%d-%H%M%S`\",\n",
    "    \"parameters\": {\n",
    "      \"inputSubscription\": \"'$SUBSCRIPTION'\",\n",
    "      \"outputTable\": \"'$PROJECT:$DATASET.$TABLE'\"\n",
    "    },\n",
    "    \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\"\n",
    "  }\n",
    "}'\n",
    "\n",
    "# Cloud Scheduler\n",
    "gcloud scheduler jobs create http scheduler-job \\\n",
    "--schedule=\"*/30 * * * *\" \\\n",
    "--uri=\"https://dataflow.googleapis.com/v1b3/projects/$PROJECT/locations/${REGION}/flexTemplates:launch\" \\\n",
    "--http-method=POST \\\n",
    "--headers Content-Type=application/json \\\n",
    "--oauth-service-account-email=email@project.iam.gserviceaccount.com \\\n",
    "--message-body='{\n",
    "    \"launch_parameter\": {\n",
    "      \"jobName\":\"job-name\"\n",
    "      \"parameters\": {\n",
    "        \"inputSubscription\": \"'$SUBSCRIPTION'\",\n",
    "        \"outputTable\": \"'$PROJECT:$DATASET.$TABLE'\"\n",
    "      },\n",
    "      \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\"\n",
    "    }\n",
    "}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
