{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9888d59e-fda1-4552-9dd6-c277a22997cc",
   "metadata": {},
   "source": [
    "# Machine Learning Infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0942fa-c4bd-415f-b759-940f5eee67af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Learning TensorFlow\n",
    "\n",
    "Build a model that is trained on data to infer the rules that determine a relationship between numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd9acf2-15ce-4d33-9e43-63c5e30821f5",
   "metadata": {},
   "source": [
    "### Install the TensorFlow pip package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e6ed7-0f83-4114-a972-6a933330906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt install python3-pip\n",
    "sudo pip3 install -U virtualenv  # system-wide install\n",
    "\n",
    "virtualenv --system-site-packages -p python3 ./venv\n",
    "source ./venv/bin/activate  # sh, bash, ksh, or zsh\n",
    "pip install --upgrade pip\n",
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4040c533-8b7d-4b4d-be52-737775cc13fb",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc868b28-2b5c-4030-a86f-c8da37a357e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    keras.layers.Dense(units=1, input_shape=[1])\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-2.0, 1.0, 4.0, 7.0, 10.0, 13.0], dtype=float)\n",
    "\n",
    "model.fit(xs, ys, epochs=500)\n",
    "\n",
    "model.predict([10.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1dcb7c-2df0-4e5a-a40b-4713d6bb00e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scikit-learn Model Serving with Online Prediction Using AI Platform\n",
    "\n",
    "![scikit-learn_AI_Platform](./img/scikit-learn_AI_Platform.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815c4e7-a500-4247-85cd-5ad4dfd0e440",
   "metadata": {},
   "source": [
    "### Create a Debian 9 virtual machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fecb8-3d91-489d-8b87-21de2bacd998",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud compute instances create scikit-vm \\\n",
    " --image-project=debian-cloud \\\n",
    " --image-family=debian-9 \\\n",
    " --service-account=$(gcloud config get-value project)@$(gcloud config get-value project).iam.gserviceaccount.com \\\n",
    " --scopes=cloud-platform,default,storage-full \\\n",
    " --zone=us-central1-a \\\n",
    " --tags http-server,https-server\n",
    "\n",
    "gcloud compute ssh --zone=us-central1-a scikit-vm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce91053-65b6-4198-9d18-b79400ceb739",
   "metadata": {},
   "source": [
    "### Train and save model\n",
    "\n",
    "Save the following content to the `train.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015d2aa-dae7-407c-938f-2a1572351b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Define the format of your input data including unused columns \n",
    "# (These are the columns from the census data files)\n",
    "COLUMNS = (\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "    'income-level'\n",
    ")\n",
    "# Categorical columns are columns that need to be turned into a numerical value \n",
    "# to be used by scikit-learn\n",
    "CATEGORICAL_COLUMNS = (\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native-country'\n",
    ")\n",
    "\n",
    "# Load the training census dataset\n",
    "with open('./census_data/adult.data', 'r') as train_data:\n",
    "    raw_training_data = pd.read_csv(train_data, header=None, names=COLUMNS)\n",
    "# Remove the column we are trying to predict ('income-level') from our features list\n",
    "# Convert the Dataframe to a lists of lists\n",
    "train_features = raw_training_data.drop('income-level', axis=1).as_matrix().tolist()\n",
    "# Create our training labels list, convert the Dataframe to a lists of lists\n",
    "train_labels = (raw_training_data['income-level'] == ' >50K').as_matrix().tolist()\n",
    "# Load the test census dataset\n",
    "with open('./census_data/adult.test', 'r') as test_data:\n",
    "    raw_testing_data = pd.read_csv(test_data, names=COLUMNS, skiprows=1)\n",
    "# Remove the column we are trying to predict ('income-level') from our features list\n",
    "# Convert the Dataframe to a lists of lists\n",
    "test_features = raw_testing_data.drop('income-level', axis=1).as_matrix().tolist()\n",
    "# Create our training labels list, convert the Dataframe to a lists of lists\n",
    "test_labels = (raw_testing_data['income-level'] == ' >50K.').as_matrix().tolist()\n",
    "\n",
    "# Since the census data set has categorical features, we need to convert\n",
    "# them to numerical values. We'll use a list of pipelines to convert each\n",
    "# categorical column and then use FeatureUnion to combine them before calling\n",
    "# the RandomForestClassifier.\n",
    "categorical_pipelines = []\n",
    "# Each categorical column needs to be extracted individually and converted to \n",
    "# a numerical value. To do this, each categorical column will use a pipeline that \n",
    "# extracts one feature column via SelectKBest(k=1) and a LabelBinarizer() to convert \n",
    "# the categorical value to a numerical one. A scores array (created below) will select \n",
    "# and extract the feature column. The scores array is created by iterating over \n",
    "# the COLUMNS and checking if it is a CATEGORICAL_COLUMN.\n",
    "for i, col in enumerate(COLUMNS[:-1]):\n",
    "    if col in CATEGORICAL_COLUMNS:\n",
    "        # Create a scores array to get the individual categorical column.\n",
    "        # Example:\n",
    "        #  data = [39, 'State-gov', 77516, 'Bachelors', 13, 'Never-married', 'Adm-clerical',\n",
    "        #         'Not-in-family', 'White', 'Male', 2174, 0, 40, 'United-States']\n",
    "        #  scores = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        #\n",
    "        # Returns: [['Sate-gov']]\n",
    "        scores = []\n",
    "        # Build the scores array\n",
    "        for j in range(len(COLUMNS[:-1])):\n",
    "            if i == j: # This column is the categorical column we want to extract.\n",
    "                scores.append(1) # Set to 1 to select this column\n",
    "            else: # Every other column should be ignored.\n",
    "                scores.append(0)\n",
    "        skb = SelectKBest(k=1)\n",
    "        skb.scores_ = scores\n",
    "        # Convert the categorical column to a numerical value\n",
    "        lbn = LabelBinarizer()\n",
    "        r = skb.transform(train_features)\n",
    "        lbn.fit(r)\n",
    "        # Create the pipeline to extract the categorical feature\n",
    "        categorical_pipelines.append(\n",
    "            ('categorical-{}'.format(i), Pipeline([\n",
    "                ('SKB-{}'.format(i), skb),\n",
    "                ('LBN-{}'.format(i), lbn)])))\n",
    "# Create pipeline to extract the numerical features\n",
    "skb = SelectKBest(k=6)\n",
    "# From COLUMNS use the features that are numerical\n",
    "skb.scores_ = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
    "categorical_pipelines.append(('numerical', skb))\n",
    "# Combine all the features using FeatureUnion\n",
    "preprocess = FeatureUnion(categorical_pipelines)\n",
    "\n",
    "# Create the classifier\n",
    "classifier = RandomForestClassifier()\n",
    "# Transform the features and fit them to the classifier\n",
    "classifier.fit(preprocess.transform(train_features), train_labels)\n",
    "# Create the overall model as a single pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('union', preprocess),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "# Export the model to a file\n",
    "joblib.dump(pipeline, 'model.joblib')\n",
    "print('Model trained and saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1efcf34-a708-48db-94d5-d6483e070bcc",
   "metadata": {},
   "source": [
    "### Upload the saved model\n",
    "\n",
    "This step takes your local `model.joblib` file and uploads it to Cloud Storage via the Cloud SDK using `gsutil`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e74580-87a7-4a77-ab4a-d2abf1dea1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil cp ./model.joblib $MODEL_PATH/model.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c284d28-a2ee-4b93-a163-d05f6581ad0b",
   "metadata": {},
   "source": [
    "### Create a model resource\n",
    "\n",
    "AI Platform organizes your trained models using model and version resources. An AI Platform model is a container for the versions of your machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31023be8-caa4-4dde-bbef-d76be9602b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud ai-platform models create $MODEL_NAME --region $REGION\n",
    "\n",
    "gcloud beta ai-platform versions create $VERSION_NAME \\\n",
    "    --model $MODEL_NAME \\\n",
    "    --origin $MODEL_PATH \\\n",
    "    --runtime-version=\"1.14\" \\\n",
    "    --framework=\"SCIKIT_LEARN\" \\\n",
    "    --python-version=\"3.5\" \\\n",
    "    --region=$REGION\n",
    "\n",
    "gcloud ai-platform versions list --model $MODEL_NAME --region us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51355f51-f3ce-49c6-b41b-fc2dba888caa",
   "metadata": {},
   "source": [
    "### Make an online prediction\n",
    "\n",
    "Save the following content to the `test.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94a33c0-ec9b-4fb6-9eaf-f1b966524210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "import os\n",
    "import pandas as pd\n",
    "from google.api_core.client_options import ClientOptions\n",
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "VERSION_NAME = os.environ['VERSION_NAME']\n",
    "MODEL_NAME = os.environ['MODEL_NAME']\n",
    "# Define the format of your input data including unused columns \n",
    "# (These are the columns from the census data files)\n",
    "COLUMNS = (\n",
    "    'age',\n",
    "    'workclass',\n",
    "    'fnlwgt',\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "    'income-level'\n",
    ")\n",
    "# Categorical columns are columns that need to be turned into \n",
    "# a numerical value to be used by scikit-learn\n",
    "CATEGORICAL_COLUMNS = (\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'native-country'\n",
    ")\n",
    "# Load the training census dataset\n",
    "with open('./census_data/adult.data', 'r') as train_data:\n",
    "    raw_training_data = pd.read_csv(train_data, header=None, names=COLUMNS)\n",
    "# Remove the column we are trying to predict ('income-level') from our features list\n",
    "# Convert the Dataframe to a lists of lists\n",
    "train_features = raw_training_data.drop('income-level', axis=1).as_matrix().tolist()\n",
    "# Create our training labels list, convert the Dataframe to a lists of lists\n",
    "train_labels = (raw_training_data['income-level'] == ' >50K').as_matrix().tolist()\n",
    "# Load the test census dataset\n",
    "with open('./census_data/adult.test', 'r') as test_data:\n",
    "    raw_testing_data = pd.read_csv(test_data, names=COLUMNS, skiprows=1)\n",
    "# Remove the column we are trying to predict ('income-level') from our features list\n",
    "# Convert the Dataframe to a lists of lists\n",
    "test_features = raw_testing_data.drop('income-level', axis=1).as_matrix().tolist()\n",
    "# Create our training labels list, convert the Dataframe to a lists of lists\n",
    "test_labels = (raw_testing_data['income-level'] == ' >50K.').as_matrix().tolist()\n",
    "\n",
    "endpoint = 'https://us-central1-ml.googleapis.com'\n",
    "client_options = ClientOptions(api_endpoint=endpoint)\n",
    "service = googleapiclient.discovery.build('ml', 'v1', client_options=client_options)\n",
    "name = 'projects/{}/models/{}'.format(PROJECT_ID, MODEL_NAME)\n",
    "name += '/versions/{}'.format(VERSION_NAME)\n",
    "# Due to the size of the data, it needs to be split in 2\n",
    "first_half = test_features[:int(len(test_features)/2)]\n",
    "second_half = test_features[int(len(test_features)/2):]\n",
    "complete_results = []\n",
    "for data in [first_half, second_half]:\n",
    "    responses = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': data}\n",
    "    ).execute()\n",
    "    if 'error' in responses:\n",
    "        print(responses['error'])\n",
    "    else:\n",
    "        complete_results.extend(responses['predictions'])\n",
    "# Print the first 10 responses\n",
    "for i, response in enumerate(complete_results[:10]):\n",
    "    print('Prediction: {}\\tLabel: {}'.format(response, test_labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5792b2-2c70-41fe-a149-890e13527a07",
   "metadata": {},
   "source": [
    "## Vertex AI\n",
    "\n",
    "BigQuery for data processing and exploratory data analysis and the Vertex AI platform to train and deploy a custom TensorFlow Regressor model to predict customer lifetime value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3038cc-4b52-4a94-8a14-f4eb5887c213",
   "metadata": {},
   "source": [
    "### Create Vertex AI custom service account for Vertex Tensorboard integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42bf93f-4008-4186-a2bc-0bc4dc22d3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud services enable \\\n",
    "  compute.googleapis.com \\\n",
    "  iam.googleapis.com \\\n",
    "  iamcredentials.googleapis.com \\\n",
    "  monitoring.googleapis.com \\\n",
    "  logging.googleapis.com \\\n",
    "  notebooks.googleapis.com \\\n",
    "  aiplatform.googleapis.com \\\n",
    "  bigquery.googleapis.com \\\n",
    "  artifactregistry.googleapis.com \\\n",
    "  cloudbuild.googleapis.com \\\n",
    "  container.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e10de-ba29-4366-ab29-96eb3320b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT_ID=vertex-custom-training-sa\n",
    "gcloud iam service-accounts create $SERVICE_ACCOUNT_ID  \\\n",
    "--description=\"A custom service account for Vertex custom training with Tensorboard\" \\\n",
    "--display-name=\"Vertex AI Custom Training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a61013-e2f8-4241-98c0-6c7f4280b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID=$(gcloud config get-value core/project)\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "--member=serviceAccount:$SERVICE_ACCOUNT_ID@$PROJECT_ID.iam.gserviceaccount.com \\\n",
    "--role=\"roles/storage.admin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e219b05-d2ec-4282-9fd6-95a1514557e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "--member=serviceAccount:$SERVICE_ACCOUNT_ID@$PROJECT_ID.iam.gserviceaccount.com \\\n",
    "--role=\"roles/bigquery.admin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcabac5-afbc-4df2-8f8f-1ae53337c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "--member=serviceAccount:$SERVICE_ACCOUNT_ID@$PROJECT_ID.iam.gserviceaccount.com \\\n",
    "--role=\"roles/aiplatform.user\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
