{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CI/CD for a KFP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "1. Learn how to create a custom Cloud Build builder to pilot CAIP Pipelines\n",
    "1. Learn how to write a Cloud Build config file to build and push all the artifacts for a KFP\n",
    "1. Learn how to setup a Cloud Build Github trigger to rebuild the KFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will walk through authoring of a **Cloud Build** CI/CD workflow that automatically builds and deploys a KFP pipeline. You will also integrate your workflow with **GitHub** by setting up a trigger that starts the  workflow when a new tag is applied to the **GitHub** repo hosting the pipeline's code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create required GKE cluster\n",
    "gcloud container clusters create cluster-1 \\\n",
    "--zone us-central1-a \\\n",
    "--cluster-version 1.21.5-gke.1302 \\\n",
    "--machine-type n1-standard-2 \\\n",
    "--scopes=https://www.googleapis.com/auth/cloud-platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring environment settings\n",
    "\n",
    "Update  the `ENDPOINT` constat  with the settings reflecting your lab environment. \n",
    "\n",
    "Then endpoint to the AI Platform Pipelines instance can be found on the [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines/clusters) page in the Google Cloud Console.\n",
    "\n",
    "1. Open the *SETTINGS* for your instance\n",
    "2. Use the value of the `host` variable in the *Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SKD* section of the *SETTINGS* window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = 'https://77bbd65bbd0b74b5-dot-us-central1.pipelines.googleusercontent.com'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the KFP CLI builder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the cell below, write a docker file that\n",
    "* Uses `gcr.io/deeplearning-platform-release/base-cpu` as base image\n",
    "* Install the python package `kfp` with version `0.2.5`\n",
    "* Starts `/bin/bash` as entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kfp-cli/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile kfp-cli/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install kfp==0.2.5\n",
    "ENTRYPOINT [\"/bin/bash\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the image and push it to your project's **Container Registry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='kfp-cli'\n",
    "TAG='latest'\n",
    "IMAGE_URI='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the cell below, use `gcloud builds` to build the `kfp-cli` Docker image and push it to the project gcr.io registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 103 bytes before compression.\n",
      "Uploading tarball of [kfp-cli] to [gs://qwiklabs-gcp-00-8962bb682264_cloudbuild/source/1642924130.219413-9cf6987f087d434d903f7b571c4c3a23.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-00-8962bb682264/locations/global/builds/9f32b423-b466-4483-8c4d-118a47c18587].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/9f32b423-b466-4483-8c4d-118a47c18587?project=334587390474].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"9f32b423-b466-4483-8c4d-118a47c18587\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-00-8962bb682264_cloudbuild/source/1642924130.219413-9cf6987f087d434d903f7b571c4c3a23.tgz#1642924130950072\n",
      "Copying gs://qwiklabs-gcp-00-8962bb682264_cloudbuild/source/1642924130.219413-9cf6987f087d434d903f7b571c4c3a23.tgz#1642924130950072...\n",
      "/ [1 files][  225.0 B/  225.0 B]                                                \n",
      "Operation completed over 1 objects/225.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/3 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "7b1a6ab2e44d: Already exists\n",
      "afe6e63412ab: Pulling fs layer\n",
      "5b73234eab2e: Pulling fs layer\n",
      "1685de22cc18: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "b47909ca4b31: Pulling fs layer\n",
      "17bb6932867c: Pulling fs layer\n",
      "334ec3463b8d: Pulling fs layer\n",
      "9832ea45781a: Pulling fs layer\n",
      "6dd58a1feee0: Pulling fs layer\n",
      "4eba552f6461: Pulling fs layer\n",
      "318bfd6f44e9: Pulling fs layer\n",
      "f68af1a5c61b: Pulling fs layer\n",
      "005c6ce38bc0: Pulling fs layer\n",
      "f70512a0e436: Pulling fs layer\n",
      "63c5da3a0938: Pulling fs layer\n",
      "559eafc8c9b9: Pulling fs layer\n",
      "558154dd3629: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "b47909ca4b31: Waiting\n",
      "17bb6932867c: Waiting\n",
      "334ec3463b8d: Waiting\n",
      "9832ea45781a: Waiting\n",
      "6dd58a1feee0: Waiting\n",
      "4eba552f6461: Waiting\n",
      "318bfd6f44e9: Waiting\n",
      "f68af1a5c61b: Waiting\n",
      "005c6ce38bc0: Waiting\n",
      "f70512a0e436: Waiting\n",
      "63c5da3a0938: Waiting\n",
      "559eafc8c9b9: Waiting\n",
      "558154dd3629: Waiting\n",
      "afe6e63412ab: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "afe6e63412ab: Pull complete\n",
      "b47909ca4b31: Verifying Checksum\n",
      "b47909ca4b31: Download complete\n",
      "1685de22cc18: Verifying Checksum\n",
      "1685de22cc18: Download complete\n",
      "334ec3463b8d: Download complete\n",
      "9832ea45781a: Verifying Checksum\n",
      "9832ea45781a: Download complete\n",
      "6dd58a1feee0: Verifying Checksum\n",
      "6dd58a1feee0: Download complete\n",
      "4eba552f6461: Verifying Checksum\n",
      "4eba552f6461: Download complete\n",
      "17bb6932867c: Verifying Checksum\n",
      "17bb6932867c: Download complete\n",
      "f68af1a5c61b: Verifying Checksum\n",
      "f68af1a5c61b: Download complete\n",
      "318bfd6f44e9: Verifying Checksum\n",
      "318bfd6f44e9: Download complete\n",
      "005c6ce38bc0: Verifying Checksum\n",
      "005c6ce38bc0: Download complete\n",
      "f70512a0e436: Verifying Checksum\n",
      "f70512a0e436: Download complete\n",
      "63c5da3a0938: Verifying Checksum\n",
      "63c5da3a0938: Download complete\n",
      "558154dd3629: Verifying Checksum\n",
      "558154dd3629: Download complete\n",
      "5b73234eab2e: Verifying Checksum\n",
      "5b73234eab2e: Download complete\n",
      "559eafc8c9b9: Verifying Checksum\n",
      "559eafc8c9b9: Download complete\n",
      "5b73234eab2e: Pull complete\n",
      "1685de22cc18: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "b47909ca4b31: Pull complete\n",
      "17bb6932867c: Pull complete\n",
      "334ec3463b8d: Pull complete\n",
      "9832ea45781a: Pull complete\n",
      "6dd58a1feee0: Pull complete\n",
      "4eba552f6461: Pull complete\n",
      "318bfd6f44e9: Pull complete\n",
      "f68af1a5c61b: Pull complete\n",
      "005c6ce38bc0: Pull complete\n",
      "f70512a0e436: Pull complete\n",
      "63c5da3a0938: Pull complete\n",
      "559eafc8c9b9: Pull complete\n",
      "558154dd3629: Pull complete\n",
      "Digest: sha256:f5e5f89edac3ca1399f03b83136545610a5a205754eee28149a55900cd9b68cc\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 9d37d1fb4b24\n",
      "Step 2/3 : RUN pip install kfp==0.2.5\n",
      " ---> Running in 227695c944e7\n",
      "Collecting kfp==0.2.5\n",
      "  Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.8.2)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (6.0)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.43.0)\n",
      "Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "  Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.3.0)\n",
      "Requirement already satisfied: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (36.0.1)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.3.3)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting cloudpickle==1.1.1\n",
      "  Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (4.3.1)\n",
      "Collecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.15.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (59.6.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.2.4)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.1.0)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (3.19.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.26.0)\n",
      "Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.3.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.2.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (21.2.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (4.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (4.0.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.18.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.2.3)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.3.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.13.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.37.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=2.4.2->kfp==0.2.5) (2.21)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.53.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.1.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema>=3.0.1->kfp==0.2.5) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (2.0.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.1.1)\n",
      "Building wheels for collected packages: kfp, argo-models, tabulate, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159979 sha256=b2a765a13af2bcbaa5d1b132ef71cfcdd7991588b0d96584b7d0f0fa9fe9c726\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "  Building wheel for argo-models (setup.py): started\n",
      "  Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "  Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=dc4fa25b84caf9e77139954be2c59bf08e030628ac4a4ad80f89cae279f86b16\n",
      "  Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "  Building wheel for tabulate (setup.py): started\n",
      "  Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "  Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23393 sha256=b67bccf046f90afa2669706a11298c0f93a22f2ee26a489f40aa4bf9eacedf46\n",
      "  Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102464 sha256=ce010fce6d897529f1ecdef2f43dd4897e2f5b73ca1e2afc47a7ef14aedd6a40\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=8ee161750d93029b3ef163c84697185021c291f3fc3990d47dec0505a034efad\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Successfully built kfp argo-models tabulate kfp-server-api strip-hints\n",
      "Installing collected packages: urllib3, kubernetes, tabulate, strip-hints, requests-toolbelt, kfp-server-api, Deprecated, cloudpickle, click, argo-models, kfp\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.7\n",
      "    Uninstalling urllib3-1.26.7:\n",
      "      Successfully uninstalled urllib3-1.26.7\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 21.7.0\n",
      "    Uninstalling kubernetes-21.7.0:\n",
      "      Successfully uninstalled kubernetes-21.7.0\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.0.0\n",
      "    Uninstalling cloudpickle-2.0.0:\n",
      "      Successfully uninstalled cloudpickle-2.0.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.3\n",
      "    Uninstalling click-8.0.3:\n",
      "      Successfully uninstalled click-8.0.3\n",
      "Successfully installed Deprecated-1.2.13 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 requests-toolbelt-0.9.1 strip-hints-0.1.10 tabulate-0.8.3 urllib3-1.24.3\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "black 21.12b0 requires click>=7.1.2, but you have click 7.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 227695c944e7\n",
      " ---> 00295450d652\n",
      "Step 3/3 : ENTRYPOINT [\"/bin/bash\"]\n",
      " ---> Running in 1c1115a570bc\n",
      "Removing intermediate container 1c1115a570bc\n",
      " ---> caca1dd8fa6d\n",
      "Successfully built caca1dd8fa6d\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-00-8962bb682264/kfp-cli:latest\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-00-8962bb682264/kfp-cli:latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-00-8962bb682264/kfp-cli]\n",
      "bed9d31784a5: Preparing\n",
      "1454938e8c75: Preparing\n",
      "1b913f373c39: Preparing\n",
      "8a3424f475cb: Preparing\n",
      "79de2e7105e3: Preparing\n",
      "80fbf1f9de72: Preparing\n",
      "95dd9c08140a: Preparing\n",
      "c77c6b1c9a72: Preparing\n",
      "eb2e1edb9042: Preparing\n",
      "321c7ad58201: Preparing\n",
      "3976e9ae051e: Preparing\n",
      "62aa5e865341: Preparing\n",
      "0d7828e68dda: Preparing\n",
      "ce1f2b34baf7: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "14f43fe5fb59: Preparing\n",
      "2ac22cf101f4: Preparing\n",
      "104b9a8c66b0: Preparing\n",
      "9f54eef41275: Preparing\n",
      "80fbf1f9de72: Waiting\n",
      "95dd9c08140a: Waiting\n",
      "c77c6b1c9a72: Waiting\n",
      "eb2e1edb9042: Waiting\n",
      "321c7ad58201: Waiting\n",
      "3976e9ae051e: Waiting\n",
      "62aa5e865341: Waiting\n",
      "0d7828e68dda: Waiting\n",
      "ce1f2b34baf7: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "14f43fe5fb59: Waiting\n",
      "2ac22cf101f4: Waiting\n",
      "104b9a8c66b0: Waiting\n",
      "9f54eef41275: Waiting\n",
      "79de2e7105e3: Mounted from deeplearning-platform-release/base-cpu\n",
      "8a3424f475cb: Mounted from deeplearning-platform-release/base-cpu\n",
      "1b913f373c39: Mounted from deeplearning-platform-release/base-cpu\n",
      "1454938e8c75: Mounted from deeplearning-platform-release/base-cpu\n",
      "95dd9c08140a: Mounted from deeplearning-platform-release/base-cpu\n",
      "80fbf1f9de72: Mounted from deeplearning-platform-release/base-cpu\n",
      "c77c6b1c9a72: Mounted from deeplearning-platform-release/base-cpu\n",
      "eb2e1edb9042: Mounted from deeplearning-platform-release/base-cpu\n",
      "321c7ad58201: Mounted from deeplearning-platform-release/base-cpu\n",
      "3976e9ae051e: Mounted from deeplearning-platform-release/base-cpu\n",
      "bed9d31784a5: Pushed\n",
      "62aa5e865341: Mounted from deeplearning-platform-release/base-cpu\n",
      "0d7828e68dda: Mounted from deeplearning-platform-release/base-cpu\n",
      "5f70bf18a086: Layer already exists\n",
      "ce1f2b34baf7: Mounted from deeplearning-platform-release/base-cpu\n",
      "2ac22cf101f4: Mounted from deeplearning-platform-release/base-cpu\n",
      "14f43fe5fb59: Mounted from deeplearning-platform-release/base-cpu\n",
      "9f54eef41275: Layer already exists\n",
      "104b9a8c66b0: Mounted from deeplearning-platform-release/base-cpu\n",
      "latest: digest: sha256:4a5737488fbda62b2f030475da3f2f59f36d0d30bde257fde3af0c8106d478ae size: 4291\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                 STATUS\n",
      "9f32b423-b466-4483-8c4d-118a47c18587  2022-01-23T07:48:51+00:00  1M59S     gs://qwiklabs-gcp-00-8962bb682264_cloudbuild/source/1642924130.219413-9cf6987f087d434d903f7b571c4c3a23.tgz  gcr.io/qwiklabs-gcp-00-8962bb682264/kfp-cli (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag {IMAGE_URI} kfp-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the **Cloud Build** workflow.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "In the cell below, you'll complete the `cloudbuild.yaml` file describing the CI/CD workflow and prescribing how environment specific settings are abstracted using **Cloud Build** variables.\n",
    "\n",
    "The CI/CD workflow automates the steps you walked through manually during `lab-02-kfp-pipeline`:\n",
    "1. Builds the trainer image\n",
    "1. Builds the base image for custom components\n",
    "1. Compiles the pipeline\n",
    "1. Uploads the pipeline to the KFP environment\n",
    "1. Pushes the trainer and base images to your project's **Container Registry**\n",
    "\n",
    "Although the KFP backend supports pipeline versioning, this feature has not been yet enable through the **KFP** CLI. As a temporary workaround, in the **Cloud Build** configuration a value of the `TAG_NAME` variable is appended to the name of the pipeline. \n",
    "\n",
    "The **Cloud Build** workflow configuration uses both standard and custom [Cloud Build builders](https://cloud.google.com/cloud-build/docs/cloud-builders). The custom builder encapsulates **KFP CLI**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile trainer_image/train.py\n",
    "\"\"\"\"Covertype Classifier trainer script.\"\"\"\n",
    "\n",
    "import pickle\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import hypertune\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path,\n",
    "    alpha, max_iter, hptune):\n",
    "    \"\"\"Trains the Covertype Classifier model.\"\"\"\n",
    "\n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "\n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    numeric_features = [\n",
    "      'Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "      'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "      'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "      'Horizontal_Distance_To_Fire_Points'\n",
    "    ]\n",
    "    \n",
    "    categorical_features = ['Wilderness_Area', 'Soil_Type']\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[(\n",
    "        'num', StandardScaler(),\n",
    "        numeric_features), ('cat', OneHotEncoder(), categorical_features)])\n",
    "\n",
    "    pipeline = Pipeline([('preprocessor', preprocessor),\n",
    "        ('classifier', SGDClassifier(loss='log'))])\n",
    "\n",
    "    num_features_type_map = {feature: 'float64' for feature in numeric_features}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map)\n",
    "\n",
    "    print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "    X_train = df_train.drop('Cover_Type', axis=1)\n",
    "    y_train = df_train['Cover_Type']\n",
    "\n",
    "    pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    if hptune:\n",
    "        X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "        y_validation = df_validation['Cover_Type']\n",
    "        accuracy = pipeline.score(X_validation, y_validation)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='accuracy', metric_value=accuracy)\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(pipeline, model_file)\n",
    "        gcs_model_path = '{}/{}'.format(job_dir, model_filename)\n",
    "        subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path],\n",
    "            stderr=sys.stdout)\n",
    "        print('Saved model in: {}'.format(gcs_model_path))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile trainer_image/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
    "WORKDIR /app\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile base_image/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/base-cpu\n",
    "RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/helper_components.py\n",
    "\"\"\"Helper components.\"\"\"\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "def retrieve_best_run(project_id: str, job_id: str) -> NamedTuple('Outputs', \n",
    "    [('metric_value', float), ('alpha', float),('max_iter', int)]):\n",
    "    \"\"\"Retrieves the parameters of the best Hypertune run.\"\"\"\n",
    "\n",
    "    from googleapiclient import discovery\n",
    "    from googleapiclient import errors\n",
    "\n",
    "    ml = discovery.build('ml', 'v1')\n",
    "\n",
    "    job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n",
    "    request = ml.projects().jobs().get(name=job_name)\n",
    "\n",
    "    try:\n",
    "        response = request.execute()\n",
    "    except errors.HttpError as err:\n",
    "        print(err)\n",
    "    except:\n",
    "        print('Unexpected error')\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    best_trial = response['trainingOutput']['trials'][0]\n",
    "\n",
    "    metric_value = best_trial['finalMetric']['objectiveValue']\n",
    "    alpha = float(best_trial['hyperparameters']['alpha'])\n",
    "    max_iter = int(best_trial['hyperparameters']['max_iter'])\n",
    "\n",
    "    return (metric_value, alpha, max_iter)\n",
    "\n",
    "def evaluate_model(dataset_path: str, model_path: str, metric_name: str) -> NamedTuple(\n",
    "    'Outputs', [('metric_name', str), ('metric_value', float),\n",
    "        ('mlpipeline_metrics', 'Metrics')]):\n",
    "    \"\"\"Evaluates a trained sklearn model.\"\"\"\n",
    "    #import joblib\n",
    "    import pickle\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import subprocess\n",
    "    import sys\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "    df_test = pd.read_csv(dataset_path)\n",
    "\n",
    "    X_test = df_test.drop('Cover_Type', axis=1)\n",
    "    y_test = df_test['Cover_Type']\n",
    "\n",
    "    # Copy the model from GCS\n",
    "    model_filename = 'model.pkl'\n",
    "    gcs_model_filepath = '{}/{}'.format(model_path, model_filename)\n",
    "    print(gcs_model_filepath)\n",
    "    subprocess.check_call(['gsutil', 'cp', gcs_model_filepath, model_filename],\n",
    "        stderr=sys.stdout)\n",
    "\n",
    "    with open(model_filename, 'rb') as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "\n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    if metric_name == 'accuracy':\n",
    "        metric_value = accuracy_score(y_test, y_hat)\n",
    "    elif metric_name == 'recall':\n",
    "        metric_value = recall_score(y_test, y_hat)\n",
    "    else:\n",
    "        metric_name = 'N/A'\n",
    "        metric_value = 0\n",
    "\n",
    "    # Export the metric\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': metric_name,\n",
    "          'numberValue': float(metric_value)\n",
    "      }]\n",
    "    }\n",
    "\n",
    "    return (metric_name, metric_value, json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipeline/covertype_training_pipeline.py\n",
    "\"\"\"KFP pipeline orchestrating BigQuery and Cloud AI Platform services.\"\"\"\n",
    "\n",
    "import os\n",
    "from helper_components import evaluate_model\n",
    "from helper_components import retrieve_best_run\n",
    "from jinja2 import Template\n",
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.dsl.types import Dict\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "USE_KFP_SA = os.getenv('USE_KFP_SA')\n",
    "\n",
    "TRAINING_FILE_PATH = 'datasets/training/data.csv'\n",
    "VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 6,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"max_iter\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [500, 1000]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"alpha\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.0001,\n",
    "                \"maxValue\": 0.001,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Helper functions\n",
    "def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "    \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "    sampling_query_template = \"\"\"\n",
    "        SELECT *\n",
    "        FROM \n",
    "            `{{ source_table }}` AS cover\n",
    "        WHERE \n",
    "        MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "    \"\"\"\n",
    "    query = Template(sampling_query_template).render(\n",
    "        source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "    return query\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')\n",
    "retrieve_best_run_op = func_to_container_op(\n",
    "    retrieve_best_run, base_image=BASE_IMAGE)\n",
    "evaluate_model_op = func_to_container_op(evaluate_model, base_image=BASE_IMAGE)\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Covertype Classifier Training',\n",
    "    description='The pipeline training and deploying the Covertype classifierpipeline_yaml'\n",
    ")\n",
    "def covertype_train(project_id,\n",
    "                    region,\n",
    "                    source_table_name,\n",
    "                    gcs_root,\n",
    "                    dataset_id,\n",
    "                    evaluation_metric_name,\n",
    "                    evaluation_metric_threshold,\n",
    "                    model_id,\n",
    "                    version_id,\n",
    "                    replace_existing_version,\n",
    "                    hypertune_settings=HYPERTUNE_SETTINGS,\n",
    "                    dataset_location='US'):\n",
    "    \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "    # Create the training split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "    training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "    create_training_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=training_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Create the validation split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "    validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "    create_validation_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=validation_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Create the testing split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "    testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "    create_testing_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=testing_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "        '--training_dataset_path',\n",
    "        create_training_split.outputs['output_gcs_path'],\n",
    "        '--validation_dataset_path',\n",
    "        create_validation_split.outputs['output_gcs_path'], '--hptune', 'True'\n",
    "    ]\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "        kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAINER_IMAGE,\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "\n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    train_args = [\n",
    "        '--training_dataset_path',\n",
    "        create_training_split.outputs['output_gcs_path'],\n",
    "        '--validation_dataset_path',\n",
    "        create_validation_split.outputs['output_gcs_path'], '--alpha',\n",
    "        get_best_trial.outputs['alpha'], '--max_iter',\n",
    "        get_best_trial.outputs['max_iter'], '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAINER_IMAGE,\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "\n",
    "    # Evaluate the model on the testing split\n",
    "    eval_model = evaluate_model_op(\n",
    "        dataset_path=str(create_testing_split.outputs['output_gcs_path']),\n",
    "        model_path=str(train_model.outputs['job_dir']),\n",
    "        metric_name=evaluation_metric_name)\n",
    "\n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(\n",
    "        eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "    deploy_model = mlengine_deploy_op(\n",
    "        model_uri=train_model.outputs['job_dir'],\n",
    "        project_id=project_id,\n",
    "        model_id=model_id,\n",
    "        version_id=version_id,\n",
    "        runtime_version=RUNTIME_VERSION,\n",
    "        python_version=PYTHON_VERSION,\n",
    "        replace_existing_version=replace_existing_version)\n",
    "\n",
    "    # Configure the pipeline to run using the service account defined\n",
    "    # in the user-gcp-sa k8s secret\n",
    "    if USE_KFP_SA == 'True':\n",
    "        kfp.dsl.get_pipeline_conf().add_op_transformer(use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "# Build the trainer image\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME', '.']\n",
    "  dir: $_PIPELINE_FOLDER/trainer_image\n",
    "\n",
    "# Build the base image for lightweight components\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME', '.']\n",
    "  dir: $_PIPELINE_FOLDER/base_image\n",
    "\n",
    "# Compile the pipeline\n",
    "# Set the environment variables below for the $_PIPELINE_DSL script\n",
    "# HINT: https://cloud.google.com/cloud-build/docs/configuring-builds/substitute-variable-values\n",
    "- name: 'gcr.io/$PROJECT_ID/kfp-cli'\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    dsl-compile --py $_PIPELINE_DSL --output $_PIPELINE_PACKAGE\n",
    "  env:\n",
    "  - 'BASE_IMAGE=gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME'\n",
    "  - 'TRAINER_IMAGE=gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME'\n",
    "  - 'RUNTIME_VERSION=$_RUNTIME_VERSION'\n",
    "  - 'PYTHON_VERSION=$_PYTHON_VERSION'\n",
    "  - 'COMPONENT_URL_SEARCH_PREFIX=$_COMPONENT_URL_SEARCH_PREFIX'\n",
    "  - 'USE_KFP_SA=$_USE_KFP_SA'\n",
    "  dir: $_PIPELINE_FOLDER/pipeline\n",
    "\n",
    "# Upload the pipeline\n",
    "# Use the kfp-cli Cloud Builder and write the command to upload the ktf pipeline \n",
    "- name: 'gcr.io/$PROJECT_ID/kfp-cli'\n",
    "  args:\n",
    "  - '-c'\n",
    "  - |\n",
    "    kfp --endpoint $_ENDPOINT pipeline upload -p ${_PIPELINE_NAME}_$TAG_NAME $_PIPELINE_PACKAGE\n",
    "  dir: $_PIPELINE_FOLDER/pipeline\n",
    "\n",
    "\n",
    "# Push the images to Container Registry\n",
    "# List the images to be pushed to the project Docker registry\n",
    "images: ['gcr.io/$PROJECT_ID/$_TRAINER_IMAGE_NAME:$TAG_NAME', \n",
    "         'gcr.io/$PROJECT_ID/$_BASE_IMAGE_NAME:$TAG_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually triggering CI/CD runs\n",
    "\n",
    "You can manually trigger **Cloud Build** runs using the `gcloud builds submit` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSTITUTIONS=\"\"\"\n",
    "_ENDPOINT={},\\\n",
    "_TRAINER_IMAGE_NAME=trainer_image,\\\n",
    "_BASE_IMAGE_NAME=base_image,\\\n",
    "TAG_NAME=test,\\\n",
    "_PIPELINE_FOLDER=.,\\\n",
    "_PIPELINE_DSL=covertype_training_pipeline.py,\\\n",
    "_PIPELINE_PACKAGE=covertype_training_pipeline.yaml,\\\n",
    "_PIPELINE_NAME=covertype_continuous_training,\\\n",
    "_RUNTIME_VERSION=1.15,\\\n",
    "_PYTHON_VERSION=3.7,\\\n",
    "_USE_KFP_SA=True,\\\n",
    "_COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
    "\"\"\".format(ENDPOINT).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 9 file(s) totalling 79.3 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://qwiklabs-gcp-00-8962bb682264_cloudbuild/source/1642924693.051181-cdf8977a4d5a4c42bd2699392fca8b0e.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/qwiklabs-gcp-00-8962bb682264/locations/global/builds/576ded26-6ad3-461b-b999-1bc2f8fc0bc7].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/576ded26-6ad3-461b-b999-1bc2f8fc0bc7?project=334587390474].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"576ded26-6ad3-461b-b999-1bc2f8fc0bc7\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://qwiklabs-gcp-00-8962bb682264_cloudbuild/source/1642924693.051181-cdf8977a4d5a4c42bd2699392fca8b0e.tgz#1642924693264234\n",
      "Copying gs://qwiklabs-gcp-00-8962bb682264_cloudbuild/source/1642924693.051181-cdf8977a4d5a4c42bd2699392fca8b0e.tgz#1642924693264234...\n",
      "/ [1 files][ 19.1 KiB/ 19.1 KiB]                                                \n",
      "Operation completed over 1 objects/19.1 KiB.                                     \n",
      "BUILD\n",
      "Starting Step #0\n",
      "Step #0: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0: Sending build context to Docker daemon  6.144kB\n",
      "Step #0: Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "Step #0: latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "Step #0: 7b1a6ab2e44d: Already exists\n",
      "Step #0: afe6e63412ab: Pulling fs layer\n",
      "Step #0: 5b73234eab2e: Pulling fs layer\n",
      "Step #0: 1685de22cc18: Pulling fs layer\n",
      "Step #0: 4f4fb700ef54: Pulling fs layer\n",
      "Step #0: b47909ca4b31: Pulling fs layer\n",
      "Step #0: 17bb6932867c: Pulling fs layer\n",
      "Step #0: 334ec3463b8d: Pulling fs layer\n",
      "Step #0: 9832ea45781a: Pulling fs layer\n",
      "Step #0: 6dd58a1feee0: Pulling fs layer\n",
      "Step #0: 4eba552f6461: Pulling fs layer\n",
      "Step #0: 318bfd6f44e9: Pulling fs layer\n",
      "Step #0: f68af1a5c61b: Pulling fs layer\n",
      "Step #0: 005c6ce38bc0: Pulling fs layer\n",
      "Step #0: f70512a0e436: Pulling fs layer\n",
      "Step #0: 63c5da3a0938: Pulling fs layer\n",
      "Step #0: 559eafc8c9b9: Pulling fs layer\n",
      "Step #0: 558154dd3629: Pulling fs layer\n",
      "Step #0: 4f4fb700ef54: Waiting\n",
      "Step #0: b47909ca4b31: Waiting\n",
      "Step #0: 17bb6932867c: Waiting\n",
      "Step #0: 334ec3463b8d: Waiting\n",
      "Step #0: 9832ea45781a: Waiting\n",
      "Step #0: 6dd58a1feee0: Waiting\n",
      "Step #0: 4eba552f6461: Waiting\n",
      "Step #0: 318bfd6f44e9: Waiting\n",
      "Step #0: f68af1a5c61b: Waiting\n",
      "Step #0: 005c6ce38bc0: Waiting\n",
      "Step #0: f70512a0e436: Waiting\n",
      "Step #0: 63c5da3a0938: Waiting\n",
      "Step #0: 559eafc8c9b9: Waiting\n",
      "Step #0: 558154dd3629: Waiting\n",
      "Step #0: afe6e63412ab: Verifying Checksum\n",
      "Step #0: afe6e63412ab: Download complete\n",
      "Step #0: 4f4fb700ef54: Verifying Checksum\n",
      "Step #0: 4f4fb700ef54: Download complete\n",
      "Step #0: afe6e63412ab: Pull complete\n",
      "Step #0: b47909ca4b31: Verifying Checksum\n",
      "Step #0: b47909ca4b31: Download complete\n",
      "Step #0: 1685de22cc18: Verifying Checksum\n",
      "Step #0: 1685de22cc18: Download complete\n",
      "Step #0: 334ec3463b8d: Verifying Checksum\n",
      "Step #0: 334ec3463b8d: Download complete\n",
      "Step #0: 9832ea45781a: Verifying Checksum\n",
      "Step #0: 9832ea45781a: Download complete\n",
      "Step #0: 17bb6932867c: Verifying Checksum\n",
      "Step #0: 17bb6932867c: Download complete\n",
      "Step #0: 4eba552f6461: Verifying Checksum\n",
      "Step #0: 4eba552f6461: Download complete\n",
      "Step #0: 6dd58a1feee0: Verifying Checksum\n",
      "Step #0: 6dd58a1feee0: Download complete\n",
      "Step #0: 318bfd6f44e9: Verifying Checksum\n",
      "Step #0: 318bfd6f44e9: Download complete\n",
      "Step #0: f68af1a5c61b: Verifying Checksum\n",
      "Step #0: f68af1a5c61b: Download complete\n",
      "Step #0: 005c6ce38bc0: Verifying Checksum\n",
      "Step #0: 005c6ce38bc0: Download complete\n",
      "Step #0: f70512a0e436: Verifying Checksum\n",
      "Step #0: f70512a0e436: Download complete\n",
      "Step #0: 63c5da3a0938: Verifying Checksum\n",
      "Step #0: 63c5da3a0938: Download complete\n",
      "Step #0: 558154dd3629: Verifying Checksum\n",
      "Step #0: 558154dd3629: Download complete\n",
      "Step #0: 5b73234eab2e: Verifying Checksum\n",
      "Step #0: 5b73234eab2e: Download complete\n",
      "Step #0: 559eafc8c9b9: Verifying Checksum\n",
      "Step #0: 559eafc8c9b9: Download complete\n",
      "Step #0: 5b73234eab2e: Pull complete\n",
      "Step #0: 1685de22cc18: Pull complete\n",
      "Step #0: 4f4fb700ef54: Pull complete\n",
      "Step #0: b47909ca4b31: Pull complete\n",
      "Step #0: 17bb6932867c: Pull complete\n",
      "Step #0: 334ec3463b8d: Pull complete\n",
      "Step #0: 9832ea45781a: Pull complete\n",
      "Step #0: 6dd58a1feee0: Pull complete\n",
      "Step #0: 4eba552f6461: Pull complete\n",
      "Step #0: 318bfd6f44e9: Pull complete\n",
      "Step #0: f68af1a5c61b: Pull complete\n",
      "Step #0: 005c6ce38bc0: Pull complete\n",
      "Step #0: f70512a0e436: Pull complete\n",
      "Step #0: 63c5da3a0938: Pull complete\n",
      "Step #0: 559eafc8c9b9: Pull complete\n",
      "Step #0: 558154dd3629: Pull complete\n",
      "Step #0: Digest: sha256:f5e5f89edac3ca1399f03b83136545610a5a205754eee28149a55900cd9b68cc\n",
      "Step #0: Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      "Step #0:  ---> 9d37d1fb4b24\n",
      "Step #0: Step 2/5 : RUN pip install -U fire cloudml-hypertune scikit-learn==0.20.4 pandas==0.24.2\n",
      "Step #0:  ---> Running in eb8bd925a72e\n",
      "Step #0: Collecting fire\n",
      "Step #0:   Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting cloudml-hypertune\n",
      "Step #0:   Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting scikit-learn==0.20.4\n",
      "Step #0:   Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Step #0: Collecting pandas==0.24.2\n",
      "Step #0:   Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Step #0: Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.19.5)\n",
      "Step #0: Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.7.3)\n",
      "Step #0: Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.2)\n",
      "Step #0: Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2021.3)\n",
      "Step #0: Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Step #0: Collecting termcolor\n",
      "Step #0:   Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Building wheels for collected packages: fire, cloudml-hypertune, termcolor\n",
      "Step #0:   Building wheel for fire (setup.py): started\n",
      "Step #0:   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=5bcd807e97d06f37f0065b45ad723eeb938d4e7a7134d94ae4ce59d443c59985\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "Step #0:   Building wheel for cloudml-hypertune (setup.py): started\n",
      "Step #0:   Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3987 sha256=e870583e829c662c1341ee774983cde3c9142ea94cdcdd04ddf13c0d88de12e3\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Step #0:   Building wheel for termcolor (setup.py): started\n",
      "Step #0:   Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=6bb62f3b7cac9f79a794cf569bd8451c34efc1836e3094ca76172260c082659f\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Step #0: Successfully built fire cloudml-hypertune termcolor\n",
      "Step #0: Installing collected packages: termcolor, scikit-learn, pandas, fire, cloudml-hypertune\n",
      "Step #0:   Attempting uninstall: scikit-learn\n",
      "Step #0:     Found existing installation: scikit-learn 1.0.1\n",
      "Step #0:     Uninstalling scikit-learn-1.0.1:\n",
      "Step #0:       Successfully uninstalled scikit-learn-1.0.1\n",
      "Step #0:   Attempting uninstall: pandas\n",
      "Step #0:     Found existing installation: pandas 1.3.5\n",
      "Step #0:     Uninstalling pandas-1.3.5:\n",
      "Step #0:       Successfully uninstalled pandas-1.3.5\n",
      "Step #0: Successfully installed cloudml-hypertune-0.1.0.dev6 fire-0.4.0 pandas-0.24.2 scikit-learn-0.20.4 termcolor-1.1.0\n",
      "Step #0: \u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "Step #0: visions 0.7.4 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #0: statsmodels 0.13.1 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #0: phik 0.12.0 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #0: pandas-profiling 3.1.0 requires pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #0: \u001b[0m\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #0: \u001b[0mRemoving intermediate container eb8bd925a72e\n",
      "Step #0:  ---> c7e1959552c5\n",
      "Step #0: Step 3/5 : WORKDIR /app\n",
      "Step #0:  ---> Running in aea953c99cfb\n",
      "Step #0: Removing intermediate container aea953c99cfb\n",
      "Step #0:  ---> 03e6cec9d07f\n",
      "Step #0: Step 4/5 : COPY train.py .\n",
      "Step #0:  ---> 0d40ff02ee57\n",
      "Step #0: Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      "Step #0:  ---> Running in 05a54eb97825\n",
      "Step #0: Removing intermediate container 05a54eb97825\n",
      "Step #0:  ---> c4f044663af9\n",
      "Step #0: Successfully built c4f044663af9\n",
      "Step #0: Successfully tagged gcr.io/qwiklabs-gcp-00-8962bb682264/trainer_image:test\n",
      "Finished Step #0\n",
      "Starting Step #1\n",
      "Step #1: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #1: Sending build context to Docker daemon  2.048kB\n",
      "Step #1: Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "Step #1:  ---> 9d37d1fb4b24\n",
      "Step #1: Step 2/2 : RUN pip install -U fire scikit-learn==0.20.4 pandas==0.24.2 kfp==0.2.5\n",
      "Step #1:  ---> Running in ddfd9a1c99a3\n",
      "Step #1: Collecting fire\n",
      "Step #1:   Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Step #1:   Preparing metadata (setup.py): started\n",
      "Step #1:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #1: Collecting scikit-learn==0.20.4\n",
      "Step #1:   Downloading scikit_learn-0.20.4-cp37-cp37m-manylinux1_x86_64.whl (5.4 MB)\n",
      "Step #1: Collecting pandas==0.24.2\n",
      "Step #1:   Downloading pandas-0.24.2-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Step #1: Collecting kfp==0.2.5\n",
      "Step #1:   Downloading kfp-0.2.5.tar.gz (116 kB)\n",
      "Step #1:   Preparing metadata (setup.py): started\n",
      "Step #1:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #1: Requirement already satisfied: numpy>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.19.5)\n",
      "Step #1: Requirement already satisfied: scipy>=0.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.20.4) (1.7.3)\n",
      "Step #1: Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2.8.2)\n",
      "Step #1: Requirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.7/site-packages (from pandas==0.24.2) (2021.3)\n",
      "Step #1: Collecting urllib3<1.25,>=1.15\n",
      "Step #1:   Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "Step #1: Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.16.0)\n",
      "Step #1: Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2021.10.8)\n",
      "Step #1: Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (6.0)\n",
      "Step #1: Requirement already satisfied: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (1.43.0)\n",
      "Step #1: Collecting kubernetes<=10.0.0,>=8.0.0\n",
      "Step #1:   Downloading kubernetes-10.0.0-py2.py3-none-any.whl (1.5 MB)\n",
      "Step #1: Requirement already satisfied: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.3.0)\n",
      "Step #1: Requirement already satisfied: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (36.0.1)\n",
      "Step #1: Requirement already satisfied: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (2.3.3)\n",
      "Step #1: Collecting requests_toolbelt>=0.8.0\n",
      "Step #1:   Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Step #1: Collecting cloudpickle==1.1.1\n",
      "Step #1:   Downloading cloudpickle-1.1.1-py2.py3-none-any.whl (17 kB)\n",
      "Step #1: Collecting kfp-server-api<=0.1.40,>=0.1.18\n",
      "Step #1:   Downloading kfp-server-api-0.1.40.tar.gz (38 kB)\n",
      "Step #1:   Preparing metadata (setup.py): started\n",
      "Step #1:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #1: Collecting argo-models==2.2.1a\n",
      "Step #1:   Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "Step #1:   Preparing metadata (setup.py): started\n",
      "Step #1:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #1: Requirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.2.5) (4.3.1)\n",
      "Step #1: Collecting tabulate==0.8.3\n",
      "Step #1:   Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "Step #1:   Preparing metadata (setup.py): started\n",
      "Step #1:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #1: Collecting click==7.0\n",
      "Step #1:   Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "Step #1: Collecting Deprecated\n",
      "Step #1:   Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Step #1: Collecting strip-hints\n",
      "Step #1:   Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "Step #1:   Preparing metadata (setup.py): started\n",
      "Step #1:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #1: Collecting termcolor\n",
      "Step #1:   Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Step #1:   Preparing metadata (setup.py): started\n",
      "Step #1:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #1: Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.2.5) (1.15.0)\n",
      "Step #1: Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (0.2.7)\n",
      "Step #1: Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.2.4)\n",
      "Step #1: Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (4.8)\n",
      "Step #1: Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.2.5) (59.6.0)\n",
      "Step #1: Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (3.19.1)\n",
      "Step #1: Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.2.1)\n",
      "Step #1: Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.1.0)\n",
      "Step #1: Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.3.2)\n",
      "Step #1: Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.2.5) (2.26.0)\n",
      "Step #1: Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (5.4.0)\n",
      "Step #1: Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (21.2.0)\n",
      "Step #1: Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (0.18.0)\n",
      "Step #1: Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (4.9.0)\n",
      "Step #1: Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp==0.2.5) (4.0.1)\n",
      "Step #1: Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.2.3)\n",
      "Step #1: Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (1.3.0)\n",
      "Step #1: Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp==0.2.5) (1.13.3)\n",
      "Step #1: Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp==0.2.5) (0.37.0)\n",
      "Step #1: Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=2.4.2->kfp==0.2.5) (2.21)\n",
      "Step #1: Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.53.0)\n",
      "Step #1: Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (1.1.2)\n",
      "Step #1: Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema>=3.0.1->kfp==0.2.5) (3.6.0)\n",
      "Step #1: Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==0.2.5) (0.4.8)\n",
      "Step #1: Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (2.0.9)\n",
      "Step #1: Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==0.2.5) (3.1)\n",
      "Step #1: Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=10.0.0,>=8.0.0->kfp==0.2.5) (3.1.1)\n",
      "Step #1: Building wheels for collected packages: kfp, argo-models, tabulate, fire, kfp-server-api, strip-hints, termcolor\n",
      "Step #1:   Building wheel for kfp (setup.py): started\n",
      "Step #1:   Building wheel for kfp (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for kfp: filename=kfp-0.2.5-py3-none-any.whl size=159979 sha256=135a54ff80fe049ae74891424455cc978d7806ea6605887c9ff0cc3d500b7bc2\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/98/74/7e/0a882d654bdf82d039460ab5c6adf8724ae56e277de7c0eaea\n",
      "Step #1:   Building wheel for argo-models (setup.py): started\n",
      "Step #1:   Building wheel for argo-models (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for argo-models: filename=argo_models-2.2.1a0-py3-none-any.whl size=57307 sha256=6354bccf90f23129bcf991c9f2d9c3cb781e380048614099a949d6dc7c2aa7cb\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/a9/4b/fd/cdd013bd2ad1a7162ecfaf954e9f1bb605174a20e3c02016b7\n",
      "Step #1:   Building wheel for tabulate (setup.py): started\n",
      "Step #1:   Building wheel for tabulate (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for tabulate: filename=tabulate-0.8.3-py3-none-any.whl size=23393 sha256=da61322a9e8bfe4e095356646e5334637d8993d810154eca508f2ed3067da5d0\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/b8/a2/a6/812a8a9735b090913e109133c7c20aaca4cf07e8e18837714f\n",
      "Step #1:   Building wheel for fire (setup.py): started\n",
      "Step #1:   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=d763fe6bc9863d9455f2da71db5394b173557f9dfbf18efce44e7757153708c0\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "Step #1:   Building wheel for kfp-server-api (setup.py): started\n",
      "Step #1:   Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for kfp-server-api: filename=kfp_server_api-0.1.40-py3-none-any.whl size=102464 sha256=4893f5f44352e03cb24e1bb8122d8a2f43f27df6f18539d65c2809fa340c88d7\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/01/e3/43/3972dea76ee89e35f090b313817089043f2609236cf560069d\n",
      "Step #1:   Building wheel for strip-hints (setup.py): started\n",
      "Step #1:   Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=9481b2f60be82dabd417ee01e128187bab36a15d4a449127bf2a8c002f503c70\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/5e/14/c3/6e44e9b2545f2d570b03f5b6d38c00b7534aa8abb376978363\n",
      "Step #1:   Building wheel for termcolor (setup.py): started\n",
      "Step #1:   Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "Step #1:   Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=af1392980e23a79a3edb6d4010833581394d396d74816f7d730a4a444c56bc60\n",
      "Step #1:   Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Step #1: Successfully built kfp argo-models tabulate fire kfp-server-api strip-hints termcolor\n",
      "Step #1: Installing collected packages: urllib3, kubernetes, termcolor, tabulate, strip-hints, requests-toolbelt, kfp-server-api, Deprecated, cloudpickle, click, argo-models, scikit-learn, pandas, kfp, fire\n",
      "Step #1:   Attempting uninstall: urllib3\n",
      "Step #1:     Found existing installation: urllib3 1.26.7\n",
      "Step #1:     Uninstalling urllib3-1.26.7:\n",
      "Step #1:       Successfully uninstalled urllib3-1.26.7\n",
      "Step #1:   Attempting uninstall: kubernetes\n",
      "Step #1:     Found existing installation: kubernetes 21.7.0\n",
      "Step #1:     Uninstalling kubernetes-21.7.0:\n",
      "Step #1:       Successfully uninstalled kubernetes-21.7.0\n",
      "Step #1:   Attempting uninstall: cloudpickle\n",
      "Step #1:     Found existing installation: cloudpickle 2.0.0\n",
      "Step #1:     Uninstalling cloudpickle-2.0.0:\n",
      "Step #1:       Successfully uninstalled cloudpickle-2.0.0\n",
      "Step #1:   Attempting uninstall: click\n",
      "Step #1:     Found existing installation: click 8.0.3\n",
      "Step #1:     Uninstalling click-8.0.3:\n",
      "Step #1:       Successfully uninstalled click-8.0.3\n",
      "Step #1:   Attempting uninstall: scikit-learn\n",
      "Step #1:     Found existing installation: scikit-learn 1.0.1\n",
      "Step #1:     Uninstalling scikit-learn-1.0.1:\n",
      "Step #1:       Successfully uninstalled scikit-learn-1.0.1\n",
      "Step #1:   Attempting uninstall: pandas\n",
      "Step #1:     Found existing installation: pandas 1.3.5\n",
      "Step #1:     Uninstalling pandas-1.3.5:\n",
      "Step #1:       Successfully uninstalled pandas-1.3.5\n",
      "Step #1: Successfully installed Deprecated-1.2.13 argo-models-2.2.1a0 click-7.0 cloudpickle-1.1.1 fire-0.4.0 kfp-0.2.5 kfp-server-api-0.1.40 kubernetes-10.0.0 pandas-0.24.2 requests-toolbelt-0.9.1 scikit-learn-0.20.4 strip-hints-0.1.10 tabulate-0.8.3 termcolor-1.1.0 urllib3-1.24.3\n",
      "Step #1: \u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "Step #1: visions 0.7.4 requires pandas>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #1: statsmodels 0.13.1 requires pandas>=0.25, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #1: phik 0.12.0 requires pandas>=0.25.1, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #1: pandas-profiling 3.1.0 requires pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3, but you have pandas 0.24.2 which is incompatible.\n",
      "Step #1: black 21.12b0 requires click>=7.1.2, but you have click 7.0 which is incompatible.\n",
      "Step #1: \u001b[0m\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #1: \u001b[0mRemoving intermediate container ddfd9a1c99a3\n",
      "Step #1:  ---> 92956261b473\n",
      "Step #1: Successfully built 92956261b473\n",
      "Step #1: Successfully tagged gcr.io/qwiklabs-gcp-00-8962bb682264/base_image:test\n",
      "Finished Step #1\n",
      "Starting Step #2\n",
      "Step #2: Pulling image: gcr.io/qwiklabs-gcp-00-8962bb682264/kfp-cli\n",
      "Step #2: Using default tag: latest\n",
      "Step #2: latest: Pulling from qwiklabs-gcp-00-8962bb682264/kfp-cli\n",
      "Step #2: 7b1a6ab2e44d: Already exists\n",
      "Step #2: afe6e63412ab: Already exists\n",
      "Step #2: 5b73234eab2e: Already exists\n",
      "Step #2: 1685de22cc18: Already exists\n",
      "Step #2: 4f4fb700ef54: Already exists\n",
      "Step #2: b47909ca4b31: Already exists\n",
      "Step #2: 17bb6932867c: Already exists\n",
      "Step #2: 334ec3463b8d: Already exists\n",
      "Step #2: 9832ea45781a: Already exists\n",
      "Step #2: 6dd58a1feee0: Already exists\n",
      "Step #2: 4eba552f6461: Already exists\n",
      "Step #2: 318bfd6f44e9: Already exists\n",
      "Step #2: f68af1a5c61b: Already exists\n",
      "Step #2: 005c6ce38bc0: Already exists\n",
      "Step #2: f70512a0e436: Already exists\n",
      "Step #2: 63c5da3a0938: Already exists\n",
      "Step #2: 559eafc8c9b9: Already exists\n",
      "Step #2: 558154dd3629: Already exists\n",
      "Step #2: 0ff58e04b278: Pulling fs layer\n",
      "Step #2: 0ff58e04b278: Download complete\n",
      "Step #2: 0ff58e04b278: Pull complete\n",
      "Step #2: Digest: sha256:4a5737488fbda62b2f030475da3f2f59f36d0d30bde257fde3af0c8106d478ae\n",
      "Step #2: Status: Downloaded newer image for gcr.io/qwiklabs-gcp-00-8962bb682264/kfp-cli:latest\n",
      "Step #2: gcr.io/qwiklabs-gcp-00-8962bb682264/kfp-cli:latest\n",
      "Finished Step #2\n",
      "Starting Step #3\n",
      "Step #3: Already have image (with digest): gcr.io/qwiklabs-gcp-00-8962bb682264/kfp-cli\n",
      "Step #3: Pipeline 9bccd960-2cdc-4c7e-8248-3eb019855d16 has been submitted\n",
      "Step #3: \n",
      "Step #3: Pipeline Details\n",
      "Step #3: ------------------\n",
      "Step #3: ID           9bccd960-2cdc-4c7e-8248-3eb019855d16\n",
      "Step #3: Name         covertype_continuous_training_test\n",
      "Step #3: Description\n",
      "Step #3: Uploaded at  2022-01-23T08:00:42+00:00\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | Parameter Name              | Default Value                                    |\n",
      "Step #3: +=============================+==================================================+\n",
      "Step #3: | project_id                  |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | region                      |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | source_table_name           |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | gcs_root                    |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | dataset_id                  |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | evaluation_metric_name      |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | evaluation_metric_threshold |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | model_id                    |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | version_id                  |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | replace_existing_version    |                                                  |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | hypertune_settings          | {                                                |\n",
      "Step #3: |                             |     \"hyperparameters\":  {                        |\n",
      "Step #3: |                             |         \"goal\": \"MAXIMIZE\",                      |\n",
      "Step #3: |                             |         \"maxTrials\": 6,                          |\n",
      "Step #3: |                             |         \"maxParallelTrials\": 3,                  |\n",
      "Step #3: |                             |         \"hyperparameterMetricTag\": \"accuracy\",   |\n",
      "Step #3: |                             |         \"enableTrialEarlyStopping\": True,        |\n",
      "Step #3: |                             |         \"params\": [                              |\n",
      "Step #3: |                             |             {                                    |\n",
      "Step #3: |                             |                 \"parameterName\": \"max_iter\",     |\n",
      "Step #3: |                             |                 \"type\": \"DISCRETE\",              |\n",
      "Step #3: |                             |                 \"discreteValues\": [500, 1000]    |\n",
      "Step #3: |                             |             },                                   |\n",
      "Step #3: |                             |             {                                    |\n",
      "Step #3: |                             |                 \"parameterName\": \"alpha\",        |\n",
      "Step #3: |                             |                 \"type\": \"DOUBLE\",                |\n",
      "Step #3: |                             |                 \"minValue\": 0.0001,              |\n",
      "Step #3: |                             |                 \"maxValue\": 0.001,               |\n",
      "Step #3: |                             |                 \"scaleType\": \"UNIT_LINEAR_SCALE\" |\n",
      "Step #3: |                             |             }                                    |\n",
      "Step #3: |                             |         ]                                        |\n",
      "Step #3: |                             |     }                                            |\n",
      "Step #3: |                             | }                                                |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Step #3: | dataset_location            | US                                               |\n",
      "Step #3: +-----------------------------+--------------------------------------------------+\n",
      "Finished Step #3\n",
      "PUSH\n",
      "Pushing gcr.io/qwiklabs-gcp-00-8962bb682264/trainer_image:test\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-00-8962bb682264/trainer_image]\n",
      "cf0aa5bd64df: Preparing\n",
      "827ba0a60550: Preparing\n",
      "1caae861703d: Preparing\n",
      "1454938e8c75: Preparing\n",
      "1b913f373c39: Preparing\n",
      "8a3424f475cb: Preparing\n",
      "79de2e7105e3: Preparing\n",
      "80fbf1f9de72: Preparing\n",
      "95dd9c08140a: Preparing\n",
      "c77c6b1c9a72: Preparing\n",
      "eb2e1edb9042: Preparing\n",
      "321c7ad58201: Preparing\n",
      "3976e9ae051e: Preparing\n",
      "62aa5e865341: Preparing\n",
      "0d7828e68dda: Preparing\n",
      "ce1f2b34baf7: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "14f43fe5fb59: Preparing\n",
      "2ac22cf101f4: Preparing\n",
      "104b9a8c66b0: Preparing\n",
      "9f54eef41275: Preparing\n",
      "8a3424f475cb: Waiting\n",
      "79de2e7105e3: Waiting\n",
      "80fbf1f9de72: Waiting\n",
      "95dd9c08140a: Waiting\n",
      "c77c6b1c9a72: Waiting\n",
      "eb2e1edb9042: Waiting\n",
      "321c7ad58201: Waiting\n",
      "3976e9ae051e: Waiting\n",
      "62aa5e865341: Waiting\n",
      "0d7828e68dda: Waiting\n",
      "ce1f2b34baf7: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "14f43fe5fb59: Waiting\n",
      "2ac22cf101f4: Waiting\n",
      "104b9a8c66b0: Waiting\n",
      "9f54eef41275: Waiting\n",
      "1b913f373c39: Layer already exists\n",
      "1454938e8c75: Layer already exists\n",
      "8a3424f475cb: Layer already exists\n",
      "79de2e7105e3: Layer already exists\n",
      "95dd9c08140a: Layer already exists\n",
      "80fbf1f9de72: Layer already exists\n",
      "eb2e1edb9042: Layer already exists\n",
      "c77c6b1c9a72: Layer already exists\n",
      "3976e9ae051e: Layer already exists\n",
      "321c7ad58201: Layer already exists\n",
      "62aa5e865341: Layer already exists\n",
      "0d7828e68dda: Layer already exists\n",
      "ce1f2b34baf7: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "14f43fe5fb59: Layer already exists\n",
      "2ac22cf101f4: Layer already exists\n",
      "104b9a8c66b0: Layer already exists\n",
      "9f54eef41275: Layer already exists\n",
      "cf0aa5bd64df: Pushed\n",
      "827ba0a60550: Pushed\n",
      "1caae861703d: Pushed\n",
      "test: digest: sha256:e2a5ebe10b765e3f7414f37b4f6780ee005054405fa3c4a3278150ea89ce6492 size: 4707\n",
      "Pushing gcr.io/qwiklabs-gcp-00-8962bb682264/base_image:test\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-00-8962bb682264/base_image]\n",
      "1c6345547538: Preparing\n",
      "1454938e8c75: Preparing\n",
      "1b913f373c39: Preparing\n",
      "8a3424f475cb: Preparing\n",
      "79de2e7105e3: Preparing\n",
      "80fbf1f9de72: Preparing\n",
      "95dd9c08140a: Preparing\n",
      "c77c6b1c9a72: Preparing\n",
      "eb2e1edb9042: Preparing\n",
      "321c7ad58201: Preparing\n",
      "3976e9ae051e: Preparing\n",
      "62aa5e865341: Preparing\n",
      "0d7828e68dda: Preparing\n",
      "ce1f2b34baf7: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "14f43fe5fb59: Preparing\n",
      "2ac22cf101f4: Preparing\n",
      "104b9a8c66b0: Preparing\n",
      "9f54eef41275: Preparing\n",
      "3976e9ae051e: Waiting\n",
      "62aa5e865341: Waiting\n",
      "0d7828e68dda: Waiting\n",
      "ce1f2b34baf7: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "14f43fe5fb59: Waiting\n",
      "2ac22cf101f4: Waiting\n",
      "104b9a8c66b0: Waiting\n",
      "80fbf1f9de72: Waiting\n",
      "95dd9c08140a: Waiting\n",
      "c77c6b1c9a72: Waiting\n",
      "eb2e1edb9042: Waiting\n",
      "321c7ad58201: Waiting\n",
      "9f54eef41275: Waiting\n",
      "1b913f373c39: Layer already exists\n",
      "8a3424f475cb: Layer already exists\n",
      "1454938e8c75: Layer already exists\n",
      "79de2e7105e3: Layer already exists\n",
      "c77c6b1c9a72: Layer already exists\n",
      "80fbf1f9de72: Layer already exists\n",
      "95dd9c08140a: Layer already exists\n",
      "eb2e1edb9042: Layer already exists\n",
      "62aa5e865341: Layer already exists\n",
      "321c7ad58201: Layer already exists\n",
      "0d7828e68dda: Layer already exists\n",
      "3976e9ae051e: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "ce1f2b34baf7: Layer already exists\n",
      "14f43fe5fb59: Layer already exists\n",
      "2ac22cf101f4: Layer already exists\n",
      "104b9a8c66b0: Layer already exists\n",
      "9f54eef41275: Layer already exists\n",
      "1c6345547538: Pushed\n",
      "test: digest: sha256:3064f48d46e1e9a26f689f48b047b0e45ae697cdb245438895af01e99c5b5dd5 size: 4292\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                      IMAGES                                                            STATUS\n",
      "576ded26-6ad3-461b-b999-1bc2f8fc0bc7  2022-01-23T07:58:13+00:00  2M50S     gs://qwiklabs-gcp-00-8962bb682264_cloudbuild/source/1642924693.051181-cdf8977a4d5a4c42bd2699392fca8b0e.tgz  gcr.io/qwiklabs-gcp-00-8962bb682264/trainer_image:test (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit . --config cloudbuild.yaml --substitutions {SUBSTITUTIONS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up GitHub integration\n",
    "\n",
    "### Exercise\n",
    "\n",
    "In this exercise you integrate your CI/CD workflow with **GitHub**, using [Cloud Build GitHub App](https://github.com/marketplace/google-cloud-build). \n",
    "You will set up a trigger that starts the CI/CD workflow when a new tag is applied to the **GitHub** repo managing the  pipeline source code. You will use a fork of this repo as your source GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Create a fork of this repo\n",
    "[Follow the GitHub documentation](https://help.github.com/en/github/getting-started-with-github/fork-a-repo) to fork this repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Create a **Cloud Build** trigger\n",
    "\n",
    "Connect the fork you created in the previous step to your Google Cloud project and create a trigger following the steps in the [Creating GitHub app trigger](https://cloud.google.com/cloud-build/docs/create-github-app-triggers) article. Use the following values on the **Edit trigger** form:\n",
    "\n",
    "|Field|Value|\n",
    "|-----|-----|\n",
    "|Name|[YOUR TRIGGER NAME]|\n",
    "|Description|[YOUR TRIGGER DESCRIPTION]|\n",
    "|Event| Tag|\n",
    "|Source| [YOUR FORK]|\n",
    "|Tag (regex)|.\\*|\n",
    "|Build Configuration|Cloud Build configuration file (yaml or json)|\n",
    "|Cloud Build configuration file location|/ workshops/kfp-caip-sklearn/lab-03-kfp-cicd/cloudbuild.yaml|\n",
    "\n",
    "\n",
    "Use the following values for the substitution variables:\n",
    "\n",
    "|Variable|Value|\n",
    "|--------|-----|\n",
    "|_BASE_IMAGE_NAME|base_image|\n",
    "|_COMPONENT_URL_SEARCH_PREFIX|https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/|\n",
    "|_ENDPOINT|[Your inverting proxy host]|\n",
    "|_PIPELINE_DSL|covertype_training_pipeline.py|\n",
    "|_PIPELINE_FOLDER|workshops/kfp-caip-sklearn/lab-03-kfp-cicd|\n",
    "|_PIPELINE_NAME|covertype_training_deployment|\n",
    "|_PIPELINE_PACKAGE|covertype_training_pipeline.yaml|\n",
    "|_PYTHON_VERSION|3.7|\n",
    "|_RUNTIME_VERSION|1.15|\n",
    "|_TRAINER_IMAGE_NAME|trainer_image|\n",
    "|_USE_KFP_SA|False|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger the build\n",
    "\n",
    "To start an automated build [create a new release of the repo in GitHub](https://help.github.com/en/github/administering-a-repository/creating-releases). Alternatively, you can start the build by applying a tag using `git`. \n",
    "```\n",
    "git tag [TAG NAME]\n",
    "git push origin --tags\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-cpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
